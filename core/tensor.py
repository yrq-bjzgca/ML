import numpy as np
from typing import List, Optional, Union, Tuple
import contextlib
class Tensor:
    """
    æ‰‹å†™å¯å¾®å¼ é‡
    è¦æ±‚ï¼šæ”¯æŒå¹¿æ’­ã€åˆ‡ç‰‡ã€padï¼›è®°å½•è®¡ç®—å›¾ï¼›é“¾å¼åå‘
    """
    # ç±»å˜é‡ï¼Œæ§åˆ¶å…¨å±€æ¢¯åº¦è®¡ç®—
    _grad_enabled = True

    def __init__(self, data, requires_grad=False):
        self.data = np.asarray(data, dtype=np.float32)
        self.shape = self.data.shape
        self.requires_grad = requires_grad and Tensor._grad_enabled
        self.grad = None
        if requires_grad:
            self.grad = np.zeros_like(self.data)
        self._backward = lambda: None #åå‘å‡½æ•°
        self._parents = [] #è®¡ç®—å›¾çˆ¶äº²
        
    # ---------- å·¥å…· ----------
    def __repr__(self):
        return f"Tensor({self.data}, shape={self.shape}, requires_grad={self.grad is not None})"

    @classmethod
    @contextlib.contextmanager
    def no_grad(cls):
        """
        ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¦ç”¨æ¢¯åº¦è®¡ç®—
        
        ç”¨æ³•:
            with Tensor.no_grad():
                # åœ¨è¿™ä¸ªå—ä¸­åˆ›å»ºçš„å¼ é‡ä¸ä¼šè®¡ç®—æ¢¯åº¦
                x = Tensor([1, 2, 3], requires_grad=True)  # å®é™… requires_grad=False
        """
        old_state = cls._grad_enabled
        cls._grad_enabled = False
        try:
            yield
        finally:
            cls._grad_enabled = old_state
    
    @classmethod
    def set_grad_enabled(cls, mode: bool):
        """
        è®¾ç½®æ¢¯åº¦è®¡ç®—æ˜¯å¦å¯ç”¨
        
        å‚æ•°:
            mode: True å¯ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒFalse ç¦ç”¨
        """
        cls._grad_enabled = mode
    
    @classmethod
    def is_grad_enabled(cls):
        """
        æ£€æŸ¥æ¢¯åº¦è®¡ç®—æ˜¯å¦å¯ç”¨
        
        è¿”å›:
            bool: å¦‚æœå¯ç”¨æ¢¯åº¦è®¡ç®—è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        return cls._grad_enabled
    
    # @property
    # def requires_grad(self):
    #     # åˆ¤æ–­æ˜¯ä¸æ˜¯éœ€è¦è®¡ç®—æ¢¯åº¦
    #     return self.grad is not None

    @property
    def ndim(self):
        """è¿”å›å¼ é‡çš„ç»´åº¦æ•°"""
        return self.data.ndim
    

    def copy(self) -> 'Tensor':
        """
        åˆ›å»ºå½“å‰å¼ é‡çš„å‰¯æœ¬
        
        è¿”å›:
            æ–°çš„ Tensor å¯¹è±¡ï¼ŒåŒ…å«æ•°æ®çš„å‰¯æœ¬
        """

        # å¤åˆ¶æ•°æ®
        data_copy = self.data.copy()

        # åˆ›å»ºæ–°çš„tensor
        new_tensor = Tensor(data_copy, requires_grad=self.requires_grad)

        # å¦‚æœåŸå§‹å¼ é‡æœ‰æ¢¯åº¦ï¼Œä¹Ÿå¤åˆ¶æ¢¯åº¦
        if self.grad is not None:
            new_tensor.grad = self.grad.copy()
        return new_tensor
    
    # ä¼˜åŒ–åçš„è½´è®¡ç®—ï¼ˆ__add__ å’Œ __mul__ ä¸­å‡é€‚ç”¨ï¼‰
    # def get_broadcast_axes(self, grad_shape, target_shape):
    #     """è®¡ç®—å¹¿æ’­æ–°å¢çš„è½´ï¼ˆéœ€æ±‚å’Œçš„è½´ï¼‰"""
    #     grad_ndim = len(grad_shape)
    #     target_ndim = len(target_shape)
    #     max_ndim = max(grad_ndim, target_ndim)

    #     # è¡¥1ä½¿ä¸¤è€…ç»´åº¦æ•°ä¸€è‡´ï¼ˆä¾¿äºé€ä½ï¼‰
    #     grad_shape_padded = (1,) * (max_ndim - grad_ndim) + grad_shape
    #     target_shape_padded = (1,) * (max_ndim - target_ndim) + target_shape
        
    #     # if grad_ndim > target_ndim:
    #     #     return tuple(range(grad_ndim - target_ndim))
    #     # if grad_ndim < target_ndim:
    #     #     return tuple(range(grad_ndim, target_ndim))
    #     # if grad_ndim == target_ndim:
    #     #     return ()
    #     # æ–°å¢çš„è½´æ˜¯ grad æ¯” target å¤šçš„å‰ N ä¸ªè½´
    #     # return tuple(range(grad_ndim - target_ndim))

    #     axes = []
    #     for i in range(max_ndim):
    #         g = grad_shape_padded[i]
    #         t = target_shape_padded[i]
    #         # è‹¥æ¢¯åº¦åœ¨è¯¥è½´çš„å°ºå¯¸ > ç›®æ ‡å°ºå¯¸ï¼Œä¸”ç›®æ ‡å°ºå¯¸ä¸º1ï¼ˆè¯´æ˜æ˜¯å¹¿æ’­æ‰©å±•çš„è½´ï¼‰
    #         if g > t and t == 1:
    #             axes.append(i)
    #     return tuple(axes)

    def get_broadcast_axes(self, grad_shape, target_shape):
        # è®¡ç®—å¹¿æ’­è½´
        grad_ndim = len(grad_shape)
        target_ndim = len(target_shape)

        # å¤„ç†ç»´åº¦ä¸ä¸€è‡´çš„æƒ…å†µ
        if grad_ndim > target_ndim:
            return tuple(range(grad_ndim - target_ndim))
        axes = []

        # ä»å³å‘å·¦å¹¿æ’­
        for i in range(1,min(grad_ndim, target_ndim)+1):
            grad_dim = grad_shape[-i]
            target_dim = target_shape[-i]
            if grad_dim != target_dim and target_dim ==1:
                axes.append(grad_ndim - i)
        return tuple(axes)

    def __add__(self, other:'Tensor')->'Tensor':
        other = other if isinstance(other, Tensor) else Tensor(other,requires_grad=False)
        # å¸¦æœ‰å¹¿æ’­
        a,b  = np.broadcast_arrays(self.data, other.data)
        out = Tensor(a + b, requires_grad=self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad:
                if self.grad is None:
                    # grad_broadcast = out.grad
                    self.grad = np.zeros_like(self.data)
                #è®¡ç®—å¹¿æ’­è½´æ±‚å’Œ
                axes = self.get_broadcast_axes(out.grad.shape, self.shape)
                if axes:
                    summed_grad = out.grad.sum(axis =axes, keepdims =True)
                    if summed_grad.shape!=self.shape:
                        # å°è¯•reshapeåˆ°ç›®æ ‡å½¢çŠ¶
                        try:
                            reshaped_grad = summed_grad.reshape(self.shape)
                            self.grad += reshaped_grad
                        except ValueError:
                            self.grad += np.broadcast_to(summed_grad, self.shape)
                    else:
                        self.grad += summed_grad
                else:
                    #æ²¡æœ‰éœ€è¦ç›¸åŠ çš„å’Œ
                    self.grad += out.grad
            
            if other.requires_grad:
                if other.grad is None:
                    # grad_broadcast = out.
                    other.grad = np.zeros_like(other.data)
                # è®¡ç®—å¹¿æ’­è½´æ±‚å’Œ
                axes = self.get_broadcast_axes(out.grad.shape, other.shape)
                if axes:
                    summed_grad = out.grad.sum(axis =axes, keepdims =True)
                    if summed_grad.shape!=other.shape:
                        # å°è¯•reshapeåˆ°ç›®æ ‡å½¢çŠ¶
                        try:
                            reshaped_grad = summed_grad.reshape(other.shape)
                            other.grad += reshaped_grad
                        except ValueError:
                            other.grad += np.broadcast_to(summed_grad, other.shape)
                    else:
                        other.grad += summed_grad
                else:
                    #æ²¡æœ‰éœ€è¦ç›¸åŠ çš„å’Œ
                    other.grad += out.grad
        out._backward = _backward
        out._parents = [self, other]
        return out
    
    # å‡æ³•
    def __sub__(self, other:'Tensor')->'Tensor':
        other = other if isinstance(other,Tensor) else Tensor(other)
        return self + (-other)
    
    #å¯¹å¼ é‡å–è´Ÿæ•°
    def __neg__(self)->'Tensor':
        out = Tensor(-self.data, requires_grad=self.grad is not None)
        def _backward():
            # if self.grad is not None:
            if self.requires_grad and self.grad is not None:
                self.grad += -out.grad
        out._backward = _backward
        out._parents = [self]
        return out
    
    def __mul__(self, other:'Tensor')->'Tensor':
        other = other if isinstance(other, Tensor) else Tensor(other,requires_grad=False)
        # L æœ€ç»ˆæŸå¤±æ ‡é‡
        # âˆ‚L/âˆ‚out = out.grad
        # âˆ‚L/âˆ‚x = self.grad 
        # âˆ‚L/âˆ‚y = other.grad
        # âˆ‚out/âˆ‚x ä½¿ç”¨numpyå¹¿æ’­å®ç°
        # out = x * y
        # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out * y
        # âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚out * x
        
        #åŸå§‹ç‰ˆæœ¬
        # out = Tensor(self.data * other.data, requires_grad= True)
        # def _backward():
        #     if self.grad is not None:
        #         self.grad += out.grad * other.data
        #     if other.grad is not None:
        #         other.grad += out.grad * self.data

        # out._backward =  _backward
        # out._parents = [self,other]
        # return out
        # å¸¦æœ‰å¹¿æ’­
        a, b = np.broadcast_arrays(self.data, other.data)
        out = Tensor(a * b, requires_grad= self.requires_grad or other.requires_grad)
        def _backward():
            if self.requires_grad:
                if self.grad is None:
                    self.grad = np.zeros_like(self.data)
                grad_broad = out.grad * b
                axes = self.get_broadcast_axes(grad_broad.shape, self.shape)
                if axes:
                    summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                    if summed_grad.shape!=self.shape:
                        # å°è¯•reshapeåˆ°ç›®æ ‡å½¢çŠ¶
                        try:
                            reshaped_grad = summed_grad.reshape(self.shape)
                            self.grad += reshaped_grad
                        except ValueError:
                            self.grad += np.broadcast_to(summed_grad, self.shape)
                    else:
                        self.grad += summed_grad
                else:
                    #æ²¡æœ‰éœ€è¦ç›¸åŠ çš„å’Œ
                    self.grad += grad_broad
                # self.grad += grad_broad.sum(axis=axis).reshape(self.shape)
                # summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                # self.grad += np.broadcast_to(summed_grad, self.shape)
            if other.requires_grad:
                if other.grad is None:
                    other.grad = np.zeros_like(other.data)
                grad_broad = out.grad * a
                axes = self.get_broadcast_axes(grad_broad.shape, other.shape)
                if axes:
                    summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                    if summed_grad.shape!=other.shape:
                        # å°è¯•reshapeåˆ°ç›®æ ‡å½¢çŠ¶
                        try:
                            reshaped_grad = summed_grad.reshape(other.shape)
                            other.grad += reshaped_grad
                        except ValueError:
                            other.grad += np.broadcast_to(summed_grad, other.shape)
                    else:
                        other.grad += summed_grad
                else:
                        other.grad += grad_broad
                # other.grad += grad_broad.sum(axis=axis).reshape(other.shape)
                # summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                # other.grad += np.broadcast_to(summed_grad, other.shape)                

        out._backward = _backward
        out._parents = [self, other]
        return out

    def __pow__(self, other:'Tensor')->'Tensor':
        """
        é‡è½½ ** è¿ç®—ç¬¦
        ç›®å‰æ”¯æŒï¼š
            - self ä¸ºä»»æ„å¼ é‡
            - other ä¸ºå¸¸æ•°æˆ– 0-D/1-D å¸¸é‡å¼ é‡ï¼ˆå¹¿æ’­ï¼‰
        åå‘ï¼š
            âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out * other * x^(other-1)
            âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚out * x^y * log(x)   ï¼ˆy ä¸ºå¼ é‡æ—¶æ‰éœ€è¦ï¼‰
        """
        other = other if isinstance(other, Tensor) else Tensor(other,requires_grad=False)
        out_data = np.pow(self.data,other.data)
        out = Tensor(out_data, requires_grad= (self.requires_grad or other.requires_grad))
        def _backward():
            if self.requires_grad:

                grad_self = out.grad * other.data * np.pow(self.data,other.data-1) 
                if grad_self.shape != self.shape:
                    axes = self.get_broadcast_axes(grad_self.shape, self.shape)
                    grad_self = grad_self.sum(axis=axes, keepdims=True)
                    grad_self = np.broadcast_to(grad_self, self.shape)
                if self.grad is None:
                    self.grad = np.zeros_like(self.data)

                # summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                # self.grad += np.broadcast_to(summed_grad, self.grad_broad)
                self.grad += grad_self

            if other.requires_grad:   
                grad_other = out.grad * other.data * np.log(self.data + 1e-12)# é˜²æ­¢log(0)
                if grad_other.shape != other.shape:
                    axes = self.get_broadcast_axes(grad_other.shape, other.shape)
                    grad_other = grad_self.sum(axis=axes, keepdims=True)
                    grad_other = np.broadcast_to(grad_other, other.shape)   
                if other.grad is None:
                    other.grad = np.zeros_like(other.data)
                # grad_broad = out.grad * other.data * np.pow(self.data,other.data-1) 
                # axes = self.get_broadcast_axes(grad_broad.shape, other.shape)
                # summed_grad = grad_broad.sum(axis =axes, keepdims =True)
                # other.grad += np.broadcast_to(summed_grad, other.grad_broad)
                    other.grad += grad_other

        out._backward = _backward
        out._parents = [self, other]
        return out
    
    def sqrt(self):
        return self ** 0.5
    
    def square(self):
        """
        å¯¹self**2 çš„æœ‰å¥½çš„æ¥å£
        """
        return self**2
    
    # é™¤æ³•
    def __truediv__(self, other:'Tensor')->'Tensor':
        other  = other if isinstance(other,Tensor) else Tensor(other, requires_grad=False)
        a, b = np.broadcast_arrays(self.data, other.data)
        out = Tensor(a/b, requires_grad=self.requires_grad or other.requires_grad)
        def _backeard():
            if self.requires_grad:
                if self.grad is None:
                    self.grad = np.zeros_like(self.data)
                # è‡ªèº«æ¢¯åº¦è®¡ç®— (dL/dx = dL/dout * 1/b)
                grad_broad = out.grad/b
                axes = self.get_broadcast_axes(grad_broad.shape, self.shape)
                self.grad += grad_broad.sum(axis=axes).reshape(self.shape)
                # summed_grad = grad_broad.sum(axis=axes, keepdims=True)
                # self.grad += np.broadcast_to(summed_grad, self.shape)

            if other.requires_grad:
                if other.grad is None:
                    other.grad = np.zeros_like(other.data)
                # å…¶ä»–å¼ é‡æ¢¯åº¦è®¡ç®— (dL/dy = -dL/dout * x/bÂ²)
                grad_broad = -out.grad*a/(b**2+1e-12)
                axes = self.get_broadcast_axes(grad_broad.shape, other.shape)
                other.grad += grad_broad.sum(axis=axes).reshape(other.shape)
                # summed_grad = grad_broad.sum(axis=axes, keepdims=True)
                # other.grad += np.broadcast_to(summed_grad, other.shape)
               

        out._backward = _backeard
        out._parents = [self, other]
        return out

    def exp(self:'Tensor')->'Tensor':
        # å¸¦æœ‰å¹¿æ’­çš„,ä¸å†™broadcast_arraysæ˜¯å› ä¸ºnpä¸­è‡ªå¸¦çš„å¹¿æ’­çš„æœºåˆ¶
        out_data = np.exp(self.data)
        out = Tensor(out_data, requires_grad= self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += out.grad * out.data
        out._backward = _backward
        out._parents = [self]
        return out
            
    def log(self:'Tensor')->'Tensor':
        # å¸¦æœ‰å¹¿æ’­çš„
        eps = 1e-12
        out_data = np.log(self.data+eps)
        out = Tensor(out_data, requires_grad= self.requires_grad)
        def _backward():
            if self.grad is not None:
                # å¸¦æœ‰ä¿æŠ¤æªæ–½
                self.grad += out.grad / (self.data + eps)
        out._backward = _backward
        out._parents = [self]
        return out
    
    # ä¸¤ä¸ªçŸ©é˜µçš„ä¹˜æ³•
    def __matmul__(self, other:'Tensor', axis=None)->'Tensor':
        other = other if isinstance(other, Tensor) else Tensor(other,requires_grad=False)
        batch_self,batch_other = self.data.shape[:-2],other.data.shape[:-2]
        # -2æ˜¯é«˜ï¼Œ -1æ˜¯å®½
        m, k1 = self.data.shape[-2], self.data.shape[-1]
        k2, n = other.data.shape[-2], other.data.shape[-1]

        if k1!=k2:
            raise ValueError(f"matul shape error, mismatch:{self.shape} and {other.shape}, by yrq")
        out_data = self.data@other.data#ä½¿ç”¨numpyè¿›è¡Œæ“çºµ
        out = Tensor(out_data, requires_grad=self.requires_grad or other.requires_grad)
        # A:(â€¦, m, k)
        # B:(â€¦, k, n)
        # C:A @ B å½¢çŠ¶ï¼š(â€¦, m, n)
        # G := âˆ‚L/âˆ‚C å½¢çŠ¶ï¼š(â€¦, m, n)ï¼ˆä¸Šæ¸¸æ¢¯åº¦ï¼‰
        # å¯¹Açš„æ¢¯åº¦ï¼šâˆ‚L/âˆ‚A = G @ Báµ€
        # å¯¹Bçš„æ¢¯åº¦ï¼šâˆ‚L/âˆ‚B = Aáµ€ @ G
        def _backward():    
            if self.requires_grad:
                if self.grad is None:
                    # âˆ‚L/âˆ‚A = G Â· Báµ€
                    self.grad = np.zeros_like(self.data)
                grad_broad = out.grad @ other.data.swapaxes(-1,-2)

                # print(f"DEBUG matmul grad: self.shape={self.shape}, other.shape={other.shape}")
                # print(f"DEBUG: out.grad.shape={out.grad.shape}, grad_broad.shape={grad_broad.shape}")
                
                if grad_broad.shape != self.shape:
                    # axis = tuple(range(grad_broad.ndim - self.ndim))
                    axes = self.get_broadcast_axes(grad_broad.shape, self.shape)
                    grad_broad = grad_broad.sum(axis=axes).reshape(self.shape)
                self.grad += grad_broad
            if other.requires_grad:
                if other.grad is None:
                    # âˆ‚L/âˆ‚B = Aáµ€ Â· G
                    other.grad = np.zeros_like(other.data)
                grad_broad = self.data.swapaxes(-1,-2) @ out.grad
                if grad_broad.shape != other.shape:
                    # axis = tuple(range(grad_broad.ndim - other.ndim))
                    axes = self.get_broadcast_axes(grad_broad.shape, other.shape)
                    grad_broad = grad_broad.sum(axis=axes).reshape(other.shape)
                other.grad += grad_broad

        out._backward = _backward
        out._parents = [self, other]
        return out
    


    # å¯¹ä¸€ä¸ªmatrixåœ¨ä¸€ä¸ªæ–¹å‘ä¸Šè¿›è¡Œæ±‚å’Œ
    def sum(self, axis=None, keepdims=False)->'Tensor': 
        current_axis  = axis
        out_data = np.sum(self.data, axis = current_axis , keepdims = keepdims)
        out = Tensor(out_data, requires_grad= self.requires_grad)
        def _backward():
            if self.requires_grad:
                if self.grad is None:
                    self.grad = np.zeros_like(self.data)
            # å°†ä¸Šæ¸¸çš„æ¢¯åº¦reshapeæˆå¸¦æœ‰keepdimsçš„å½¢çŠ¶ï¼Œä¿è¯çŸ©é˜µçš„ç»´åº¦
                if keepdims:
                    grad_reshape = out.grad
                else:
                    if current_axis  is None:
                        grad_reshape = np.reshape(out.grad, (1,) * self.ndim)
                        # grad_reshape = np.expand_dims(out.grad, axis=tuple(range(self.ndim)))
                    else:
                        # if isinstance(axes,int):
                        #     axes = (current_axis ,)
                        axes = (current_axis,) if isinstance(current_axis, int) else current_axis
                        grad_reshape = out.grad
                        for ax in axes:
                            grad_reshape = np.expand_dims(grad_reshape,axis = ax)
                            
                    grad_broadcast = np.broadcast_to(grad_reshape,self.shape)
                    self.grad += grad_broadcast
        out._backward = _backward
        out._parents = [self]
        return out
    
    def mean(self, axis=None, keepdims=False)->'Tensor':
        out_data = np.mean(self.data, axis = axis, keepdims = keepdims)
        out = Tensor(out_data, requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad:
                if self.grad is None:
                    self.grad = np.zeros_like(self.data) 
                # n = self.data.size // out_data.size #é™ç»´è½´ä¸Šçš„å…ƒç´ æ•°ç›®
                # grad_broadcast = np.broadcast_to(out_data,self.shape)/n
                # self.grad += grad_broadcast
                if axis is None:
                    count = self.data.size
                else:
                    if  isinstance(axis, int):
                        count = self.data.shape[axis]
                    else:
                        count = np.prod([self.data.shape[ax] for ax in axis])  
                    if keepdims:
                        grad_expanded = out.grad
                    else:
                        if axis is None:
                            grad_expanded = np.reshape(out.grad, (1,)*self.ndim)
                        else:
                            axes = (axis,) if isinstance(axis,int) else axis
                            grad_expanded = out.grad
                            for ax in sorted(axes):
                                grad_expanded = np.expand_dims(grad_expanded, axis=ax)
                    # count = self.data.shape[axis] if isinstance(axis, int) else np.prod([self.data.shape[ax] for ax in axis])
                grad_board = np.broadcast_to(out.grad, self.shape)/count
                self.grad += grad_board
        out._backward = _backward
        out._parents = [self]
        return out
    
    # å˜å½¢ç®—å­ï¼šè·‘ä¸€éè¿›è¡Œè¿˜åŸ
    def reshape(self, *new_shape)->'Tensor':
        # å¤„ç†-1çš„æƒ…å†µ ï¼Ÿ
        # new_shape = tuple(new_shape)
        out_data = np.reshape(self.data, new_shape)
        out = Tensor(out_data, requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                self.grad += np.reshape(out.grad, self.shape)
        out._backward = _backward
        out._parents = [self]
        return out
    
    # åœ¨tensorçš„è¾¹ç¼˜æ’å…¥å¸¸æ•°å€¼0,ä½¿å¾—å°ºå¯¸å˜å¤§
    # å…¬å¼ï¼šout[i] = constant å½“ i è½åœ¨ pad åŒºåŸŸï¼›å¦åˆ™ out[i] = x[i - pad_left]
    # è¾“å…¥å¼ é‡ x å½¢çŠ¶ï¼š(dâ‚€, dâ‚, â€¦, d_{n-1})
    # pad_widthï¼š( (leftâ‚€, rightâ‚€), (leftâ‚, rightâ‚), â€¦ )
    # è¾“å‡º out å½¢çŠ¶ï¼š(dâ‚€+leftâ‚€+rightâ‚€, dâ‚+leftâ‚+rightâ‚, â€¦)
    # âˆ‚L/âˆ‚x[j] = Î£_{i âˆˆ pad_region(j)} âˆ‚L/âˆ‚out[i]
    # âˆ‚L/âˆ‚x[j] = âˆ‚L/âˆ‚out[j + pad_left]
    """
    x = Tensor([[1, 2], [3, 4]], requires_grad=True)
    y = x.pad(((1, 1), (1, 1)))   # 0 å¡«å……ä¸€åœˆ
    y.backward(Tensor(np.ones_like(y.data)))  # ä¸Šæ¸¸æ¢¯åº¦å…¨ 1
    """
    # pad_width = ((1, 1),   # ç¬¬ 0 è½´ï¼ˆè¡Œï¼‰: å‰(ä¸Š) 1ï¼Œå(ä¸‹) 1
    #         (1, 1))   # ç¬¬ 1 è½´ï¼ˆåˆ—ï¼‰: å‰(å·¦) 1ï¼Œå(å³) 1
    # å¤åˆ¶ç®—å­ï¼š å—æ±‚å’Œè¿˜åŸ
    def pad(self, pad_width, mode='constant', constant_values=0)->'Tensor': 
        """
        å¢å¼ºçš„padæ“ä½œï¼Œæ”¯æŒ4Då¼ é‡
        pad_width: ((å‰batch, åbatch), (å‰é€šé“, åé€šé“), (å‰é«˜, åé«˜), (å‰å®½, åå®½))
        """
        out_data = np.pad(self.data, pad_width, mode = mode, constant_values = constant_values)
        out = Tensor(out_data, requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                # å­¦ä¸€ä¸‹tupleå’Œsliceçš„ç”¨æ³•
                slices = tuple(slice(l,-r if r!= 0 else None) for l,r in pad_width)
                grad_crop = out.grad[slices]
                self.grad += grad_crop
        out._backward = _backward
        out._parents = [self]
        return out
    # ç»´åº¦äº¤æ¢ï¼Œäº¤æ¢ä¸¤ä¸ªè½´çš„é¡ºåº
    # å…¬å¼ï¼šout[i, j] = x[j, i]
    # å˜å½¢ç®—å­ï¼šè·‘ä¸€éè¿›è¡Œè¿˜åŸ
    # def transpose(self, axis1, axis2)->'Tensor':
    #     out_data = np.transpose(self.data, axes = (axis1,axis2))
    #     out = Tensor(out_data,requires_grad=True)
    #     def _backward():
    #         if self.grad is not None:
    #             grad_trans = np.transpose(out.grad, axes = (axis1,axis2))
    #             self.grad += grad_trans
    #     out._backward = _backward
    #     out._parents = [self]
    #     return out


    # æ”¯æŒå¤šè½´äº¤æ¢
    def transpose(self,*axes)->'Tensor':
        if not axes:
            #é»˜è®¤è½¬ç½®æ‰€æœ‰çš„è½´
            axes = tuple(reversed(range(self.ndim)))
        out_data = np.transpose(self.data, axes)
        out = Tensor(out_data, requires_grad=self.requires_grad)

        def _backward():
            if self.requires_grad and self.grad is not None:
                # è®¡ç®—é€†è½¬è´¨è½´
                inv_axes = np.argsort(axes)
                self.grad += np.transpose(out.grad ,inv_axes)
        out._backward = _backward
        out._parents = [self]
        return out

    # é€šè¿‡åˆ‡ç‰‡è·å¾—å­å¼ é‡
    def __getitem__(self, key)->'Tensor':
        out_data = self.data[key]
        out = Tensor(out_data,requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                np.add.at(self.grad, key, out.grad)
        out._backward = _backward
        out._parents = [self]
        return out
    
    # æŒ‰ç…§ç´¢å¼•å¯¹Tensor
    def slice(self, key)->'Tensor':
        out_data = self.data[key]
        out = Tensor(out_data, requires_grad=True)
        def _backward():
            if self.grad is not None:
                np.add.at(self.grad, key, out.grad)
        out._backward = _backward
        out._parents = [self]
        return out

    # åœ¨æŒ‡å®šçš„ä½ç½®æ’å…¥ä¸€ä¸ªé•¿åº¦ä¸º1çš„ç»´åº¦
    # å˜å½¢ç®—å­ï¼šè·‘ä¸€éè¿›è¡Œè¿˜åŸ
    def expand_dims(self, axis)->'Tensor':
        out_data = np.expand_dims(self.data, axis)
        out = Tensor(out_data,requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                grad_expand = np.squeeze(out.grad, axis=axis)
                self.grad += grad_expand
        out._backward = _backward
        out._parents = [self]
        return out       

    """
    repeats: int æˆ– list[int]ï¼Œæ¯è½´é‡å¤æ¬¡æ•°
    axis: æ²¿å“ªæ ¹è½´é‡å¤ï¼›None è¡¨ç¤ºæ‰å¹³åé‡å¤
    """
    # å¤åˆ¶ç®—å­ï¼š å—æ±‚å’Œè¿˜åŸ
    def repeat(self, repeats, axis)->'Tensor':
        repeats = np.array(repeats, dtype = int)
        out_data = np.repeat(self.data, repeats, axis=axis)
        out = Tensor(out_data,requires_grad=self.requires_grad)
        def _backward():
            if self.requires_grad and self.grad is not None:
                # æ„é€ æ¯ä¸ªå—çš„ç´¢å¼•
                indices = np.concatenate([[0],np.cumsum(repeats)])
                # æ²¿ç€æŒ‡å®šçš„è½´å¯¹æ¢¯åº¦å—æ±‚å’Œ
                grad_repeat = np.add.reduceat(out.grad, indices[:-1], axis=axis)
                self.grad += grad_repeat
        out._backward = _backward
        out._parents = [self]
        return out   

    def var(self, axis=None, keepdims=False, correction=1):
        """
        è®¡ç®—å¼ é‡çš„æ–¹å·®
        
        å‚æ•°:
            axis: æ²¿æŒ‡å®šè½´è®¡ç®—æ–¹å·®ï¼ŒNone è¡¨ç¤ºè®¡ç®—æ‰€æœ‰å…ƒç´ çš„æ–¹å·®
            keepdims: æ˜¯å¦ä¿æŒç»´åº¦
            correction: è‡ªç”±åº¦ä¿®æ­£ï¼Œé»˜è®¤ä¸º 1 (æ ·æœ¬æ–¹å·®)
                       0 è¡¨ç¤ºæ€»ä½“æ–¹å·®ï¼Œ1 è¡¨ç¤ºæ— åä¼°è®¡çš„æ ·æœ¬æ–¹å·®
        
        è¿”å›:
            æ–¹å·®å¼ é‡
        """
        # è®¡ç®—å‡å€¼
        mean_value = self.mean(axis=axis,keepdims=True)
        # è®¡ç®—å¹³æ–¹åå·®
        squarred_diff = (self-mean_value)**2

        # è®¡ç®—å¹³æ–¹åå·®çš„å‡å€¼
        if axis is None:
            # æ‰€æœ‰å…ƒç´ çš„æ–¹å·®
            n_elements = np.prod(self.shape)
            variance = squarred_diff.sum()/(n_elements - correction)
        else:
            # æŒ‡å®šè½´çš„æ–¹å·®
            if isinstance(axis, int):
                axis = (axis, ) # å°†å•ä¸ªè½´è½¬åŒ–ä¸ºå…ƒç»„ï¼Œç»Ÿä¸€å¤„ç†
            n_elements = 1
            for ax in axis:
                n_elements *= self.shape[ax] # è®¡ç®—æ²¿ç€æŒ‡å®šè½´çš„å…ƒç´ æ€»æ•°
            variance = squarred_diff.sum(axis = axis, keepdims = keepdims)/(n_elements - correction)
        return variance


    
    def std(self, axis=None, keepdims=False, correction=1):
        """
        è®¡ç®—å¼ é‡çš„æ ‡å‡†å·®
        
        å‚æ•°:
            axis: æ²¿æŒ‡å®šè½´è®¡ç®—æ ‡å‡†å·®ï¼ŒNone è¡¨ç¤ºè®¡ç®—æ‰€æœ‰å…ƒç´ çš„æ ‡å‡†å·®
            keepdims: æ˜¯å¦ä¿æŒç»´åº¦
            correction: è‡ªç”±åº¦ä¿®æ­£ï¼Œé»˜è®¤ä¸º 1 (æ ·æœ¬æ ‡å‡†å·®)
        
        è¿”å›:
            æ ‡å‡†å·®å¼ é‡
        """
        variance = self.var(axis=axis, keepdims=keepdims, correction=correction)
        return variance ** 0.5

    def zero_grad(self):
        if self.grad is not None:
            self.grad.fill(0.0) #åŸåœ°æ¸…é›¶ï¼Œæ¯”åˆ›å»ºä¸€ä¸ªæ•°ç»„æ›´åŠ çš„é«˜æ•ˆ
        return self

    def max(self, axis=None, keepdims=False) -> 'Tensor':
        current_axis = axis
        out_data = np.max(self.data, axis=current_axis, keepdims=keepdims)
        argmax_val = np.argmax(self.data, axis=current_axis, keepdims=keepdims)
        out = Tensor(out_data, requires_grad=self.requires_grad)
        
        def _backward():
            nonlocal argmax_val, current_axis
            if self.requires_grad:
                if self.grad is None:
                    self.grad = np.zeros_like(self.data)
                
                grad_mask = np.zeros_like(self.data)
                
                if current_axis is None:
                    # å…¨å±€æœ€å¤§å€¼
                    grad_mask.flat[argmax_val] = out.grad
                else:
                    if isinstance(current_axis, (tuple,list)):
                        raise NotImplementedError("not support multiply axis min, by yrq, in core min")
                    
                    # æ‰‹åŠ¨å®ç°put_along_axisçš„é€»è¾‘
                    if keepdims:
                        indices = argmax_val
                    else:
                        # ä¸ºargmax_valæ·»åŠ è¢«å‹ç¼©çš„ç»´åº¦
                        indices = np.expand_dims(argmax_val, axis=current_axis)
                    
                    # æ‰‹åŠ¨åˆ›å»ºç´¢å¼•å¹¶èµ‹å€¼
                    indices_tuple = []
                    for i in range(self.data.ndim):
                        if i == current_axis:
                            indices_tuple.append(indices)
                        else:
                            # åˆ›å»ºå¹¿æ’­ç´¢å¼•
                            idx = np.arange(self.data.shape[i])
                            shape = [1] * self.data.ndim
                            shape[i] = self.data.shape[i]
                            idx = idx.reshape(shape)
                            indices_tuple.append(idx)
                    
                    # ç¡®ä¿out.gradçš„å½¢çŠ¶ä¸ç´¢å¼•é€‰æ‹©çš„å½¢çŠ¶åŒ¹é…
                    selected_shape = indices_tuple[0].shape
                    if out.grad.size == 1:
                        # æ ‡é‡æƒ…å†µ
                        grad_value = out.grad
                    else:
                        # è°ƒæ•´out.gradçš„å½¢çŠ¶
                        grad_reshaped = out.grad.reshape(selected_shape)
                        grad_value = grad_reshaped
                    
                    # ä½¿ç”¨é«˜çº§ç´¢å¼•èµ‹å€¼
                    grad_mask[tuple(indices_tuple)] = grad_value
                
                self.grad += grad_mask
        
        out._backward = _backward
        out._parents = [self]
        return out

    def min(self, axis=None, keepdims=False) -> 'Tensor':
            current_axis = axis
            out_data = np.min(self.data, axis=current_axis, keepdims=keepdims)
            argmin_val = np.argmin(self.data, axis=current_axis, keepdims=keepdims)
            out = Tensor(out_data, requires_grad=self.requires_grad)
            
            def _backward():
                nonlocal argmin_val, current_axis
                if self.requires_grad:
                    if self.grad is None:
                        self.grad = np.zeros_like(self.data)
                    
                    grad_mask = np.zeros_like(self.data)
                    
                    if current_axis is None:
                        # å…¨å±€æœ€å°å€¼
                        grad_mask.flat[argmin_val] = out.grad
                    else:
                        if isinstance(current_axis, (tuple,list)):
                            raise NotImplementedError("not support multiply axis min, by yrq, in core min")
                        # æ‰‹åŠ¨å®ç°put_along_axisçš„é€»è¾‘
                        if keepdims:
                            indices = argmin_val
                        else:
                            # ä¸ºargmin_valæ·»åŠ è¢«å‹ç¼©çš„ç»´åº¦
                            indices = np.expand_dims(argmin_val, axis=current_axis)
                        
                        # æ‰‹åŠ¨åˆ›å»ºç´¢å¼•å¹¶èµ‹å€¼
                        indices_tuple = []
                        for i in range(self.data.ndim):
                            if i == current_axis:
                                indices_tuple.append(indices)
                            else:
                                # åˆ›å»ºå¹¿æ’­ç´¢å¼•
                                idx = np.arange(self.data.shape[i])
                                shape = [1] * self.data.ndim
                                shape[i] = self.data.shape[i]
                                idx = idx.reshape(shape)
                                indices_tuple.append(idx)
                        
                        # ç¡®ä¿out.gradçš„å½¢çŠ¶ä¸ç´¢å¼•é€‰æ‹©çš„å½¢çŠ¶åŒ¹é…
                        selected_shape = indices_tuple[0].shape
                        if out.grad.size == 1:
                            # æ ‡é‡æƒ…å†µ
                            grad_value = out.grad
                        else:
                            # è°ƒæ•´out.gradçš„å½¢çŠ¶
                            grad_reshaped = out.grad.reshape(selected_shape)
                            grad_value = grad_reshaped
                        
                        # ä½¿ç”¨é«˜çº§ç´¢å¼•èµ‹å€¼
                        grad_mask[tuple(indices_tuple)] = grad_value
                    
                    self.grad += grad_mask
            
            out._backward = _backward
            out._parents = [self]
            return out

    @property
    def T(self)->'Tensor':
        if self.ndim !=2:
            raise ValueError(f"T only used in 2 dim, the current dim is {self.ndim}")
        return self.transpose(1,0)

    # æ˜¯pythonçš„å†…ç½®çš„æ–¹æ³•,è¿”å›å¯¹è±¡å ç”¨çš„å†…å­˜å­—èŠ‚æ•°ï¼Œå’Œå…ƒç´ ä¸ªæ•°æ— å…³
    # @property
    # def __sizeof__(self):
    #     return self.data.size
    
    @property
    def size(self):
        return self.data.size
    
    # ===== åå‘ä¼ æ’­å…¥å£ =====
    def backward(self, grad_output=None): 
    # å†…éƒ¨ï¼šæ‹“æ‰‘æ’åº + é“¾å¼å›è°ƒ
        
        if grad_output is None:
            # è‹¥å½“å‰å¼ é‡æ˜¯æ ‡é‡ï¼ˆshape=()ï¼‰ï¼Œé»˜è®¤æ¢¯åº¦ä¸º 1.0
            if self.data.ndim == 0:
                # grad_output = np.ones_like(self.data)
                grad_output = np.array(1.0,dtype = np.float32)
            else:
                raise ValueError("grad_output must be provided for non-scalar tensors, written by yrq, in backward")
        else:
            # ç¡®ä¿grad_output æ˜¯numpyæ•°ç»„ä¸”å½¢çŠ¶åŒ¹é…
            grad_output = np.asarray(grad_output, dtype=np.float32)
            if grad_output.shape!=self.shape:
                raise ValueError(f"the shape {grad_output.shape} and shape {self.shape} is not mathch, written by yrq, in backward")
        topo = []
        visited = set()
        # æ‹“æ‰‘æ’åºï¼ˆDFSé€†åºï¼‰
        def build_topo(v):
            if id(v) not in visited:
                visited.add(id(v))
                for parent in v._parents:
                    build_topo(parent)
                topo.append(v)
        build_topo(self)

        # å­˜åœ¨0-Dæ ‡é‡ä¸èƒ½ä½¿ç”¨[:]èµ‹å€¼
        # if self.grad is None:
        #     self.grad = np.zeros_like(self.data)
        # self.grad[:] = grad_output #ä½¿ç”¨åˆ‡ç‰‡èµ‹å€¼ä¿è¯å½¢çŠ¶
        if self.grad is None or self.grad.shape != self.data.shape:
            self.grad = np.empty_like(self.data) # é¦–æ¬¡æˆ–è€…å½¢çŠ¶å˜åŒ–çš„æ—¶å€™é‡æ–°åˆ†é…
        np.copyto(self.grad, grad_output) #æ”¯æŒ0-D -> N-Då¹¿æ’­

        for node in reversed(topo):
            node._backward()
        return self
    
# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰
    def check_grad(computed, expected, eps=1e-5):
        assert np.allclose(computed, expected, atol=eps), \
            f"æ¢¯åº¦ä¸åŒ¹é…: è®¡ç®—å€¼={computed}ï¼ŒæœŸæœ›å€¼={expected}"

    print("===== å¼€å§‹ 1. åŸºç¡€è¿ç®—æµ‹è¯•ï¼ˆ+ã€-ã€*ã€/ï¼‰ =====")
    # æµ‹è¯•åŠ æ³•
    a = Tensor([2.0, 3.0], requires_grad=True)
    b = Tensor([4.0, 5.0], requires_grad=True)
    c = a + b
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [1.0, 1.0])
    check_grad(b.grad, [1.0, 1.0])
    a.zero_grad()
    b.zero_grad()

    # æµ‹è¯•å‡æ³•
    c = a - b
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [1.0, 1.0])
    check_grad(b.grad, [-1.0, -1.0])
    a.zero_grad()
    b.zero_grad()

    # æµ‹è¯•ä¹˜æ³•
    c = a * b
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [4.0, 5.0])  # bçš„å€¼
    check_grad(b.grad, [2.0, 3.0])  # açš„å€¼
    a.zero_grad()
    b.zero_grad()

    # æµ‹è¯•é™¤æ³•
    c = a / b
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [1/4, 1/5])          # 1/b
    check_grad(b.grad, [-2/(4**2), -3/(5**2)])  # -a/bÂ²
    a.zero_grad()
    b.zero_grad()

    print("===== 2. å¹¿æ’­æµ‹è¯• =====")
    # å¹¿æ’­åŠ æ³•
    a = Tensor([[1.0, 2.0]], requires_grad=True)  # å½¢çŠ¶(1,2)
    b = Tensor([[3.0], [4.0]], requires_grad=True)  # å½¢çŠ¶(2,1)
    c = a + b  # å½¢çŠ¶(2,2)
    c.backward(np.ones((2, 2)))
    check_grad(a.grad, [[2.0, 2.0]])  # æ²¿0è½´æ±‚å’Œ
    check_grad(b.grad, [[2.0], [2.0]])  # æ²¿1è½´æ±‚å’Œ
    a.zero_grad()
    b.zero_grad()

    # å¹¿æ’­ä¹˜æ³•
    c = a * b
    c.backward(np.ones((2, 2)))
    check_grad(a.grad, [[3+4, 3+4]])  # bçš„å’Œ
    check_grad(b.grad, [[1+2], [1+2]])  # açš„å’Œ
    a.zero_grad()
    b.zero_grad()

    print("===== 3. çŸ©é˜µä¹˜æ³•æµ‹è¯• =====")
    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)  # (2,2)
    b = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)  # (2,2)
    c = a @ b
    c.backward(np.ones((2, 2)))
    # éªŒè¯açš„æ¢¯åº¦ï¼šones @ b.T
    expected_a_grad = np.ones((2,2)) @ b.data.T
    check_grad(a.grad, expected_a_grad)
    # éªŒè¯bçš„æ¢¯åº¦ï¼ša.T @ ones
    expected_b_grad = a.data.T @ np.ones((2,2))
    check_grad(b.grad, expected_b_grad)
    a.zero_grad()
    b.zero_grad()

    print("===== 4. æ¿€æ´»å‡½æ•°æµ‹è¯•ï¼ˆexpã€logï¼‰ =====")
    # æµ‹è¯•exp
    a = Tensor([1.0, 2.0], requires_grad=True)
    c = a.exp()
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, np.exp([1.0, 2.0]))  # exp(x)çš„å¯¼æ•°æ˜¯è‡ªèº«
    a.zero_grad()

    # æµ‹è¯•log
    a = Tensor([2.0, 3.0], requires_grad=True)
    c = a.log()
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [1/2, 1/3])  # log(x)çš„å¯¼æ•°æ˜¯1/x
    a.zero_grad()

    print("===== 5. èšåˆ èšåˆå‡½æ•°æµ‹è¯•ï¼ˆsumã€meanï¼‰ =====")
    # æµ‹è¯•sum
    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    c = a.sum(axis=0)  # æ²¿0è½´æ±‚å’Œ
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, np.ones((2, 2)))  # å¹¿æ’­åå…¨ä¸º1
    a.zero_grad()

    # æµ‹è¯•mean
    c = a.mean(axis=1, keepdims=True)  # æ²¿1è½´æ±‚å¹³å‡
    c.backward(np.ones((2, 1)))
    expected_grad = np.ones((2, 2)) * (1/2)  # å¹³å‡æ¢¯åº¦
    check_grad(a.grad, expected_grad)
    a.zero_grad()

    print("Testing no_grad context manager...")
    # æ­£å¸¸æƒ…å†µ
    x1 = Tensor([1, 2, 3], requires_grad=True)
    assert x1.requires_grad == True, "æ­£å¸¸æƒ…å†µ requires_grad åº”è¯¥ä¸º True"
    
    # åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­
    with Tensor.no_grad():
        x2 = Tensor([1, 2, 3], requires_grad=True)
        assert x2.requires_grad == False, "no_grad ä¸­ requires_grad åº”è¯¥ä¸º False"
        
        # æ£€æŸ¥å…¨å±€çŠ¶æ€
        assert Tensor.is_grad_enabled() == False, "no_grad ä¸­å…¨å±€çŠ¶æ€åº”è¯¥ä¸º False"
    
    # ç¦»å¼€ no_grad åæ¢å¤
    assert Tensor.is_grad_enabled() == True, "ç¦»å¼€ no_grad åå…¨å±€çŠ¶æ€åº”è¯¥æ¢å¤"
    
    x3 = Tensor([1, 2, 3], requires_grad=True)
    assert x3.requires_grad == True, "ç¦»å¼€ no_grad å requires_grad åº”è¯¥æ¢å¤"
    

  
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    x = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)
    
    # æµ‹è¯•å‡å€¼
    mean_all = x.mean()
    assert np.allclose(mean_all.data, 3.5), "å…¨å±€å‡å€¼è®¡ç®—é”™è¯¯"
    
    mean_axis0 = x.mean(axis=0)
    assert np.allclose(mean_axis0.data, [2.5, 3.5, 4.5]), "æ²¿è½´0å‡å€¼è®¡ç®—é”™è¯¯"
    
    mean_axis1 = x.mean(axis=1)
    assert np.allclose(mean_axis1.data, [2, 5]), "æ²¿è½´1å‡å€¼è®¡ç®—é”™è¯¯"
    
    # æµ‹è¯•æ–¹å·®
    var_all = x.var()
    expected_var = np.var([[1, 2, 3], [4, 5, 6]], ddof=1)  # æ ·æœ¬æ–¹å·®
    assert np.allclose(var_all.data, expected_var), "å…¨å±€æ–¹å·®è®¡ç®—é”™è¯¯"
    
    var_axis0 = x.var(axis=0)
    expected_var_axis0 = np.var([[1, 2, 3], [4, 5, 6]], axis=0, ddof=1)
    assert np.allclose(var_axis0.data, expected_var_axis0), "æ²¿è½´0æ–¹å·®è®¡ç®—é”™è¯¯"
    
    # æµ‹è¯•æ ‡å‡†å·®
    std_all = x.std()
    expected_std = np.std([[1, 2, 3], [4, 5, 6]], ddof=1)
    assert np.allclose(std_all.data, expected_std), "å…¨å±€æ ‡å‡†å·®è®¡ç®—é”™è¯¯"
    

    print("===== 6. å½¢çŠ¶æ“ä½œæµ‹è¯•ï¼ˆreshapeã€transposeã€expand_dimsï¼‰ =====")
    # æµ‹è¯•reshape
    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    c = a.reshape(1, 4)
    c.backward(np.array([[1.0, 1.0, 1.0, 1.0]]))
    check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦å½¢çŠ¶è¿˜åŸ
    a.zero_grad()

    # æµ‹è¯•transpose
    c = a.transpose(1, 0)  # è½¬ç½®
    c.backward(np.ones((2, 2)))
    check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦ä¹Ÿè½¬ç½®å›æ¥
    a.zero_grad()

    # æµ‹è¯•expand_dims
    c = a.expand_dims(axis=0)  # æ–°å¢0è½´
    c.backward(np.ones((1, 2, 2)))
    check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦æŒ¤å‹æ–°å¢ç»´åº¦
    a.zero_grad()

    print("===== 7. åˆ‡ç‰‡ä¸padæµ‹è¯• =====")
    # æµ‹è¯•åˆ‡ç‰‡
    a = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
    c = a[1:3]  # å–ç´¢å¼•1å’Œ2
    c.backward(np.array([1.0, 1.0]))
    check_grad(a.grad, [0.0, 1.0, 1.0, 0.0])  # åˆ‡ç‰‡ä½ç½®æ¢¯åº¦ä¸º1
    a.zero_grad()

    # æµ‹è¯•pad
    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    c = a.pad(((1, 1), (1, 1)))  # å››å‘¨å„pad 1åœˆ
    c.backward(np.ones((4, 4)))
    check_grad(a.grad, np.ones((2, 2)))  # ä¸­é—´åŒºåŸŸæ¢¯åº¦ä¸º1
    a.zero_grad()

    print("=== æµ‹è¯•4Då¼ é‡å¡«å…… ===")
    
    # åˆ›å»º4Då¼ é‡ (batch, channels, height, width)
    x = Tensor(np.ones((2, 3, 4, 4)), requires_grad=True)
    print(f"åŸå§‹å½¢çŠ¶: {x.shape}")
    
    # 4Då¡«å……: ((batchå‰, batchå), (é€šé“å‰, é€šé“å), (é«˜åº¦å‰, é«˜åº¦å), (å®½åº¦å‰, å®½åº¦å))
    pad_width = ((0, 0), (0, 0), (1, 1), (1, 1))  # åœ¨é«˜åº¦å’Œå®½åº¦ä¸Šå„å¡«å……1
    
    # åº”ç”¨å¡«å……
    x_padded = x.pad(pad_width)
    print(f"å¡«å……åå½¢çŠ¶: {x_padded.shape}")
    
    # éªŒè¯å½¢çŠ¶
    expected_shape = (2, 3, 6, 6)  # 4+1+1=6, 4+1+1=6
    assert x_padded.shape == expected_shape, f"æœŸæœ› {expected_shape}, å®é™… {x_padded.shape}"
    print("âœ“ å½¢çŠ¶æ­£ç¡®")
    
    # éªŒè¯å¡«å……å€¼
    # ä¸­é—´åŒºåŸŸåº”è¯¥æ˜¯åŸå§‹æ•°æ®ï¼Œè¾¹ç•Œåº”è¯¥æ˜¯0
    center_region = x_padded.data[:, :, 1:5, 1:5]  # å»é™¤è¾¹ç•Œ
    assert np.allclose(center_region, 1.0), "ä¸­å¿ƒåŒºåŸŸå€¼ä¸æ­£ç¡®"
    print("âœ“ å¡«å……å€¼æ­£ç¡®")
    
    # æµ‹è¯•æ¢¯åº¦ä¼ æ’­
    loss = x_padded.sum()
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦å½¢çŠ¶
    assert x.grad.shape == x.shape, "æ¢¯åº¦å½¢çŠ¶ä¸æ­£ç¡®"
    print("âœ“ æ¢¯åº¦å½¢çŠ¶æ­£ç¡®")
    
    # æ£€æŸ¥æ¢¯åº¦å€¼ - åº”è¯¥åªæœ‰ä¸­å¿ƒåŒºåŸŸæœ‰æ¢¯åº¦
    expected_grad = np.ones((2, 3, 4, 4))
    assert np.allclose(x.grad, expected_grad), "æ¢¯åº¦å€¼ä¸æ­£ç¡®"
    print("âœ“ æ¢¯åº¦å€¼æ­£ç¡®")
    
    print("ğŸ‰ 4Då¡«å……æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")

    print("===== 8. é‡å¤æ“ä½œæµ‹è¯•ï¼ˆrepeatï¼‰ =====")
    a = Tensor([[1.0, 2.0]], requires_grad=True)
    c = a.repeat(repeats=2, axis=0)  # æ²¿0è½´é‡å¤2æ¬¡
    c.backward(np.array([[1.0, 1.0], [1.0, 1.0]]))
    check_grad(a.grad, [[2.0, 2.0]])  # é‡å¤åŒºåŸŸæ¢¯åº¦æ±‚å’Œ
    a.zero_grad()

    print("===== 9. æå€¼æµ‹è¯•ï¼ˆmaxã€minï¼‰ =====")
    # æµ‹è¯•max
    a = Tensor([[3.0, 1.0], [2.0, 4.0]], requires_grad=True)
    c = a.max(axis=1)  # æ²¿1è½´å–æœ€å¤§å€¼
    c.backward(np.array([1.0, 1.0]))
    expected_max_grad = np.zeros((2, 2))
    expected_max_grad[0, 0] = 1.0  # ç¬¬0è¡Œæœ€å¤§å€¼ä½ç½®
    expected_max_grad[1, 1] = 1.0  # ç¬¬1è¡Œæœ€å¤§å€¼ä½ç½®
    check_grad(a.grad, expected_max_grad)
    a.zero_grad()

    # æµ‹è¯•min
    c = a.min(axis=1)  # æ²¿1è½´å–æœ€å°å€¼
    c.backward(np.array([1.0, 1.0]))
    expected_min_grad = np.zeros((2, 2))
    expected_min_grad[0, 1] = 1.0  # ç¬¬0è¡Œæœ€å°å€¼ä½ç½®
    expected_min_grad[1, 0] = 1.0  # ç¬¬1è¡Œæœ€å°å€¼ä½ç½®
    check_grad(a.grad, expected_min_grad)
    a.zero_grad()

    print("===== 10. é“¾å¼ä¼ æ’­æµ‹è¯• =====")
    # å¤æ‚è®¡ç®—é“¾ï¼šz = (x*y + exp(x)) / mean(y)
    x = Tensor([2.0, 3.0], requires_grad=True)
    y = Tensor([4.0, 5.0], requires_grad=True)
    z = (x * y + x.exp()) / y.mean()
    z.backward(np.array([1.0, 1.0]))
    
    
    n = y.size
    mean_y = y.data.mean()
    c_sum = (x.data * y.data + np.exp(x.data)).sum()
    dx_expected = (y.data + np.exp(x.data)) / mean_y
    dy_expected = (x.data / mean_y) - c_sum / (mean_y**2 * n)
    
    check_grad(x.grad, dx_expected)
    check_grad(y.grad, dy_expected)

    print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")