from .tensor import Tensor
import numpy as np
from typing import List, Union
import warnings

# å½“å‰æ–‡ä»¶æµ‹è¯•çš„æ—¶å€™å–æ¶ˆä¸‹é¢æ³¨é‡Š
# from tensor import Tensor
class Optimizer:
    """
    ä¼˜åŒ–å™¨çš„åŸºç±»
    æä¾›å‚æ•°ç®¡ç†ï¼Œæ¢¯åº¦å¤„ç†å’Œæ•°ç»„ç¨³å®šæ€§æ£€æŸ¥
    """
    def __init__(self, params: List, lr: float = 1e-3):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        å‚æ•°:
            params: éœ€è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨
            lr: å­¦ä¹ ç‡
        """
        self.params = params
        self.lr = lr
        
        # é²æ£’æ€§æ£€æŸ¥
        self._validate_params()
        
        # åˆå§‹åŒ–çŠ¶æ€å­—å…¸ï¼ˆå­ç±»å¯ä»¥æ‰©å±•ï¼‰
        self.state = {}

    def _validate_params(self) -> None:
        """éªŒè¯å‚æ•°åˆ—è¡¨"""
        if not self.params:
            raise ValueError("param is empty, by yrq")
        
        for i, param in enumerate(self.params):
            if not hasattr(param, 'data'):
                raise ValueError(f"param {i} don't have attribute data, by yrq")
            if not hasattr(param, 'grad'):
                raise ValueError(f"param {i} don't have attribute grad, by yrq")
            
    # å®‰å…¨å°†æ¢¯åº¦å¹¿æ’­åˆ°ç›®æ ‡
    def _safe_broadcast_grad(self, grad:np.ndarray, target_shape:tuple)->np.ndarray:
        # å¦‚æœå½¢çŠ¶ç›¸åŒï¼Œç›´æ¥è¿”å›
        if grad.shape == target_shape:
            return grad
        
        #å°è¯•è¿›è¡Œå¹¿æ’­
        try:
            #ç›´æ¥å¹¿æ’­
            broad_grad = np.broadcast_to(grad, target_shape)
            return broad_grad
        except ValueError:
            # å¦‚æœå¹¿æ’­å¤±è´¥ï¼Œå°è¯•æ±‚å’Œåˆ°ç›®æ ‡çš„å½¢çŠ¶
            # æ‰¾åˆ°éœ€è¦çš„æ±‚å’Œçš„è½´
            axes = self._get_sum_axis(grad, target_shape)
            if axes:
                summed_grad = grad.sum(axis = axes, keepdims = True)
                #å†æ¬¡å°è¯•å¹¿æ’­
                return np.broadcast_to(summed_grad, target_shape)
            else:
                raise ValueError(f"can't broadcast from {grad.shape} to {target_shape}, by yrq")
                
    def _get_sum_axis(self, grad_shape:tuple, target_shape:tuple)->tuple:
        """
        è®¡ç®—æ±‚å’Œéœ€è¦çš„è½´
        å‚æ•°ï¼š
            grad_shape:æ¢¯åº¦å½¢çŠ¶
            target_shape:ç›®æ ‡å½¢çŠ¶

        å¹¿æ’­è§„åˆ™ï¼šä»å³å‘å·¦æ¯”è¾ƒç»´åº¦
        - å¦‚æœç»´åº¦ç›¸ç­‰ï¼Œæˆ–å…¶ä¸­ä¸€ä¸ªä¸º1ï¼Œå¯ä»¥å¹¿æ’­
        - å¦‚æœæ¢¯åº¦ç»´åº¦>1ä¸”ç›®æ ‡ç»´åº¦=1ï¼Œéœ€è¦åœ¨è¿™ä¸ªè½´æ±‚å’Œ
        - å¦‚æœæ¢¯åº¦ç»´åº¦=1ä¸”ç›®æ ‡ç»´åº¦>1ï¼Œå¯ä»¥ç›´æ¥å¹¿æ’­

        è¿”å›ï¼š
            éœ€è¦æ±‚å’Œçš„è½´
        """

        # ç¡®ä¿è¾“å…¥æ˜¯å…ƒç»„
        if not isinstance(grad_shape, tuple):
            grad_shape = grad_shape.shape if hasattr(grad_shape, 'shape') else tuple(grad_shape)
        if not isinstance(target_shape, tuple):
            target_shape = target_shape.shape if hasattr(target_shape, 'shape') else tuple(target_shape)
        
        grad_ndim = len(grad_shape)
        target_ndim = len(target_shape)

        # å¦‚æœæ¢¯åº¦ç»´åº¦æ›´å¤šï¼Œå‰å‡ ä¸ªç»´åº¦éœ€è¦æ±‚å’Œ
        if grad_ndim>target_ndim:
            return tuple(range(grad_ndim - target_ndim))
        
        # å¦åˆ™æ‰¾åˆ°éœ€è¦æ±‚å’Œçš„è½´ï¼Œæ¢¯åº¦ä¸º1ä½†æ˜¯ç›®æ ‡ä¸­æ²¡æœ‰1çš„è½´
        axes = []

        min_ndim = min(target_ndim, grad_ndim)

        # ä»å³è¾¹å¼€å§‹å¹¿æ’­
        for i in range(1, min_ndim + 1):
            grad_dim = grad_shape[-i]
            target_dim = target_shape[-i]

            if grad_dim != 1 and target_dim ==1:
                axes.append(grad_ndim - i)

        return tuple(axes)
    
    def _check_numerical_stability(self, array: np.ndarray, name: str) -> bool:
        """
        æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        
        å‚æ•°:
            array: è¦æ£€æŸ¥çš„æ•°ç»„
            name: æ•°ç»„åç§°ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰
            
        è¿”å›:
            Trueå¦‚æœç¨³å®šï¼ŒFalseå¦‚æœä¸ç¨³å®š
        """
        if np.any(np.isnan(array)):
            warnings.warn("{name} contain nan, by yrq")
            return False
        if np.any(np.isinf(array)):
            warnings.warn("{name} contain inf, by yrq")
            return False
        return True
    
    def _clip_if_large(self, array: np.ndarray, threshold: float = 1e6) -> np.ndarray:
        """
        å¦‚æœæ•°ç»„å€¼è¿‡å¤§åˆ™è¿›è¡Œè£å‰ª
        
        å‚æ•°:
            array: è¦æ£€æŸ¥çš„æ•°ç»„
            threshold: é˜ˆå€¼
            
        è¿”å›:
            è£å‰ªåçš„æ•°ç»„
        """
        if np.max(np.abs(array)) > threshold:
            warnings.warn("the aray size is large need clip, by yrq")
            return np.clip(array, -threshold, threshold)
        return array

    def step(self)->None:
        """
        æ‰§è¡Œä¸€æ­¥å‚æ•°æ›´æ–°
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°stepçš„æ–¹æ³•")
    

    #æ¸…ç©ºæ‰€æœ‰çš„å‚æ•°ç¼“å­˜
    def zero_grad(self) -> None: 
        for param in self.params:
            # if param.grad is not None:
            #     param.zero_grad()
            if hasattr(param, 'zero_grad'):
                param.zero_grad()
            elif param.grad is not None:
                param.grad.fill(0.0)

class GD:...


class SGD(Optimizer):
    def __init__(self, params:List, lr: float = 1e-3, momentum: float = 0.0):
        super().__init__(params, lr)
        self.momentum = momentum
        # åˆå§‹åŒ–é€Ÿåº¦ç¼“å­˜
        for i,param in enumerate(self.params):
            self.state[f'velocity_{i}'] = np.zeros_like(param.data)

    def step(self):
        for i,param in enumerate(self.params):
            if param.grad is None:
                continue
            if param.grad.shape != param.data.shape:
                grad = self._safe_broadcast_grad(param.grad, param.data.shape)
            else:
                grad = param.grad
            
            if not self._check_numerical_stability(grad, f"the param {i} grad"):
                continue

            # è·å–é€Ÿåº¦
            velocity = self.state[f'velocity_{i}']
            
            # æ›´æ–°åŠ¨é‡
            velocity = self.momentum * velocity - self.lr * grad
            velocity = self._clip_if_large(velocity)
            
            # ä¿å­˜æ›´æ–°åçš„é€Ÿåº¦
            self.state[f'velocity_{i}'] = velocity
            
            # æ›´æ–°å‚æ•°
            param.data += velocity
            
            # æ£€æŸ¥å‚æ•°ç¨³å®šæ€§
            self._check_numerical_stability(param.data, f"param {i}")
    
class Momentum(SGD):
    """
    åŠ¨é‡ä¼˜åŒ–å™¨
    
    å‚æ•°:
        params: éœ€è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨ [Tensor]
        lr: å­¦ä¹ ç‡ (float)
        momentum: åŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸è®¾ä¸º0.9 (float)
    """
    def __init__(self,  params: list[Tensor], lr: float, momentum: float = 0.9):
        super().__init__(params, lr, momentum)

class AdaGrad(Optimizer):
    """
    AdaGradä¼˜åŒ–å™¨ - è‡ªé€‚åº”å­¦ä¹ ç‡
    
    å‚æ•°:
        params: éœ€è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨ [Tensor]
        lr: å­¦ä¹ ç‡ (float)ï¼Œé€šå¸¸è®¾ä¸º0.01
        eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶ (float)
    """
    def __init__(self, params: list[Tensor], lr: float, eps: float = 1e-8): 
        super().__init__(params, lr)
        self.eps = eps
        # åˆå§‹åŒ–é€Ÿåº¦ç¼“å­˜
        for i,param in enumerate(self.params):
            self.state[f'cache_{i}'] = np.zeros_like(param.data)

    def step(self):
        for i,param in enumerate(self.params):

            if param.grad is None:
                continue
            if param.grad.shape != param.data.shape:
                grad = self._safe_broadcast_grad(param.grad, param.data.shape)
            else:
                grad = param.grad
            
            if not self._check_numerical_stability(grad, f"the param {i} grad"):
                continue

            # è·å–cache
            cache = self.state[f'cache_{i}']
            
            # æ›´æ–°ç¼“å­˜
            cache += grad**2
            cache = self._clip_if_large(cache)

            # ä¿å­˜æ›´æ–°ä¹‹åçš„ç¼“å­˜
            self.state[f'cache_{i}'] = cache

            #è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡
            cache_sqrt = np.sqrt(cache) + self.eps
            cache_sqrt = self._clip_if_large(cache_sqrt)
            
            # æ›´æ–°å‚æ•°
            param.data -= self.lr * param.grad/cache_sqrt

            # æ£€æŸ¥å‚æ•°ç¨³å®šæ€§
            self._check_numerical_stability(param.data, f"param {i}")


class Adam(Optimizer):
    """
    Adamä¼˜åŒ–å™¨ - ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
    
    å‚æ•°:
        params: éœ€è¦ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨ [Tensor]
        lr: å­¦ä¹ ç‡ (float)ï¼Œé€šå¸¸è®¾ä¸º1e-3
        beta1: ä¸€é˜¶çŸ©ä¼°è®¡çš„è¡°å‡ç‡ (float)
        beta2: äºŒé˜¶çŸ©ä¼°è®¡çš„è¡°å‡ç‡ (float)  
        eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•° (float)
    """
    
    def __init__(self, params: list, lr: float = 1e-3, \
                 beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):
        # self.params = params
        # self.lr = lr
        # self.beta1 = beta1  # ä¸€é˜¶çŸ©è¡°å‡ç‡
        # self.beta2 = beta2  # äºŒé˜¶çŸ©è¡°å‡ç‡
        # self.eps = eps
        # self.t = 0  # æ—¶é—´æ­¥
        
        # # åˆå§‹åŒ–çŸ©ä¼°è®¡
        # self.m = []  # ä¸€é˜¶çŸ©ï¼ˆç±»ä¼¼åŠ¨é‡ï¼‰
        # self.v = []  # äºŒé˜¶çŸ©ï¼ˆç±»ä¼¼AdaGradçš„ç¼“å­˜ï¼‰
        
        # for param in self.params:
        #     self.m.append(np.zeros_like(param.data))
        #     self.v.append(np.zeros_like(param.data))
        super().__init__(params, lr)
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.t = 0 #æ—¶é—´æ­¥

        for i,param in enumerate(self.params):
            self.state[f'm_{i}'] = np.zeros_like(param.data) #ä¸€é˜¶çŸ©
            self.state[f'v_{i}'] = np.zeros_like(param.data) #äºŒé˜¶çŸ©

    def step(self) -> None:
        """
        æ‰§è¡Œä¸€æ­¥å‚æ•°æ›´æ–°
        æ›´æ–°å…¬å¼:
            t = t + 1
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad^2
            m_hat = m / (1 - beta1^t)
            v_hat = v / (1 - beta2^t)
            param -= lr * m_hat / (sqrt(v_hat) + eps)
        """
        self.t += 1  # æ›´æ–°æ—¶é—´æ­¥
        

        for i,param in enumerate(self.params):
            if param.grad is None:
                continue
            if param.grad.shape != param.data.shape:
                grad = self._safe_broadcast_grad(param.grad, param.data.shape)
            else:
                grad = param.grad
            
            if not self._check_numerical_stability(grad, f"the param {i} grad"):
                continue

            # è·å–m,v
            m = self.state[f'm_{i}']
            v = self.state[f'v_{i}']

            # æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆå¸¦åå·®ï¼‰
            m = self.beta1 * m + (1 - self.beta1) * grad
            m = self._clip_if_large(m)
            # æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆå¸¦åå·®ï¼‰
            v = self.beta2 * v + (1 - self.beta2) * (grad ** 2)
            v = self._clip_if_large(v)

            self.state[f'm_{i}'] = m
            self.state[f'v_{i}'] = v

            # è®¡ç®—åå·®ä¿®æ­£åçš„ä¼°è®¡
            m_hat = m / (1 - self.beta1 ** self.t)
            v_hat = v / (1 - self.beta2 ** self.t)
            
            # æ›´æ–°å‚æ•°
            v_hat_sqrt = np.sqrt(v_hat)
            if np.any(v_hat_sqrt<self.eps):
                v_hat_sqrt = np.maximum(v_hat_sqrt,self.eps)
            v_hat_sqrt = self._clip_if_large(v_hat_sqrt)
            param.data -= self.lr * m_hat / v_hat_sqrt

            # æ£€æŸ¥å‚æ•°ç¨³å®šæ€§
            self._check_numerical_stability(param.data, f"param {i}")

if __name__ == "__main__":
    """
    ä¼˜åŒ–å™¨æµ‹è¯•ä»£ç 
    ç›´æ¥æµ‹è¯•SGDã€Momentumã€AdaGradå’ŒAdamä¼˜åŒ–å™¨
    """
    
    print("=" * 70)
    print("å¼€å§‹ä¼˜åŒ–å™¨æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•1: åŸºç¡€åŠŸèƒ½æµ‹è¯• - ç®€å•äºŒæ¬¡å‡½æ•°ä¼˜åŒ–
    print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•: f(x) = (x - 3)^2")
    print("-" * 40)
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    x_sgd = Tensor([0.0], requires_grad=True)
    x_momentum = Tensor([0.0], requires_grad=True)
    x_adagrad = Tensor([0.0], requires_grad=True)
    x_adam = Tensor([0.0], requires_grad=True)
    
    optimizers = {
        'SGD': SGD([x_sgd], lr=0.1),
        'Momentum': Momentum([x_momentum], lr=0.1, momentum=0.9),
        'AdaGrad': AdaGrad([x_adagrad], lr=0.5),
        'Adam': Adam([x_adam], lr=0.3)
    }
    
    for name, optimizer in optimizers.items():
        x = optimizer.params[0]
        losses = []
        
        # ä¼˜åŒ–å¾ªç¯
        for step in range(50):
            # è®¡ç®—æŸå¤±: f(x) = (x - 3)^2
            loss = (x - 3.0) ** 2
            loss.backward(np.array([1.0]))
            
            losses.append(loss.data.copy())
            
            # æ£€æŸ¥æ”¶æ•›
            if loss.data < 1e-6:
                break
                
            optimizer.step()
            optimizer.zero_grad()
        
        final_x = x.data[0]
        final_loss = losses[-1]
        print(f"{name} | æœ€ç»ˆ x = {final_x} | æœ€ç»ˆæŸå¤± = {final_loss} | æ­¥æ•° = {len(losses)}")
        
        # éªŒè¯ç»“æœ
        assert abs(final_x - 3.0) < 0.1, f"{name} æœªèƒ½æ”¶æ•›åˆ°æ­£ç¡®å€¼"
    
    print("âœ“ åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•2: å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•
    print("\n2. å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•")
    print("-" * 40)
    
    # åˆ›å»ºå¤šä¸ªå‚æ•°
    w1 = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    w2 = Tensor([0.5, -0.5], requires_grad=True)
    
    # ä½¿ç”¨Adamä¼˜åŒ–å™¨
    optimizer = Adam([w1, w2], lr=0.01)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for step in range(100):
        # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„è®¡ç®—
        output = w1 @ w2.expand_dims(axis=1)  # çŸ©é˜µä¹˜æ³•
        loss = (output - Tensor([[1.0], [2.0]])) ** 2
        total_loss = loss.sum()
        
        total_loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        if step % 25 == 0:
            print(f"æ­¥éª¤ {step:3d}: æŸå¤± = {total_loss.data:.6f}")
    
    print("âœ“ å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•3: å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•
    print("\n3. å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•")
    print("-" * 40)
    
    # åˆ›å»ºå½¢çŠ¶ä¸åŒ¹é…çš„å‚æ•°å’Œæ¢¯åº¦
    param = Tensor([[1.0], [2.0], [3.0]], requires_grad=True)
    optimizer = Adam([param], lr=0.1)
    
    print(f"å‚æ•°å½¢çŠ¶: {param.shape}")
    
    # æ¨¡æ‹Ÿå¹¿æ’­æ¢¯åº¦åœºæ™¯
    # æ‰‹åŠ¨è®¾ç½®ä¸€ä¸ªéœ€è¦å¹¿æ’­çš„æ¢¯åº¦
    param.grad = np.array([
        [0.1, 0.2, 0.3, 0.4],
        [0.5, 0.6, 0.7, 0.8], 
        [0.9, 1.0, 1.1, 1.2]
    ])
    
    print(f"æ¢¯åº¦å½¢çŠ¶: {param.grad.shape}")
    
    try:
        old_param = param.data.copy()
        optimizer.step()
        print(f"å‚æ•°æ›´æ–°æˆåŠŸ!")
        print(f"å‚æ•°å˜åŒ–èŒƒå›´: [{np.min(param.data - old_param):.4f}, {np.max(param.data - old_param):.4f}]")
        print("âœ“ å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âœ— å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•4: æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
    print("\n4. æ•°å€¼ç¨³å®šæ€§æµ‹è¯•")
    print("-" * 40)

    # æµ‹è¯•1: å¤§æ¢¯åº¦
    param1 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
    optimizer1 = Adam([param1], lr=0.1)
    param1.grad = np.array([1e8, 1e8, 1e8])
    try:
        optimizer1.step()
        print("âœ“ å¤§æ¢¯åº¦å¤„ç†æ­£å¸¸")
    except Exception as e:
        print(f"âœ— å¤§æ¢¯åº¦å¤„ç†å¤±è´¥: {e}")

    # æµ‹è¯•2: ç¬¬ä¸€æ¬¡è¿­ä»£çš„é›¶æ¢¯åº¦
    param2 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
    optimizer2 = Adam([param2], lr=0.1)
    param2.grad = np.array([0.0, 0.0, 0.0])
    try:
        old_param = param2.data.copy()
        optimizer2.step()
        # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼Œæ¢¯åº¦ä¸ºé›¶æ—¶å‚æ•°ç¡®å®ä¸åº”æ”¹å˜
        if np.allclose(param2.data, old_param):
            print("âœ“ ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦å¤„ç†æ­£å¸¸")
        else:
            print("âš  ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦æ—¶å‚æ•°æœ‰å˜åŒ–")
    except Exception as e:
        print(f"âœ— ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦å¤„ç†å¤±è´¥: {e}")

    # æµ‹è¯•3: åç»­è¿­ä»£çš„é›¶æ¢¯åº¦
    param3 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
    optimizer3 = Adam([param3], lr=0.1)

    # å…ˆè¿›è¡Œä¸€æ¬¡æ­£å¸¸æ›´æ–°
    param3.grad = np.array([0.1, 0.2, 0.3])
    optimizer3.step()
    initial_param = param3.data.copy()

    # å†æµ‹è¯•é›¶æ¢¯åº¦
    param3.grad = np.array([0.0, 0.0, 0.0])
    old_param = param3.data.copy()
    optimizer3.step()
    change = np.abs(param3.data - old_param).max()

    print(f"ç¬¬ä¸€æ¬¡æ›´æ–°åå‚æ•°: {initial_param}")
    print(f"é›¶æ¢¯åº¦æ›´æ–°å‰å‚æ•°: {old_param}")
    print(f"é›¶æ¢¯åº¦æ›´æ–°åå‚æ•°: {param3.data}")
    print(f"å‚æ•°å˜åŒ–é‡: {change}")

    # åˆç†çš„æ£€æŸ¥ï¼šå˜åŒ–åº”è¯¥é€æ¸è¡°å‡ï¼Œè€Œä¸æ˜¯å®Œå…¨ä¸ºé›¶
    if change < 0.1:  # è®¾ç½®åˆç†çš„é˜ˆå€¼
        print("âœ“ åç»­è¿­ä»£é›¶æ¢¯åº¦å¤„ç†æ­£å¸¸")
    else:
        print(f"âš  åç»­è¿­ä»£é›¶æ¢¯åº¦æ—¶å‚æ•°å˜åŒ–è¾ƒå¤§: {change}")
    
    # æµ‹è¯•5: ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•
    print("\n5. ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•")
    print("-" * 40)
    
    param1 = Tensor([5.0], requires_grad=True)
    param2 = Tensor([5.0], requires_grad=True)
    
    # ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨é…ç½®ä½†ä¸åŒçš„å®ä¾‹
    optimizer1 = Adam([param1], lr=0.1)
    optimizer2 = Adam([param2], lr=0.1)
    
    # å¯¹ç¬¬ä¸€ä¸ªä¼˜åŒ–å™¨æ‰§è¡Œå¤šæ­¥
    for _ in range(10):
        loss = (param1 - 2.0) ** 2
        loss.backward(np.array([1.0]))
        optimizer1.step()
        optimizer1.zero_grad()
    
    # å¯¹ç¬¬äºŒä¸ªä¼˜åŒ–å™¨æ‰§è¡Œä¸€æ­¥
    loss = (param2 - 2.0) ** 2
    loss.backward(np.array([1.0]))
    optimizer2.step()
    optimizer2.zero_grad()
    
    print(f"å¤šæ¬¡ä¼˜åŒ–åçš„å‚æ•°: {param1.data[0]:.4f}")
    print(f"å•æ¬¡ä¼˜åŒ–åçš„å‚æ•°: {param2.data[0]:.4f}")
    
    # ä¸¤ä¸ªå‚æ•°åº”è¯¥ä¸åŒï¼Œå› ä¸ºä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€ä¸åŒ
    assert not np.allclose(param1.data, param2.data), "ä¼˜åŒ–å™¨çŠ¶æ€åº”è¯¥å½±å“å‚æ•°æ›´æ–°"
    print("âœ“ ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•6: å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•
    print("\n6. å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•")
    print("-" * 40)
    
    # æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ
    learning_rates = [0.01, 0.1, 0.5]
    changes = []
    for lr in learning_rates:
        x = Tensor([10.0], requires_grad=True)
        optimizer = SGD([x], lr=lr)
        
        # å•æ­¥ä¼˜åŒ–
        loss = (x - 0.0) ** 2
        loss.backward(np.array([1.0]))
        old_x = x.data.copy()
        optimizer.step()
        optimizer.zero_grad()
        
        change = abs(old_x[0] - x.data[0])
        changes.append(change)
        print(f"å­¦ä¹ ç‡ {lr:4.2f}: å‚æ•°å˜åŒ– = {change:.4f}")
        
    

    # éªŒè¯å­¦ä¹ ç‡è¶Šå¤§ï¼Œå‚æ•°å˜åŒ–è¶Šå¤§
    for i in range(1, len(changes)):
        assert changes[i] > changes[i-1], f"å­¦ä¹ ç‡å¢åŠ æ—¶å‚æ•°å˜åŒ–åº”è¯¥å¢å¤§ï¼Œä½† {changes[i]} <= {changes[i-1]}"
    
    # éªŒè¯å­¦ä¹ ç‡ä¸å‚æ•°å˜åŒ–çš„çº¿æ€§å…³ç³»
    # ç†è®ºä¸Šï¼šchange = lr * gradient = lr * 20.0
    expected_changes = [lr * 20.0 for lr in learning_rates]

    for i, (actual, expected) in enumerate(zip(changes, expected_changes)):
        # å…è®¸10%çš„è¯¯å·®
        assert abs(actual - expected) < expected * 0.1, \
            f"å­¦ä¹ ç‡ {learning_rates[i]} çš„å‚æ•°å˜åŒ– {actual:.4f} ä¸é¢„æœŸ {expected:.4f} ä¸ç¬¦"
        
        print(f"  å­¦ä¹ ç‡ {learning_rates[i]:.2f}: å®é™…å˜åŒ– {actual:.4f}, é¢„æœŸå˜åŒ– {expected:.4f}, è¯¯å·® {abs(actual - expected):.4f}")

    # éªŒè¯å­¦ä¹ ç‡è¶Šå¤§ï¼Œå‚æ•°å˜åŒ–è¶Šå¤§
    for i in range(1, len(changes)):
        assert changes[i] > changes[i-1], \
            f"å­¦ä¹ ç‡å¢åŠ æ—¶å‚æ•°å˜åŒ–åº”è¯¥å¢å¤§ï¼Œä½† {changes[i]} <= {changes[i-1]}"


    print("âœ“ å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•é€šè¿‡")
    

    # æµ‹è¯•7: åŠ¨é‡æ•ˆæœæµ‹è¯•
    print("\n7. åŠ¨é‡æ•ˆæœæµ‹è¯•")
    print("-" * 40)

    # æ–¹æ³•1: æµ‹è¯•åŠ¨é‡åœ¨æŒç»­æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ
    print("æµ‹è¯•åŠ¨é‡åœ¨æŒç»­æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ")

    # åˆ›å»ºå‚æ•°
    x_momentum = Tensor([0.0], requires_grad=True)
    x_no_momentum = Tensor([0.0], requires_grad=True)

    optimizer_momentum = Momentum([x_momentum], lr=0.1, momentum=0.9)
    optimizer_no_momentum = SGD([x_no_momentum], lr=0.1, momentum=0.0)

    # æ¨¡æ‹ŸæŒç»­çš„æ­£æ¢¯åº¦ï¼ˆåŠ¨é‡åº”è¯¥åŠ é€Ÿæ”¶æ•›ï¼‰
    gradients = [1.0, 1.0, 1.0, 1.0, 1.0]  # æŒç»­æ­£æ¢¯åº¦

    print("æ¢¯åº¦æ›´æ–°è¿‡ç¨‹:")
    for i, grad in enumerate(gradients):
        # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
        x_momentum.grad = np.array([grad])
        old_momentum = x_momentum.data[0]
        optimizer_momentum.step()
        momentum_change = x_momentum.data[0] - old_momentum
        optimizer_momentum.zero_grad()
        
        # æ— åŠ¨é‡çš„ä¼˜åŒ–
        x_no_momentum.grad = np.array([grad])
        old_no_momentum = x_no_momentum.data[0]
        optimizer_no_momentum.step()
        no_momentum_change = x_no_momentum.data[0] - old_no_momentum
        optimizer_no_momentum.zero_grad()
        
        print(f"æ­¥éª¤ {i+1}: åŠ¨é‡å˜åŒ–={momentum_change:.4f}, æ— åŠ¨é‡å˜åŒ–={no_momentum_change:.4f}")

    print(f"æœ‰åŠ¨é‡æœ€ç»ˆå‚æ•°: {x_momentum.data[0]:.4f}")
    print(f"æ— åŠ¨é‡æœ€ç»ˆå‚æ•°: {x_no_momentum.data[0]:.4f}")

    # æ ¹æ®ä½ çš„ä¼˜åŒ–å™¨å®ç°ï¼Œæ­£æ¢¯åº¦ä¼šå¯¼è‡´å‚æ•°å‘è´Ÿæ–¹å‘ç§»åŠ¨
    # å› æ­¤åŠ¨é‡åº”è¯¥è®©å‚æ•°å˜å¾—æ›´è´Ÿï¼ˆå³æ›´å°ï¼‰
    assert x_momentum.data[0] < x_no_momentum.data[0], "åŠ¨é‡åº”è¯¥åœ¨è´Ÿæ¢¯åº¦æ–¹å‘ä¸ŠåŠ é€Ÿä¼˜åŒ–"

    print("âœ“ åŠ¨é‡åŠ é€Ÿæ•ˆæœæµ‹è¯•é€šè¿‡")

    # æ–¹æ³•2: æµ‹è¯•åŠ¨é‡åœ¨æŒç»­è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ
    print("æµ‹è¯•åŠ¨é‡åœ¨æŒç»­è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ")

    # åˆ›å»ºå‚æ•°
    x_momentum_neg = Tensor([0.0], requires_grad=True)
    x_no_momentum_neg = Tensor([0.0], requires_grad=True)

    optimizer_momentum_neg = Momentum([x_momentum_neg], lr=0.1, momentum=0.9)
    optimizer_no_momentum_neg = SGD([x_no_momentum_neg], lr=0.1, momentum=0.0)

    # æ¨¡æ‹ŸæŒç»­çš„è´Ÿæ¢¯åº¦ï¼ˆåŠ¨é‡åº”è¯¥åŠ é€Ÿæ”¶æ•›ï¼‰
    gradients_neg = [-1.0, -1.0, -1.0, -1.0, -1.0]  # æŒç»­è´Ÿæ¢¯åº¦

    print("è´Ÿæ¢¯åº¦æ›´æ–°è¿‡ç¨‹:")
    for i, grad in enumerate(gradients_neg):
        # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
        x_momentum_neg.grad = np.array([grad])
        old_momentum = x_momentum_neg.data[0]
        optimizer_momentum_neg.step()
        momentum_change = x_momentum_neg.data[0] - old_momentum
        optimizer_momentum_neg.zero_grad()
        
        # æ— åŠ¨é‡çš„ä¼˜åŒ–
        x_no_momentum_neg.grad = np.array([grad])
        old_no_momentum = x_no_momentum_neg.data[0]
        optimizer_no_momentum_neg.step()
        no_momentum_change = x_no_momentum_neg.data[0] - old_no_momentum
        optimizer_no_momentum_neg.zero_grad()
        
        print(f"æ­¥éª¤ {i+1}: åŠ¨é‡å˜åŒ–={momentum_change:.4f}, æ— åŠ¨é‡å˜åŒ–={no_momentum_change:.4f}")

    print(f"æœ‰åŠ¨é‡æœ€ç»ˆå‚æ•°(è´Ÿæ¢¯åº¦): {x_momentum_neg.data[0]:.4f}")
    print(f"æ— åŠ¨é‡æœ€ç»ˆå‚æ•°(è´Ÿæ¢¯åº¦): {x_no_momentum_neg.data[0]:.4f}")

    # è´Ÿæ¢¯åº¦ä¼šå¯¼è‡´å‚æ•°å‘æ­£æ–¹å‘ç§»åŠ¨ï¼ŒåŠ¨é‡åº”è¯¥è®©å‚æ•°å˜å¾—æ›´å¤§
    assert x_momentum_neg.data[0] > x_no_momentum_neg.data[0], "åŠ¨é‡åº”è¯¥åœ¨æ­£æ¢¯åº¦æ–¹å‘ä¸ŠåŠ é€Ÿä¼˜åŒ–"

    print("âœ“ åŠ¨é‡è´Ÿæ¢¯åº¦åŠ é€Ÿæ•ˆæœæµ‹è¯•é€šè¿‡")

    # æµ‹è¯•åŠ¨é‡åœ¨æŒ¯è¡æ¢¯åº¦ä¸‹çš„å¹³æ»‘æ•ˆæœ
    print("æµ‹è¯•åŠ¨é‡åœ¨æŒ¯è¡æ¢¯åº¦ä¸‹çš„å¹³æ»‘æ•ˆæœ")

    # é‡æ–°åˆå§‹åŒ–å‚æ•°
    x_momentum = Tensor([0.0], requires_grad=True)
    x_no_momentum = Tensor([0.0], requires_grad=True)

    optimizer_momentum = Momentum([x_momentum], lr=0.1, momentum=0.9)
    optimizer_no_momentum = SGD([x_no_momentum], lr=0.1, momentum=0.0)

    # æ¨¡æ‹Ÿæ›´å¼ºçš„æŒ¯è¡æ¢¯åº¦
    gradients = [2.0, -1.8, 1.6, -1.4, 1.2, -1.0, 0.8, -0.6, 0.4, -0.2]

    # è®°å½•è·¯å¾„
    momentum_path = [x_momentum.data[0]]  # åŒ…æ‹¬åˆå§‹ç‚¹
    no_momentum_path = [x_no_momentum.data[0]]

    for grad in gradients:
        # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
        x_momentum.grad = np.array([grad])
        optimizer_momentum.step()
        momentum_path.append(x_momentum.data[0])
        optimizer_momentum.zero_grad()
        
        # æ— åŠ¨é‡çš„ä¼˜åŒ–
        x_no_momentum.grad = np.array([grad])
        optimizer_no_momentum.step()
        no_momentum_path.append(x_no_momentum.data[0])
        optimizer_no_momentum.zero_grad()

    # è®¡ç®—è·¯å¾„çš„ä¸€é˜¶å·®åˆ†
    momentum_diff = np.diff(momentum_path)
    no_momentum_diff = np.diff(no_momentum_path)

    print(f"æœ‰åŠ¨é‡è·¯å¾„å·®åˆ†: {momentum_diff}")
    print(f"æ— åŠ¨é‡è·¯å¾„å·®åˆ†: {no_momentum_diff}")

    # è®¡ç®—å·®åˆ†åºåˆ—çš„æ ‡å‡†å·®
    momentum_diff_std = np.std(momentum_diff)
    no_momentum_diff_std = np.std(no_momentum_diff)

    print(f"æœ‰åŠ¨é‡è·¯å¾„å·®åˆ†æ ‡å‡†å·®: {momentum_diff_std:.4f}")
    print(f"æ— åŠ¨é‡è·¯å¾„å·®åˆ†æ ‡å‡†å·®: {no_momentum_diff_std:.4f}")

    # åŠ¨é‡åº”è¯¥å‡å°‘è·¯å¾„çš„æŒ¯è¡ï¼Œå³å·®åˆ†åºåˆ—çš„æ ‡å‡†å·®æ›´å°
    assert momentum_diff_std < no_momentum_diff_std, "åŠ¨é‡åº”è¯¥å‡å°‘ä¼˜åŒ–è·¯å¾„çš„æŒ¯è¡ï¼ˆå·®åˆ†åºåˆ—æ ‡å‡†å·®æ›´å°ï¼‰"

    print("âœ“ åŠ¨é‡å¹³æ»‘æ•ˆæœæµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 70)
    print("æ‰€æœ‰ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ! ğŸ‰")
    print("=" * 70)
    
    # æ€§èƒ½æ¯”è¾ƒæ€»ç»“
    print("\nä¼˜åŒ–å™¨æ€§èƒ½æ€»ç»“:")
    print("- SGD: åŸºç¡€ä½†å¯é ï¼Œé€‚åˆç®€å•é—®é¢˜")
    print("- Momentum: åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘æŒ¯è¡") 
    print("- AdaGrad: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ•°æ®")
    print("- Adam: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€šå¸¸æ•ˆæœæœ€ä½³")