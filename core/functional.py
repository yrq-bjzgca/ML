
from .tensor import Tensor

# ä¸‹é¢æ˜¯è¿è¡Œfuncionalçš„æ—¶å€™å–æ¶ˆæ³¨é‡Š
# from tensor import Tensor
import numpy as np

# ===== ä¸€å…ƒæ¿€æ´» =====
def relu(x: Tensor, inplace=False) -> Tensor:
    """
    å‰å‘ï¼šout = max(0, x)
    åå‘ï¼šâˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out âŠ™ (x>0)
    TODOï¼š
        1. è®¡ç®— out_data
        2. æ–°å»º Tensor outï¼ŒæŒ‚èµ·è®¡ç®—å›¾
        3. å®ç° _backward å›è°ƒï¼Œå®Œæˆæ¢¯åº¦å›ä¼ 
    """
    out_data = np.maximum(0, x.data)
    if inplace:
        return out_data
    out = Tensor(out_data, requires_grad=True)
    def _backward():
        if x.grad is not None:
            x.grad += out.grad * (x.data>0)
    out._backward = _backward
    out._parents = [x]
    return out

def sigmoid(x: Tensor, inplace=False) -> Tensor:
    """
    Sigmoid(x)=1/(1+e^(-x))
    åå‘ï¼šdL/dx = sigmoid(x)*(1-sigmoid(x))
    """
    # L æœ€ç»ˆæŸå¤±æ ‡é‡
    # âˆ‚L/âˆ‚out = out.grad
    # âˆ‚L/âˆ‚x = x.grad 
    # âˆ‚L/âˆ‚y = other.grad
    # âˆ‚out/âˆ‚x ä½¿ç”¨numpyå¹¿æ’­å®ç°
    # out = sigmoid(x)
    # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out *âˆ‚out/âˆ‚x = out.grad * sigmoid(x)*(1-sigmoid(x))
    
    out_data = 1/(1+np.exp(-x.data))
    if inplace:
        return out_data
    out = Tensor(out_data, requires_grad=True)
    def _backward():
        if x.grad is not None:
            x.grad += out.grad * (out_data*(1-out_data))
    out._backward = _backward
    out._parents = [x]
    return out
    
def tanh(x: Tensor, inplace=False) -> Tensor: 
    """
    tanh(x) = (e^(x)-e^(-x))/(e^(x)+e^(-x))
    """
    out_data = (np.exp(x.data)-np.exp(-x.data))/(np.exp(x.data)+np.exp(-x.data))
    if inplace:
        return out_data
    out = Tensor(out_data, requires_grad=True)
    # out = tanh(x)
    # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out *âˆ‚out/âˆ‚x = out.grad * (1-tanh^2(x))
    def _backward():
        if x.grad is not None:
            x.grad += out.grad * (1-out_data*out_data)
    out._backward = _backward 
    out._parents = [x]
    return out



# ===== å½’ä¸€åŒ– =====
def softmax(x: Tensor, axis=-1) -> Tensor:
    """
    å‰å‘ï¼šæ•°å€¼ç¨³å®šç‰ˆ softmax
          max_val = x.max(axis, keepdims)
          x_stable = x - max_val
          exp_x = exp(x_stable)
          out = exp_x / exp_x.sum(axis, keepdims)

    åå‘ï¼šâˆ‚L/âˆ‚x = out * (grad_out - Î£(grad_out * out))
    TODOï¼š
        1. å®Œæˆå‰å‘è®¡ç®—ï¼ˆå¤ç”¨ Tensor çš„ max/exp/sumï¼‰
        2. æ–°å»º Tensor outï¼ŒæŒ‚è®¡ç®—å›¾
        3. _backward é‡ŒæŒ‰å…¬å¼å›ä¼ 
    """

    """
    softmax(x_i) = exp(x_i) / sum(exp(x_j))
    åå‘ï¼šdL/dx = softmax * (dL/dout - sum(dL/dout * softmax))
    """
    # 1. æ•°å€¼ç¨³å®šï¼šå‡æœ€å¤§å€¼
    max_val = x.max(axis=axis, keepdims=True)          # ä½ å¾—å®ç° x.max
    x_stable = x - max_val                             # broadcast ä½ å·²æ”¯æŒ

    # 2. æŒ‡æ•°
    exp_x = x_stable.exp()                             # ä½ å¾—å®ç° Tensor.exp()

    # 3. å½’ä¸€åŒ–
    sum_exp = exp_x.sum(axis=axis, keepdims=True)      # ä½ å¾—å®ç° x.sum
    out = exp_x / sum_exp                              # broadcast é™¤æ³•

    # out = softmax(x)
    # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚out *âˆ‚out/âˆ‚x
    # âˆ‚L/âˆ‚x[i] = âˆ‚L/âˆ‚out *âˆ‚out/âˆ‚x[i] =  Î£_j âˆ‚L/âˆ‚out[j] Â· s[j](Î´_{ij} âˆ’ s[i])
    # = s[i] Â· (âˆ‚L/âˆ‚out[i] âˆ’ Î£_j âˆ‚L/âˆ‚out[j] Â· s[j])

    # 4. é“¾å¼å›è°ƒï¼šåå‘å…¬å¼
    def _backward():
        if x.grad is not None:
            # ç•™ç©ºï¼šæ ¹æ®é“¾å¼è§„åˆ™å®Œæˆæ¢¯åº¦
            # æç¤ºï¼šgrad_out = out.grad
            #       grad_x = out * (grad_out - (grad_out * out).sum(axis, keepdims=True))
            grad_out = out.grad
            s = (grad_out * out.data).sum(axis = axis, keepdims = True)
            x.grad += out.data * (grad_out - s)

    out._backward = _backward
    out._parents = [x]
    return out

def log_softmax(x: Tensor, axis=-1) -> Tensor:
    """
    å‰å‘ï¼šlog(softmax(x)) = x - log(sum(exp(x)))
          ä»éœ€å‡ max ä¿è¯æ•°å€¼ç¨³å®š
    åå‘ï¼šâˆ‚L/âˆ‚x = grad_out - exp(out)*grad_out.sum(axis, keepdims)
    TODOï¼š
        1. å®Œæˆå‰å‘è®¡ç®—ï¼ˆå¤ç”¨ Tensor çš„ max/exp/sumï¼‰
        2. æ–°å»º Tensor outï¼ŒæŒ‚è®¡ç®—å›¾
        3. _backward é‡ŒæŒ‰å…¬å¼å›ä¼ 
    """

    # log_softmax(xáµ¢) = ln(exp(xáµ¢) / Î£â±¼ exp(xâ±¼)) = xáµ¢ âˆ’ ln(Î£â±¼ exp(xâ±¼))
    # æ•°å€¼ç¨³å®šï¼Œå‡å»æœ€å¤§å€¼
    max_val = x.max(axis=axis, keepdims = True)
    x_stable = x - max_val
    # æŒ‡æ•°+æ±‚å’Œ
    exp_x = x_stable.exp()
    sum_exp = exp_x.sum(axis = axis, keepdims = True)
    # ln(softmax) = x âˆ’ ln(sum_exp)
    log_sum_exp = sum_exp.log() #è¡¥å……tensorçš„logå‡½æ•°
    out = x_stable - log_sum_exp #éœ€è¦ä½¿ç”¨å¹¿æ’­å‡æ³•
    # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y âŠ™ (1 âˆ’ exp(y))â€ƒâ€ƒ(y = log_softmax(x))
    def _backward():
        if x.requires_grad:
            if x.grad is None:
                x.grad = np.zeros_like(x.data)

            grad_out = out.grad #(N,C)
            exp_out = np.exp(out.data) #(N,C)
            # å¯¹ç±»åˆ«è½´æ±‚å’Œï¼Œä¿æŒç»´åº¦
            # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y - softmax(x) * sum(âˆ‚L/âˆ‚y)
            sum_grad = grad_out.sum(axis=axis, keepdims=True)  #(N,1)
            x_grad = grad_out - exp_out * sum_grad # ä½¿ç”¨å¹¿æ’­ä¹˜æ³•

            if x.grad is None:
                x.grad = np.zeros_like(x.data)
            x.grad += x_grad
    out._backward = _backward
    out._parents = [x]

    return out


# ===== æŸå¤± =====

def nll_loss(log_probs:Tensor, targets:Tensor)->'Tensor':

    """
    å‰å‘ï¼š
        N = log_probs.shape[0]
        idx  = targets.data.astype(int)   # è¿™é‡Œå…è®¸ç”¨ numpy å–å€¼
        selected = log_probs[range(N), idx]   # ç”¨ Tensor ç´¢å¼•ä¿æŒå›¾
        loss = -selected.mean()               # è¿”å›æ ‡é‡ Tensor
    åå‘ï¼š
        æ— éœ€æ‰‹å†™ï¼Œselected.mean() ä¼šè‡ªåŠ¨å®Œæˆ
    TODOï¼šä»…å®Œæˆå‰å‘å³å¯ï¼Œè®¡ç®—å›¾ä¿æŒå®Œæ•´
    """

    """
    log_probs: (N, C)
    targets: (N,) int class indices
    """
    N = log_probs.shape[0]
    idx = targets.data.astype(int)
    selected = log_probs[range(N), idx]
    loss = -selected.mean()
    return loss

def cross_entropy(logits: Tensor, targets: Tensor) -> Tensor: 
    """
    æ¨èå®ç°ï¼š
        log_p = log_softmax(logits, axis=-1)
        return nll_loss(log_p, targets)
    è¿™æ ·æ— éœ€æ‰‹å†™ _backwardï¼›è‹¥åšæŒæ‰‹åŠ¨ï¼Œå¯ä¿ç•™åŸç¡¬ç¼–ç ç‰ˆæœ¬ã€‚
    TODOï¼šäºŒé€‰ä¸€
    """

    """
    logits:(N, C) raw score
    targets:(N,) class index
    """
    # log_p = log_softmax(logits, axis=-1)            # (N,C)
    # # é€‰å–ç›®æ ‡ log-prob
    # idx = targets.data.astype(int)
    # selected = log_p.data[np.arange(len(idx)),idx]  # (N,)
    # loss_data = -selected.mean()                    # scalar
    # out = Tensor(loss_data, requires_grad=True)
    # def _backward():
    #     # âˆ‚L/âˆ‚logits = (softmax - one_hot) / N
    #     if logits is not None:
    #         p = logits.data.exp()/logits.data.exp().sum(axis = -1, keepdims = True)
    #         p[np.arange(len(idx)), idx] -= 1
    #         logits.grad += out.grad*p/len(idx)     #å¹³å‡æ¢¯åº¦

    # out._backward = _backward
    # out._parents = [logits, targets]
    # return out
    log_p = log_softmax(logits, axis= -1)

    # æ·»åŠ æ•°å€¼æ£€æŸ¥
    # print(f"DEBUG cross_entropy: logitsèŒƒå›´=[{logits.data.min():.4f}, {logits.data.max():.4f}]")
    # print(f"DEBUG: log_pèŒƒå›´=[{log_p.data.min():.4f}, {log_p.data.max():.4f}]")
    
    return nll_loss(log_p, targets=targets)

def mse_loss(pred: Tensor, target: Tensor) -> Tensor:
    """
    å‰å‘ï¼šout = ((pred - target)**2).mean()
    åå‘ï¼šæ¡†æ¶è‡ªåŠ¨å®Œæˆ
    TODOï¼šä¸€è¡Œå³å¯
    """
    # out_data = ((pred - target)**2).mean()
    # out = Tensor(out_data, requires_grad=True) #å¯¼è‡´è®¡ç®—å›¾ä¸­æ–­

    # return ((pred - target)*(pred - target)).mean()
    return ((pred - target)**2).mean()

# ===== å·ç§¯æ± åŒ–ï¼ˆCNN é˜¶æ®µå†å†™ï¼‰=====
def conv2d(x: Tensor, w: Tensor, b: Tensor=None, stride=1, pad=0) -> Tensor: ...
def max_pool2d(x: Tensor, kernel_size=2, stride=2) -> Tensor: ...

# ===== é«˜çº§ï¼ˆLSTM/Transformer é˜¶æ®µï¼‰=====
def lstm_cell(x: Tensor, hx: Tensor, cx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor) -> (Tensor, Tensor): ...
def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Tensor=None) -> Tensor: ...


if __name__=="__main__":

# å‡è®¾ä½ çš„Tensorç±»å·²ç»å®šä¹‰åœ¨ä¸Šé¢
# è¿™é‡Œåªå±•ç¤ºæµ‹è¯•ä»£ç 

    def test_activation_functions():
        print("===== æ¿€æ´»å‡½æ•°æµ‹è¯• =====")
        
        # æµ‹è¯•ReLU
        print("1. ReLUæµ‹è¯•")
        x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
        y = relu(x)
        y.backward(np.ones_like(y.data))
        
        print(f"è¾“å…¥: {x.data}")
        print(f"ReLUè¾“å‡º: {y.data}")
        print(f"æ¢¯åº¦: {x.grad}")
        expected_grad = np.array([0.0, 0.0, 0.0, 1.0, 1.0])
        assert np.allclose(x.grad, expected_grad), f"ReLUæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
        print("ReLUæµ‹è¯•é€šè¿‡ âœ“")
        
        # æµ‹è¯•Sigmoid
        print("\n2. Sigmoidæµ‹è¯•")
        x = Tensor([-1.0, 0.0, 1.0], requires_grad=True)
        y = sigmoid(x)
        y.backward(np.ones_like(y.data))
        
        print(f"è¾“å…¥: {x.data}")
        print(f"Sigmoidè¾“å‡º: {y.data}")
        print(f"æ¢¯åº¦: {x.grad}")
        
        # æ‰‹åŠ¨è®¡ç®—æœŸæœ›æ¢¯åº¦
        sigmoid_output = 1 / (1 + np.exp(-x.data))
        expected_grad = sigmoid_output * (1 - sigmoid_output)
        assert np.allclose(x.grad, expected_grad, atol=1e-6), f"Sigmoidæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
        print("Sigmoidæµ‹è¯•é€šè¿‡ âœ“")
        
        # æµ‹è¯•Tanh
        print("\n3. Tanhæµ‹è¯•")
        x = Tensor([-1.0, 0.0, 1.0], requires_grad=True)
        y = tanh(x)
        y.backward(np.ones_like(y.data))
        
        print(f"è¾“å…¥: {x.data}")
        print(f"Tanhè¾“å‡º: {y.data}")
        print(f"æ¢¯åº¦: {x.grad}")
        
        # æ‰‹åŠ¨è®¡ç®—æœŸæœ›æ¢¯åº¦
        tanh_output = np.tanh(x.data)
        expected_grad = 1 - tanh_output ** 2
        assert np.allclose(x.grad, expected_grad, atol=1e-6), f"Tanhæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
        print("Tanhæµ‹è¯•é€šè¿‡ âœ“")

    def test_softmax():
        print("\n===== Softmaxæµ‹è¯• =====")
        
        # æµ‹è¯•1: åŸºç¡€softmax
        x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
        y = softmax(x, axis=-1)
        y.backward(np.ones_like(y.data))
        
        print(f"è¾“å…¥: {x.data}")
        print(f"Softmaxè¾“å‡º: {y.data}")
        print(f"è¾“å‡ºå’Œ: {y.data.sum()}")
        print(f"æ¢¯åº¦: {x.grad}")
        
        # æ£€æŸ¥è¾“å‡ºå’Œä¸º1
        assert abs(y.data.sum() - 1.0) < 1e-6, "Softmaxè¾“å‡ºå’Œä¸ä¸º1"
        print("SoftmaxåŸºç¡€æµ‹è¯•é€šè¿‡ âœ“")
        
        # æµ‹è¯•2: æ•°å€¼ç¨³å®šæ€§
        x = Tensor([[1000.0, 1000.0, 1000.0]], requires_grad=True)
        y = softmax(x, axis=-1)
        
        print(f"å¤§æ•°å€¼è¾“å…¥: {x.data}")
        print(f"Softmaxè¾“å‡º: {y.data}")
        assert not np.any(np.isnan(y.data)), "Softmaxæ•°å€¼ä¸ç¨³å®š"
        print("Softmaxæ•°å€¼ç¨³å®šæ€§æµ‹è¯•é€šè¿‡ âœ“")

    def test_log_softmax():
        print("\n===== Log Softmaxæµ‹è¯• =====")
        
        x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
        y = log_softmax(x, axis=-1)
        y.backward(np.ones_like(y.data))
        
        print(f"è¾“å…¥: {x.data}")
        print(f"Log Softmaxè¾“å‡º: {y.data}")
        print(f"æ¢¯åº¦: {x.grad}")
        
        # éªŒè¯ log_softmax(x) = log(softmax(x))
        softmax_out = softmax(x, axis=-1)
        expected = np.log(softmax_out.data + 1e-12)
        assert np.allclose(y.data, expected, atol=1e-6), "Log Softmaxè¾“å‡ºé”™è¯¯"
        print("Log Softmaxæµ‹è¯•é€šè¿‡ âœ“")

    def test_loss_functions():
        print("\n===== æŸå¤±å‡½æ•°æµ‹è¯• =====")
        
        # æµ‹è¯•NLL Loss
        print("1. NLL Lossæµ‹è¯•")
        log_probs = Tensor([
            [-1.0, -2.0, -3.0],  # çœŸå®ç±»åœ¨ä½ç½®0
            [-3.0, -1.0, -2.0]   # çœŸå®ç±»åœ¨ä½ç½®1
        ], requires_grad=True)
        targets = Tensor([0, 1])  # ç±»åˆ«ç´¢å¼•
        
        loss = nll_loss(log_probs, targets)
        loss.backward()
        
        print(f"Logæ¦‚ç‡: {log_probs.data}")
        print(f"ç›®æ ‡: {targets.data}")
        print(f"NLL Loss: {loss.data}")
        print(f"Logæ¦‚ç‡æ¢¯åº¦: {log_probs.grad}")
        
        # æ‰‹åŠ¨éªŒè¯
        selected = np.array([log_probs.data[0, 0], log_probs.data[1, 1]])
        expected_loss = -selected.mean()
        assert abs(loss.data - expected_loss) < 1e-6, "NLL Lossè®¡ç®—é”™è¯¯"
        print("NLL Lossæµ‹è¯•é€šè¿‡ âœ“")
        
        # æµ‹è¯•Cross Entropy
        print("\n2. Cross Entropyæµ‹è¯•")
        logits = Tensor([
            [2.0, 1.0, 0.1],  # çœŸå®ç±»åœ¨ä½ç½®0
            [0.1, 2.0, 0.1]   # çœŸå®ç±»åœ¨ä½ç½®1
        ], requires_grad=True)
        targets = Tensor([0, 1])
        
        loss = cross_entropy(logits, targets)
        loss.backward()
        
        print(f"Logits: {logits.data}")
        print(f"ç›®æ ‡: {targets.data}")
        print(f"Cross Entropy Loss: {loss.data}")
        print(f"Logitsæ¢¯åº¦: {logits.grad}")
        
        # éªŒè¯æ¢¯åº¦å½¢çŠ¶
        assert logits.grad.shape == logits.data.shape, "Cross Entropyæ¢¯åº¦å½¢çŠ¶é”™è¯¯"
        print("Cross Entropyæµ‹è¯•é€šè¿‡ âœ“")
        
        # æµ‹è¯•MSE Loss
        print("\n3. MSE Lossæµ‹è¯•")
        pred = Tensor([1.0, 2.0, 3.0], requires_grad=True)
        target = Tensor([1.5, 1.8, 2.9])
        
        loss = mse_loss(pred, target)
        loss.backward()
        
        print(f"é¢„æµ‹: {pred.data}")
        print(f"ç›®æ ‡: {target.data}")
        print(f"MSE Loss: {loss.data}")
        print(f"é¢„æµ‹æ¢¯åº¦: {pred.grad}")
        
        # æ‰‹åŠ¨éªŒè¯
        expected_loss = ((pred.data - target.data) ** 2).mean()
        assert abs(loss.data - expected_loss) < 1e-6, "MSE Lossè®¡ç®—é”™è¯¯"
        print("MSE Lossæµ‹è¯•é€šè¿‡ âœ“")

    def test_complex_chain():
        print("\n===== å¤æ‚é“¾å¼æµ‹è¯• =====")
        
        # æ„å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
        x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
        w = Tensor([[0.5, -0.5], [0.1, 0.9]], requires_grad=True)
        b = Tensor([0.1, 0.2], requires_grad=True)
        
        # å‰å‘ä¼ æ’­
        linear = x @ w + b  # çŸ©é˜µä¹˜æ³• + åç½®
        activated = relu(linear)  # ReLUæ¿€æ´»
        normalized = softmax(activated, axis=-1)  # Softmaxå½’ä¸€åŒ–
        
        # è®¡ç®—æŸå¤±
        targets = Tensor([0, 1])  # ä¸¤ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«
        loss = cross_entropy(normalized, targets)
        
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"çº¿æ€§è¾“å‡ºå½¢çŠ¶: {linear.shape}")
        print(f"æ¿€æ´»è¾“å‡ºå½¢çŠ¶: {activated.shape}")
        print(f"å½’ä¸€åŒ–è¾“å‡ºå½¢çŠ¶: {normalized.shape}")
        print(f"æŸå¤±: {loss.data}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
        assert x.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
        assert w.grad is not None, "æƒé‡æ¢¯åº¦æœªè®¡ç®—"
        assert b.grad is not None, "åç½®æ¢¯åº¦æœªè®¡ç®—"
        
        print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
        print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {w.grad.shape}")
        print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {b.grad.shape}")
        
        print("å¤æ‚é“¾å¼æµ‹è¯•é€šè¿‡ âœ“")

    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_activation_functions()
    test_softmax()
    test_log_softmax()
    test_loss_functions()
    test_complex_chain()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")