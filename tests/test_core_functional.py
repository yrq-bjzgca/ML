
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from energy.regularize import (
    L2Regularizer, 
    FLOPsCalculator,
    FLOPsRegularizer,
    EnergyAwareRegularizer,
    CombinedRegularizer
)
from nn import Sequential, Linear, ReLU, Conv2d
from core.tensor import Tensor
from core.optim import SGD, AdaGrad,Momentum,Adam

from core.functional import relu,sigmoid,tanh,softmax,log_softmax,cross_entropy,conv2d,nll_loss,mse_loss

# å‡è®¾ä½ çš„Tensorç±»å·²ç»å®šä¹‰åœ¨ä¸Šé¢
# è¿™é‡Œåªå±•ç¤ºæµ‹è¯•ä»£ç 

def test_activation_functions():
    print("===== æ¿€æ´»å‡½æ•°æµ‹è¯• =====")
    
    # æµ‹è¯•ReLU
    print("1. ReLUæµ‹è¯•")
    x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    y = relu(x)
    y.backward(np.ones_like(y.data))
    
    print(f"è¾“å…¥: {x.data}")
    print(f"ReLUè¾“å‡º: {y.data}")
    print(f"æ¢¯åº¦: {x.grad}")
    expected_grad = np.array([0.0, 0.0, 0.0, 1.0, 1.0])
    assert np.allclose(x.grad, expected_grad), f"ReLUæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
    print("ReLUæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•Sigmoid
    print("\n2. Sigmoidæµ‹è¯•")
    x = Tensor([-1.0, 0.0, 1.0], requires_grad=True)
    y = sigmoid(x)
    y.backward(np.ones_like(y.data))
    
    print(f"è¾“å…¥: {x.data}")
    print(f"Sigmoidè¾“å‡º: {y.data}")
    print(f"æ¢¯åº¦: {x.grad}")
    
    # æ‰‹åŠ¨è®¡ç®—æœŸæœ›æ¢¯åº¦
    sigmoid_output = 1 / (1 + np.exp(-x.data))
    expected_grad = sigmoid_output * (1 - sigmoid_output)
    assert np.allclose(x.grad, expected_grad, atol=1e-6), f"Sigmoidæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
    print("Sigmoidæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•Tanh
    print("\n3. Tanhæµ‹è¯•")
    x = Tensor([-1.0, 0.0, 1.0], requires_grad=True)
    y = tanh(x)
    y.backward(np.ones_like(y.data))
    
    print(f"è¾“å…¥: {x.data}")
    print(f"Tanhè¾“å‡º: {y.data}")
    print(f"æ¢¯åº¦: {x.grad}")
    
    # æ‰‹åŠ¨è®¡ç®—æœŸæœ›æ¢¯åº¦
    tanh_output = np.tanh(x.data)
    expected_grad = 1 - tanh_output ** 2
    assert np.allclose(x.grad, expected_grad, atol=1e-6), f"Tanhæ¢¯åº¦é”™è¯¯: {x.grad} != {expected_grad}"
    print("Tanhæµ‹è¯•é€šè¿‡ âœ“")

def test_softmax():
    print("\n===== Softmaxæµ‹è¯• =====")
    
    # æµ‹è¯•1: åŸºç¡€softmax
    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
    y = softmax(x, axis=-1)
    y.backward(np.ones_like(y.data))
    
    print(f"è¾“å…¥: {x.data}")
    print(f"Softmaxè¾“å‡º: {y.data}")
    print(f"è¾“å‡ºå’Œ: {y.data.sum()}")
    print(f"æ¢¯åº¦: {x.grad}")
    
    # æ£€æŸ¥è¾“å‡ºå’Œä¸º1
    assert abs(y.data.sum() - 1.0) < 1e-6, "Softmaxè¾“å‡ºå’Œä¸ä¸º1"
    print("SoftmaxåŸºç¡€æµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•2: æ•°å€¼ç¨³å®šæ€§
    x = Tensor([[1000.0, 1000.0, 1000.0]], requires_grad=True)
    y = softmax(x, axis=-1)
    
    print(f"å¤§æ•°å€¼è¾“å…¥: {x.data}")
    print(f"Softmaxè¾“å‡º: {y.data}")
    assert not np.any(np.isnan(y.data)), "Softmaxæ•°å€¼ä¸ç¨³å®š"
    print("Softmaxæ•°å€¼ç¨³å®šæ€§æµ‹è¯•é€šè¿‡ âœ“")

def test_log_softmax():
    print("\n===== Log Softmaxæµ‹è¯• =====")
    
    x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)
    y = log_softmax(x, axis=-1)
    y.backward(np.ones_like(y.data))
    
    print(f"è¾“å…¥: {x.data}")
    print(f"Log Softmaxè¾“å‡º: {y.data}")
    print(f"æ¢¯åº¦: {x.grad}")
    
    # éªŒè¯ log_softmax(x) = log(softmax(x))
    softmax_out = softmax(x, axis=-1)
    expected = np.log(softmax_out.data + 1e-12)
    assert np.allclose(y.data, expected, atol=1e-6), "Log Softmaxè¾“å‡ºé”™è¯¯"
    print("Log Softmaxæµ‹è¯•é€šè¿‡ âœ“")

def test_loss_functions():
    print("\n===== æŸå¤±å‡½æ•°æµ‹è¯• =====")
    
    # æµ‹è¯•NLL Loss
    print("1. NLL Lossæµ‹è¯•")
    log_probs = Tensor([
        [-1.0, -2.0, -3.0],  # çœŸå®ç±»åœ¨ä½ç½®0
        [-3.0, -1.0, -2.0]   # çœŸå®ç±»åœ¨ä½ç½®1
    ], requires_grad=True)
    targets = Tensor([0, 1])  # ç±»åˆ«ç´¢å¼•
    
    loss = nll_loss(log_probs, targets)
    loss.backward()
    
    print(f"Logæ¦‚ç‡: {log_probs.data}")
    print(f"ç›®æ ‡: {targets.data}")
    print(f"NLL Loss: {loss.data}")
    print(f"Logæ¦‚ç‡æ¢¯åº¦: {log_probs.grad}")
    
    # æ‰‹åŠ¨éªŒè¯
    selected = np.array([log_probs.data[0, 0], log_probs.data[1, 1]])
    expected_loss = -selected.mean()
    assert abs(loss.data - expected_loss) < 1e-6, "NLL Lossè®¡ç®—é”™è¯¯"
    print("NLL Lossæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•Cross Entropy
    print("\n2. Cross Entropyæµ‹è¯•")
    logits = Tensor([
        [2.0, 1.0, 0.1],  # çœŸå®ç±»åœ¨ä½ç½®0
        [0.1, 2.0, 0.1]   # çœŸå®ç±»åœ¨ä½ç½®1
    ], requires_grad=True)
    targets = Tensor([0, 1])
    
    loss = cross_entropy(logits, targets)
    loss.backward()
    
    print(f"Logits: {logits.data}")
    print(f"ç›®æ ‡: {targets.data}")
    print(f"Cross Entropy Loss: {loss.data}")
    print(f"Logitsæ¢¯åº¦: {logits.grad}")
    
    # éªŒè¯æ¢¯åº¦å½¢çŠ¶
    assert logits.grad.shape == logits.data.shape, "Cross Entropyæ¢¯åº¦å½¢çŠ¶é”™è¯¯"
    print("Cross Entropyæµ‹è¯•é€šè¿‡ âœ“")
    
    # æµ‹è¯•MSE Loss
    print("\n3. MSE Lossæµ‹è¯•")
    pred = Tensor([1.0, 2.0, 3.0], requires_grad=True)
    target = Tensor([1.5, 1.8, 2.9])
    
    loss = mse_loss(pred, target)
    loss.backward()
    
    print(f"é¢„æµ‹: {pred.data}")
    print(f"ç›®æ ‡: {target.data}")
    print(f"MSE Loss: {loss.data}")
    print(f"é¢„æµ‹æ¢¯åº¦: {pred.grad}")
    
    # æ‰‹åŠ¨éªŒè¯
    expected_loss = ((pred.data - target.data) ** 2).mean()
    assert abs(loss.data - expected_loss) < 1e-6, "MSE Lossè®¡ç®—é”™è¯¯"
    print("MSE Lossæµ‹è¯•é€šè¿‡ âœ“")

def test_complex_chain():
    print("\n===== å¤æ‚é“¾å¼æµ‹è¯• =====")
    
    # æ„å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
    x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    w = Tensor([[0.5, -0.5], [0.1, 0.9]], requires_grad=True)
    b = Tensor([0.1, 0.2], requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    linear = x @ w + b  # çŸ©é˜µä¹˜æ³• + åç½®
    activated = relu(linear)  # ReLUæ¿€æ´»
    normalized = softmax(activated, axis=-1)  # Softmaxå½’ä¸€åŒ–
    
    # è®¡ç®—æŸå¤±
    targets = Tensor([0, 1])  # ä¸¤ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«
    loss = cross_entropy(normalized, targets)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"çº¿æ€§è¾“å‡ºå½¢çŠ¶: {linear.shape}")
    print(f"æ¿€æ´»è¾“å‡ºå½¢çŠ¶: {activated.shape}")
    print(f"å½’ä¸€åŒ–è¾“å‡ºå½¢çŠ¶: {normalized.shape}")
    print(f"æŸå¤±: {loss.data}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    assert x.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
    assert w.grad is not None, "æƒé‡æ¢¯åº¦æœªè®¡ç®—"
    assert b.grad is not None, "åç½®æ¢¯åº¦æœªè®¡ç®—"
    
    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {w.grad.shape}")
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {b.grad.shape}")
    
    print("å¤æ‚é“¾å¼æµ‹è¯•é€šè¿‡ âœ“")


# è¿è¡Œæ‰€æœ‰æµ‹è¯•
test_activation_functions()
test_softmax()
test_log_softmax()
test_loss_functions()
test_complex_chain()

print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")