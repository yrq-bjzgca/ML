
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from energy.regularize import (
    L2Regularizer, 
    FLOPsCalculator,
    FLOPsRegularizer,
    EnergyAwareRegularizer,
    CombinedRegularizer
)
from nn import Sequential, Linear, ReLU, Conv2d
from core.tensor import Tensor

# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦è¿‘ä¼¼ç›¸ç­‰
def check_grad(computed, expected, eps=1e-5):
    assert np.allclose(computed, expected, atol=eps), \
        f"æ¢¯åº¦ä¸åŒ¹é…: è®¡ç®—å€¼={computed}ï¼ŒæœŸæœ›å€¼={expected}"

print("===== å¼€å§‹ 1. åŸºç¡€è¿ç®—æµ‹è¯•ï¼ˆ+ã€-ã€*ã€/ï¼‰ =====")
# æµ‹è¯•åŠ æ³•
a = Tensor([2.0, 3.0], requires_grad=True)
b = Tensor([4.0, 5.0], requires_grad=True)
c = a + b
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [1.0, 1.0])
check_grad(b.grad, [1.0, 1.0])
a.zero_grad()
b.zero_grad()

# æµ‹è¯•å‡æ³•
c = a - b
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [1.0, 1.0])
check_grad(b.grad, [-1.0, -1.0])
a.zero_grad()
b.zero_grad()

# æµ‹è¯•ä¹˜æ³•
c = a * b
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [4.0, 5.0])  # bçš„å€¼
check_grad(b.grad, [2.0, 3.0])  # açš„å€¼
a.zero_grad()
b.zero_grad()

# æµ‹è¯•é™¤æ³•
c = a / b
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [1/4, 1/5])          # 1/b
check_grad(b.grad, [-2/(4**2), -3/(5**2)])  # -a/bÂ²
a.zero_grad()
b.zero_grad()

print("===== 2. å¹¿æ’­æµ‹è¯• =====")
# å¹¿æ’­åŠ æ³•
a = Tensor([[1.0, 2.0]], requires_grad=True)  # å½¢çŠ¶(1,2)
b = Tensor([[3.0], [4.0]], requires_grad=True)  # å½¢çŠ¶(2,1)
c = a + b  # å½¢çŠ¶(2,2)
c.backward(np.ones((2, 2)))
check_grad(a.grad, [[2.0, 2.0]])  # æ²¿0è½´æ±‚å’Œ
check_grad(b.grad, [[2.0], [2.0]])  # æ²¿1è½´æ±‚å’Œ
a.zero_grad()
b.zero_grad()

# å¹¿æ’­ä¹˜æ³•
c = a * b
c.backward(np.ones((2, 2)))
check_grad(a.grad, [[3+4, 3+4]])  # bçš„å’Œ
check_grad(b.grad, [[1+2], [1+2]])  # açš„å’Œ
a.zero_grad()
b.zero_grad()

print("===== 3. çŸ©é˜µä¹˜æ³•æµ‹è¯• =====")
a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)  # (2,2)
b = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)  # (2,2)
c = a @ b
c.backward(np.ones((2, 2)))
# éªŒè¯açš„æ¢¯åº¦ï¼šones @ b.T
expected_a_grad = np.ones((2,2)) @ b.data.T
check_grad(a.grad, expected_a_grad)
# éªŒè¯bçš„æ¢¯åº¦ï¼ša.T @ ones
expected_b_grad = a.data.T @ np.ones((2,2))
check_grad(b.grad, expected_b_grad)
a.zero_grad()
b.zero_grad()

print("===== 4. æ¿€æ´»å‡½æ•°æµ‹è¯•ï¼ˆexpã€logï¼‰ =====")
# æµ‹è¯•exp
a = Tensor([1.0, 2.0], requires_grad=True)
c = a.exp()
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, np.exp([1.0, 2.0]))  # exp(x)çš„å¯¼æ•°æ˜¯è‡ªèº«
a.zero_grad()

# æµ‹è¯•log
a = Tensor([2.0, 3.0], requires_grad=True)
c = a.log()
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [1/2, 1/3])  # log(x)çš„å¯¼æ•°æ˜¯1/x
a.zero_grad()

print("===== 5. èšåˆ èšåˆå‡½æ•°æµ‹è¯•ï¼ˆsumã€meanï¼‰ =====")
# æµ‹è¯•sum
a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
c = a.sum(axis=0)  # æ²¿0è½´æ±‚å’Œ
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, np.ones((2, 2)))  # å¹¿æ’­åå…¨ä¸º1
a.zero_grad()

# æµ‹è¯•mean
c = a.mean(axis=1, keepdims=True)  # æ²¿1è½´æ±‚å¹³å‡
c.backward(np.ones((2, 1)))
expected_grad = np.ones((2, 2)) * (1/2)  # å¹³å‡æ¢¯åº¦
check_grad(a.grad, expected_grad)
a.zero_grad()

print("Testing no_grad context manager...")
# æ­£å¸¸æƒ…å†µ
x1 = Tensor([1, 2, 3], requires_grad=True)
assert x1.requires_grad == True, "æ­£å¸¸æƒ…å†µ requires_grad åº”è¯¥ä¸º True"

# åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­
with Tensor.no_grad():
    x2 = Tensor([1, 2, 3], requires_grad=True)
    assert x2.requires_grad == False, "no_grad ä¸­ requires_grad åº”è¯¥ä¸º False"
    
    # æ£€æŸ¥å…¨å±€çŠ¶æ€
    assert Tensor.is_grad_enabled() == False, "no_grad ä¸­å…¨å±€çŠ¶æ€åº”è¯¥ä¸º False"

# ç¦»å¼€ no_grad åæ¢å¤
assert Tensor.is_grad_enabled() == True, "ç¦»å¼€ no_grad åå…¨å±€çŠ¶æ€åº”è¯¥æ¢å¤"

x3 = Tensor([1, 2, 3], requires_grad=True)
assert x3.requires_grad == True, "ç¦»å¼€ no_grad å requires_grad åº”è¯¥æ¢å¤"



# åˆ›å»ºæµ‹è¯•å¼ é‡
x = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)

# æµ‹è¯•å‡å€¼
mean_all = x.mean()
assert np.allclose(mean_all.data, 3.5), "å…¨å±€å‡å€¼è®¡ç®—é”™è¯¯"

mean_axis0 = x.mean(axis=0)
assert np.allclose(mean_axis0.data, [2.5, 3.5, 4.5]), "æ²¿è½´0å‡å€¼è®¡ç®—é”™è¯¯"

mean_axis1 = x.mean(axis=1)
assert np.allclose(mean_axis1.data, [2, 5]), "æ²¿è½´1å‡å€¼è®¡ç®—é”™è¯¯"

# æµ‹è¯•æ–¹å·®
var_all = x.var()
expected_var = np.var([[1, 2, 3], [4, 5, 6]], ddof=1)  # æ ·æœ¬æ–¹å·®
assert np.allclose(var_all.data, expected_var), "å…¨å±€æ–¹å·®è®¡ç®—é”™è¯¯"

var_axis0 = x.var(axis=0)
expected_var_axis0 = np.var([[1, 2, 3], [4, 5, 6]], axis=0, ddof=1)
assert np.allclose(var_axis0.data, expected_var_axis0), "æ²¿è½´0æ–¹å·®è®¡ç®—é”™è¯¯"

# æµ‹è¯•æ ‡å‡†å·®
std_all = x.std()
expected_std = np.std([[1, 2, 3], [4, 5, 6]], ddof=1)
assert np.allclose(std_all.data, expected_std), "å…¨å±€æ ‡å‡†å·®è®¡ç®—é”™è¯¯"


print("===== 6. å½¢çŠ¶æ“ä½œæµ‹è¯•ï¼ˆreshapeã€transposeã€expand_dimsï¼‰ =====")
# æµ‹è¯•reshape
a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
c = a.reshape(1, 4)
c.backward(np.array([[1.0, 1.0, 1.0, 1.0]]))
check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦å½¢çŠ¶è¿˜åŸ
a.zero_grad()

# æµ‹è¯•transpose
c = a.transpose(1, 0)  # è½¬ç½®
c.backward(np.ones((2, 2)))
check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦ä¹Ÿè½¬ç½®å›æ¥
a.zero_grad()

# æµ‹è¯•expand_dims
c = a.expand_dims(axis=0)  # æ–°å¢0è½´
c.backward(np.ones((1, 2, 2)))
check_grad(a.grad, np.ones((2, 2)))  # æ¢¯åº¦æŒ¤å‹æ–°å¢ç»´åº¦
a.zero_grad()

print("===== 7. åˆ‡ç‰‡ä¸padæµ‹è¯• =====")
# æµ‹è¯•åˆ‡ç‰‡
a = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
c = a[1:3]  # å–ç´¢å¼•1å’Œ2
c.backward(np.array([1.0, 1.0]))
check_grad(a.grad, [0.0, 1.0, 1.0, 0.0])  # åˆ‡ç‰‡ä½ç½®æ¢¯åº¦ä¸º1
a.zero_grad()

# æµ‹è¯•pad
a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
c = a.pad(((1, 1), (1, 1)))  # å››å‘¨å„pad 1åœˆ
c.backward(np.ones((4, 4)))
check_grad(a.grad, np.ones((2, 2)))  # ä¸­é—´åŒºåŸŸæ¢¯åº¦ä¸º1
a.zero_grad()

print("=== æµ‹è¯•4Då¼ é‡å¡«å…… ===")

# åˆ›å»º4Då¼ é‡ (batch, channels, height, width)
x = Tensor(np.ones((2, 3, 4, 4)), requires_grad=True)
print(f"åŸå§‹å½¢çŠ¶: {x.shape}")

# 4Då¡«å……: ((batchå‰, batchå), (é€šé“å‰, é€šé“å), (é«˜åº¦å‰, é«˜åº¦å), (å®½åº¦å‰, å®½åº¦å))
pad_width = ((0, 0), (0, 0), (1, 1), (1, 1))  # åœ¨é«˜åº¦å’Œå®½åº¦ä¸Šå„å¡«å……1

# åº”ç”¨å¡«å……
x_padded = x.pad(pad_width)
print(f"å¡«å……åå½¢çŠ¶: {x_padded.shape}")

# éªŒè¯å½¢çŠ¶
expected_shape = (2, 3, 6, 6)  # 4+1+1=6, 4+1+1=6
assert x_padded.shape == expected_shape, f"æœŸæœ› {expected_shape}, å®é™… {x_padded.shape}"
print("âœ“ å½¢çŠ¶æ­£ç¡®")

# éªŒè¯å¡«å……å€¼
# ä¸­é—´åŒºåŸŸåº”è¯¥æ˜¯åŸå§‹æ•°æ®ï¼Œè¾¹ç•Œåº”è¯¥æ˜¯0
center_region = x_padded.data[:, :, 1:5, 1:5]  # å»é™¤è¾¹ç•Œ
assert np.allclose(center_region, 1.0), "ä¸­å¿ƒåŒºåŸŸå€¼ä¸æ­£ç¡®"
print("âœ“ å¡«å……å€¼æ­£ç¡®")

# æµ‹è¯•æ¢¯åº¦ä¼ æ’­
loss = x_padded.sum()
loss.backward()

# æ£€æŸ¥æ¢¯åº¦å½¢çŠ¶
assert x.grad.shape == x.shape, "æ¢¯åº¦å½¢çŠ¶ä¸æ­£ç¡®"
print("âœ“ æ¢¯åº¦å½¢çŠ¶æ­£ç¡®")

# æ£€æŸ¥æ¢¯åº¦å€¼ - åº”è¯¥åªæœ‰ä¸­å¿ƒåŒºåŸŸæœ‰æ¢¯åº¦
expected_grad = np.ones((2, 3, 4, 4))
assert np.allclose(x.grad, expected_grad), "æ¢¯åº¦å€¼ä¸æ­£ç¡®"
print("âœ“ æ¢¯åº¦å€¼æ­£ç¡®")

print("ğŸ‰ 4Då¡«å……æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")

print("===== 8. é‡å¤æ“ä½œæµ‹è¯•ï¼ˆrepeatï¼‰ =====")
a = Tensor([[1.0, 2.0]], requires_grad=True)
c = a.repeat(repeats=2, axis=0)  # æ²¿0è½´é‡å¤2æ¬¡
c.backward(np.array([[1.0, 1.0], [1.0, 1.0]]))
check_grad(a.grad, [[2.0, 2.0]])  # é‡å¤åŒºåŸŸæ¢¯åº¦æ±‚å’Œ
a.zero_grad()

print("===== 9. æå€¼æµ‹è¯•ï¼ˆmaxã€minï¼‰ =====")
# æµ‹è¯•max
a = Tensor([[3.0, 1.0], [2.0, 4.0]], requires_grad=True)
c = a.max(axis=1)  # æ²¿1è½´å–æœ€å¤§å€¼
c.backward(np.array([1.0, 1.0]))
expected_max_grad = np.zeros((2, 2))
expected_max_grad[0, 0] = 1.0  # ç¬¬0è¡Œæœ€å¤§å€¼ä½ç½®
expected_max_grad[1, 1] = 1.0  # ç¬¬1è¡Œæœ€å¤§å€¼ä½ç½®
check_grad(a.grad, expected_max_grad)
a.zero_grad()

# æµ‹è¯•min
c = a.min(axis=1)  # æ²¿1è½´å–æœ€å°å€¼
c.backward(np.array([1.0, 1.0]))
expected_min_grad = np.zeros((2, 2))
expected_min_grad[0, 1] = 1.0  # ç¬¬0è¡Œæœ€å°å€¼ä½ç½®
expected_min_grad[1, 0] = 1.0  # ç¬¬1è¡Œæœ€å°å€¼ä½ç½®
check_grad(a.grad, expected_min_grad)
a.zero_grad()

print("===== 10. é“¾å¼ä¼ æ’­æµ‹è¯• =====")
# å¤æ‚è®¡ç®—é“¾ï¼šz = (x*y + exp(x)) / mean(y)
x = Tensor([2.0, 3.0], requires_grad=True)
y = Tensor([4.0, 5.0], requires_grad=True)
z = (x * y + x.exp()) / y.mean()
z.backward(np.array([1.0, 1.0]))


n = y.size
mean_y = y.data.mean()
c_sum = (x.data * y.data + np.exp(x.data)).sum()
dx_expected = (y.data + np.exp(x.data)) / mean_y
dy_expected = (x.data / mean_y) - c_sum / (mean_y**2 * n)

check_grad(x.grad, dx_expected)
check_grad(y.grad, dy_expected)

print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")