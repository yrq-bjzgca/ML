import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from energy.regularize import (
    L2Regularizer, 
    FLOPsCalculator,
    FLOPsRegularizer,
    EnergyAwareRegularizer,
    CombinedRegularizer
)
from nn import Sequential, Linear, ReLU, Conv2d
from core.tensor import Tensor
from core.optim import SGD, AdaGrad,Momentum,Adam


print("=" * 70)
print("å¼€å§‹ä¼˜åŒ–å™¨æµ‹è¯•")
print("=" * 70)

# æµ‹è¯•1: åŸºç¡€åŠŸèƒ½æµ‹è¯• - ç®€å•äºŒæ¬¡å‡½æ•°ä¼˜åŒ–
print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•: f(x) = (x - 3)^2")
print("-" * 40)

# å®šä¹‰ä¼˜åŒ–å™¨
x_sgd = Tensor([0.0], requires_grad=True)
x_momentum = Tensor([0.0], requires_grad=True)
x_adagrad = Tensor([0.0], requires_grad=True)
x_adam = Tensor([0.0], requires_grad=True)

optimizers = {
    'SGD': SGD([x_sgd], lr=0.1),
    'Momentum': Momentum([x_momentum], lr=0.1, momentum=0.9),
    'AdaGrad': AdaGrad([x_adagrad], lr=0.5),
    'Adam': Adam([x_adam], lr=0.3)
}

for name, optimizer in optimizers.items():
    x = optimizer.params[0]
    losses = []
    
    # ä¼˜åŒ–å¾ªç¯
    for step in range(50):
        # è®¡ç®—æŸå¤±: f(x) = (x - 3)^2
        loss = (x - 3.0) ** 2
        loss.backward(np.array([1.0]))
        
        losses.append(loss.data.copy())
        
        # æ£€æŸ¥æ”¶æ•›
        if loss.data < 1e-6:
            break
            
        optimizer.step()
        optimizer.zero_grad()
    
    final_x = x.data[0]
    final_loss = losses[-1]
    print(f"{name} | æœ€ç»ˆ x = {final_x} | æœ€ç»ˆæŸå¤± = {final_loss} | æ­¥æ•° = {len(losses)}")
    
    # éªŒè¯ç»“æœ
    assert abs(final_x - 3.0) < 0.1, f"{name} æœªèƒ½æ”¶æ•›åˆ°æ­£ç¡®å€¼"

print("âœ“ åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")

# æµ‹è¯•2: å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•
print("\n2. å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•")
print("-" * 40)

# åˆ›å»ºå¤šä¸ªå‚æ•°
w1 = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
w2 = Tensor([0.5, -0.5], requires_grad=True)

# ä½¿ç”¨Adamä¼˜åŒ–å™¨
optimizer = Adam([w1, w2], lr=0.01)

# æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
for step in range(100):
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„è®¡ç®—
    output = w1 @ w2.expand_dims(axis=1)  # çŸ©é˜µä¹˜æ³•
    loss = (output - Tensor([[1.0], [2.0]])) ** 2
    total_loss = loss.sum()
    
    total_loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    
    if step % 25 == 0:
        print(f"æ­¥éª¤ {step:3d}: æŸå¤± = {total_loss.data:.6f}")

print("âœ“ å¤šå‚æ•°ä¼˜åŒ–æµ‹è¯•é€šè¿‡")

# æµ‹è¯•3: å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•
print("\n3. å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•")
print("-" * 40)

# åˆ›å»ºå½¢çŠ¶ä¸åŒ¹é…çš„å‚æ•°å’Œæ¢¯åº¦
param = Tensor([[1.0], [2.0], [3.0]], requires_grad=True)
optimizer = Adam([param], lr=0.1)

print(f"å‚æ•°å½¢çŠ¶: {param.shape}")

# æ¨¡æ‹Ÿå¹¿æ’­æ¢¯åº¦åœºæ™¯
# æ‰‹åŠ¨è®¾ç½®ä¸€ä¸ªéœ€è¦å¹¿æ’­çš„æ¢¯åº¦
param.grad = np.array([
    [0.1, 0.2, 0.3, 0.4],
    [0.5, 0.6, 0.7, 0.8], 
    [0.9, 1.0, 1.1, 1.2]
])

print(f"æ¢¯åº¦å½¢çŠ¶: {param.grad.shape}")

try:
    old_param = param.data.copy()
    optimizer.step()
    print(f"å‚æ•°æ›´æ–°æˆåŠŸ!")
    print(f"å‚æ•°å˜åŒ–èŒƒå›´: [{np.min(param.data - old_param):.4f}, {np.max(param.data - old_param):.4f}]")
    print("âœ“ å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•é€šè¿‡")
except Exception as e:
    print(f"âœ— å¹¿æ’­æ¢¯åº¦å¤„ç†æµ‹è¯•å¤±è´¥: {e}")

# æµ‹è¯•4: æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
print("\n4. æ•°å€¼ç¨³å®šæ€§æµ‹è¯•")
print("-" * 40)

# æµ‹è¯•1: å¤§æ¢¯åº¦
param1 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
optimizer1 = Adam([param1], lr=0.1)
param1.grad = np.array([1e8, 1e8, 1e8])
try:
    optimizer1.step()
    print("âœ“ å¤§æ¢¯åº¦å¤„ç†æ­£å¸¸")
except Exception as e:
    print(f"âœ— å¤§æ¢¯åº¦å¤„ç†å¤±è´¥: {e}")

# æµ‹è¯•2: ç¬¬ä¸€æ¬¡è¿­ä»£çš„é›¶æ¢¯åº¦
param2 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
optimizer2 = Adam([param2], lr=0.1)
param2.grad = np.array([0.0, 0.0, 0.0])
try:
    old_param = param2.data.copy()
    optimizer2.step()
    # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼Œæ¢¯åº¦ä¸ºé›¶æ—¶å‚æ•°ç¡®å®ä¸åº”æ”¹å˜
    if np.allclose(param2.data, old_param):
        print("âœ“ ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦å¤„ç†æ­£å¸¸")
    else:
        print("âš  ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦æ—¶å‚æ•°æœ‰å˜åŒ–")
except Exception as e:
    print(f"âœ— ç¬¬ä¸€æ¬¡è¿­ä»£é›¶æ¢¯åº¦å¤„ç†å¤±è´¥: {e}")

# æµ‹è¯•3: åç»­è¿­ä»£çš„é›¶æ¢¯åº¦
param3 = Tensor([1.0, 2.0, 3.0], requires_grad=True)
optimizer3 = Adam([param3], lr=0.1)

# å…ˆè¿›è¡Œä¸€æ¬¡æ­£å¸¸æ›´æ–°
param3.grad = np.array([0.1, 0.2, 0.3])
optimizer3.step()
initial_param = param3.data.copy()

# å†æµ‹è¯•é›¶æ¢¯åº¦
param3.grad = np.array([0.0, 0.0, 0.0])
old_param = param3.data.copy()
optimizer3.step()
change = np.abs(param3.data - old_param).max()

print(f"ç¬¬ä¸€æ¬¡æ›´æ–°åå‚æ•°: {initial_param}")
print(f"é›¶æ¢¯åº¦æ›´æ–°å‰å‚æ•°: {old_param}")
print(f"é›¶æ¢¯åº¦æ›´æ–°åå‚æ•°: {param3.data}")
print(f"å‚æ•°å˜åŒ–é‡: {change}")

# åˆç†çš„æ£€æŸ¥ï¼šå˜åŒ–åº”è¯¥é€æ¸è¡°å‡ï¼Œè€Œä¸æ˜¯å®Œå…¨ä¸ºé›¶
if change < 0.1:  # è®¾ç½®åˆç†çš„é˜ˆå€¼
    print("âœ“ åç»­è¿­ä»£é›¶æ¢¯åº¦å¤„ç†æ­£å¸¸")
else:
    print(f"âš  åç»­è¿­ä»£é›¶æ¢¯åº¦æ—¶å‚æ•°å˜åŒ–è¾ƒå¤§: {change}")

# æµ‹è¯•5: ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•
print("\n5. ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•")
print("-" * 40)

param1 = Tensor([5.0], requires_grad=True)
param2 = Tensor([5.0], requires_grad=True)

# ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨é…ç½®ä½†ä¸åŒçš„å®ä¾‹
optimizer1 = Adam([param1], lr=0.1)
optimizer2 = Adam([param2], lr=0.1)

# å¯¹ç¬¬ä¸€ä¸ªä¼˜åŒ–å™¨æ‰§è¡Œå¤šæ­¥
for _ in range(10):
    loss = (param1 - 2.0) ** 2
    loss.backward(np.array([1.0]))
    optimizer1.step()
    optimizer1.zero_grad()

# å¯¹ç¬¬äºŒä¸ªä¼˜åŒ–å™¨æ‰§è¡Œä¸€æ­¥
loss = (param2 - 2.0) ** 2
loss.backward(np.array([1.0]))
optimizer2.step()
optimizer2.zero_grad()

print(f"å¤šæ¬¡ä¼˜åŒ–åçš„å‚æ•°: {param1.data[0]:.4f}")
print(f"å•æ¬¡ä¼˜åŒ–åçš„å‚æ•°: {param2.data[0]:.4f}")

# ä¸¤ä¸ªå‚æ•°åº”è¯¥ä¸åŒï¼Œå› ä¸ºä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€ä¸åŒ
assert not np.allclose(param1.data, param2.data), "ä¼˜åŒ–å™¨çŠ¶æ€åº”è¯¥å½±å“å‚æ•°æ›´æ–°"
print("âœ“ ä¼˜åŒ–å™¨çŠ¶æ€é‡ç½®æµ‹è¯•é€šè¿‡")

# æµ‹è¯•6: å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•
print("\n6. å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•")
print("-" * 40)

# æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ
learning_rates = [0.01, 0.1, 0.5]
changes = []
for lr in learning_rates:
    x = Tensor([10.0], requires_grad=True)
    optimizer = SGD([x], lr=lr)
    
    # å•æ­¥ä¼˜åŒ–
    loss = (x - 0.0) ** 2
    loss.backward(np.array([1.0]))
    old_x = x.data.copy()
    optimizer.step()
    optimizer.zero_grad()
    
    change = abs(old_x[0] - x.data[0])
    changes.append(change)
    print(f"å­¦ä¹ ç‡ {lr:4.2f}: å‚æ•°å˜åŒ– = {change:.4f}")
    


# éªŒè¯å­¦ä¹ ç‡è¶Šå¤§ï¼Œå‚æ•°å˜åŒ–è¶Šå¤§
for i in range(1, len(changes)):
    assert changes[i] > changes[i-1], f"å­¦ä¹ ç‡å¢åŠ æ—¶å‚æ•°å˜åŒ–åº”è¯¥å¢å¤§ï¼Œä½† {changes[i]} <= {changes[i-1]}"

# éªŒè¯å­¦ä¹ ç‡ä¸å‚æ•°å˜åŒ–çš„çº¿æ€§å…³ç³»
# ç†è®ºä¸Šï¼šchange = lr * gradient = lr * 20.0
expected_changes = [lr * 20.0 for lr in learning_rates]

for i, (actual, expected) in enumerate(zip(changes, expected_changes)):
    # å…è®¸10%çš„è¯¯å·®
    assert abs(actual - expected) < expected * 0.1, \
        f"å­¦ä¹ ç‡ {learning_rates[i]} çš„å‚æ•°å˜åŒ– {actual:.4f} ä¸é¢„æœŸ {expected:.4f} ä¸ç¬¦"
    
    print(f"  å­¦ä¹ ç‡ {learning_rates[i]:.2f}: å®é™…å˜åŒ– {actual:.4f}, é¢„æœŸå˜åŒ– {expected:.4f}, è¯¯å·® {abs(actual - expected):.4f}")

# éªŒè¯å­¦ä¹ ç‡è¶Šå¤§ï¼Œå‚æ•°å˜åŒ–è¶Šå¤§
for i in range(1, len(changes)):
    assert changes[i] > changes[i-1], \
        f"å­¦ä¹ ç‡å¢åŠ æ—¶å‚æ•°å˜åŒ–åº”è¯¥å¢å¤§ï¼Œä½† {changes[i]} <= {changes[i-1]}"


print("âœ“ å­¦ä¹ ç‡æ•ˆæœæµ‹è¯•é€šè¿‡")


# æµ‹è¯•7: åŠ¨é‡æ•ˆæœæµ‹è¯•
print("\n7. åŠ¨é‡æ•ˆæœæµ‹è¯•")
print("-" * 40)

# æ–¹æ³•1: æµ‹è¯•åŠ¨é‡åœ¨æŒç»­æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ
print("æµ‹è¯•åŠ¨é‡åœ¨æŒç»­æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ")

# åˆ›å»ºå‚æ•°
x_momentum = Tensor([0.0], requires_grad=True)
x_no_momentum = Tensor([0.0], requires_grad=True)

optimizer_momentum = Momentum([x_momentum], lr=0.1, momentum=0.9)
optimizer_no_momentum = SGD([x_no_momentum], lr=0.1, momentum=0.0)

# æ¨¡æ‹ŸæŒç»­çš„æ­£æ¢¯åº¦ï¼ˆåŠ¨é‡åº”è¯¥åŠ é€Ÿæ”¶æ•›ï¼‰
gradients = [1.0, 1.0, 1.0, 1.0, 1.0]  # æŒç»­æ­£æ¢¯åº¦

print("æ¢¯åº¦æ›´æ–°è¿‡ç¨‹:")
for i, grad in enumerate(gradients):
    # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
    x_momentum.grad = np.array([grad])
    old_momentum = x_momentum.data[0]
    optimizer_momentum.step()
    momentum_change = x_momentum.data[0] - old_momentum
    optimizer_momentum.zero_grad()
    
    # æ— åŠ¨é‡çš„ä¼˜åŒ–
    x_no_momentum.grad = np.array([grad])
    old_no_momentum = x_no_momentum.data[0]
    optimizer_no_momentum.step()
    no_momentum_change = x_no_momentum.data[0] - old_no_momentum
    optimizer_no_momentum.zero_grad()
    
    print(f"æ­¥éª¤ {i+1}: åŠ¨é‡å˜åŒ–={momentum_change:.4f}, æ— åŠ¨é‡å˜åŒ–={no_momentum_change:.4f}")

print(f"æœ‰åŠ¨é‡æœ€ç»ˆå‚æ•°: {x_momentum.data[0]:.4f}")
print(f"æ— åŠ¨é‡æœ€ç»ˆå‚æ•°: {x_no_momentum.data[0]:.4f}")

# æ ¹æ®ä½ çš„ä¼˜åŒ–å™¨å®ç°ï¼Œæ­£æ¢¯åº¦ä¼šå¯¼è‡´å‚æ•°å‘è´Ÿæ–¹å‘ç§»åŠ¨
# å› æ­¤åŠ¨é‡åº”è¯¥è®©å‚æ•°å˜å¾—æ›´è´Ÿï¼ˆå³æ›´å°ï¼‰
assert x_momentum.data[0] < x_no_momentum.data[0], "åŠ¨é‡åº”è¯¥åœ¨è´Ÿæ¢¯åº¦æ–¹å‘ä¸ŠåŠ é€Ÿä¼˜åŒ–"

print("âœ“ åŠ¨é‡åŠ é€Ÿæ•ˆæœæµ‹è¯•é€šè¿‡")

# æ–¹æ³•2: æµ‹è¯•åŠ¨é‡åœ¨æŒç»­è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ
print("æµ‹è¯•åŠ¨é‡åœ¨æŒç»­è´Ÿæ¢¯åº¦æ–¹å‘ä¸Šçš„åŠ é€Ÿæ•ˆæœ")

# åˆ›å»ºå‚æ•°
x_momentum_neg = Tensor([0.0], requires_grad=True)
x_no_momentum_neg = Tensor([0.0], requires_grad=True)

optimizer_momentum_neg = Momentum([x_momentum_neg], lr=0.1, momentum=0.9)
optimizer_no_momentum_neg = SGD([x_no_momentum_neg], lr=0.1, momentum=0.0)

# æ¨¡æ‹ŸæŒç»­çš„è´Ÿæ¢¯åº¦ï¼ˆåŠ¨é‡åº”è¯¥åŠ é€Ÿæ”¶æ•›ï¼‰
gradients_neg = [-1.0, -1.0, -1.0, -1.0, -1.0]  # æŒç»­è´Ÿæ¢¯åº¦

print("è´Ÿæ¢¯åº¦æ›´æ–°è¿‡ç¨‹:")
for i, grad in enumerate(gradients_neg):
    # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
    x_momentum_neg.grad = np.array([grad])
    old_momentum = x_momentum_neg.data[0]
    optimizer_momentum_neg.step()
    momentum_change = x_momentum_neg.data[0] - old_momentum
    optimizer_momentum_neg.zero_grad()
    
    # æ— åŠ¨é‡çš„ä¼˜åŒ–
    x_no_momentum_neg.grad = np.array([grad])
    old_no_momentum = x_no_momentum_neg.data[0]
    optimizer_no_momentum_neg.step()
    no_momentum_change = x_no_momentum_neg.data[0] - old_no_momentum
    optimizer_no_momentum_neg.zero_grad()
    
    print(f"æ­¥éª¤ {i+1}: åŠ¨é‡å˜åŒ–={momentum_change:.4f}, æ— åŠ¨é‡å˜åŒ–={no_momentum_change:.4f}")

print(f"æœ‰åŠ¨é‡æœ€ç»ˆå‚æ•°(è´Ÿæ¢¯åº¦): {x_momentum_neg.data[0]:.4f}")
print(f"æ— åŠ¨é‡æœ€ç»ˆå‚æ•°(è´Ÿæ¢¯åº¦): {x_no_momentum_neg.data[0]:.4f}")

# è´Ÿæ¢¯åº¦ä¼šå¯¼è‡´å‚æ•°å‘æ­£æ–¹å‘ç§»åŠ¨ï¼ŒåŠ¨é‡åº”è¯¥è®©å‚æ•°å˜å¾—æ›´å¤§
assert x_momentum_neg.data[0] > x_no_momentum_neg.data[0], "åŠ¨é‡åº”è¯¥åœ¨æ­£æ¢¯åº¦æ–¹å‘ä¸ŠåŠ é€Ÿä¼˜åŒ–"

print("âœ“ åŠ¨é‡è´Ÿæ¢¯åº¦åŠ é€Ÿæ•ˆæœæµ‹è¯•é€šè¿‡")

# æµ‹è¯•åŠ¨é‡åœ¨æŒ¯è¡æ¢¯åº¦ä¸‹çš„å¹³æ»‘æ•ˆæœ
print("æµ‹è¯•åŠ¨é‡åœ¨æŒ¯è¡æ¢¯åº¦ä¸‹çš„å¹³æ»‘æ•ˆæœ")

# é‡æ–°åˆå§‹åŒ–å‚æ•°
x_momentum = Tensor([0.0], requires_grad=True)
x_no_momentum = Tensor([0.0], requires_grad=True)

optimizer_momentum = Momentum([x_momentum], lr=0.1, momentum=0.9)
optimizer_no_momentum = SGD([x_no_momentum], lr=0.1, momentum=0.0)

# æ¨¡æ‹Ÿæ›´å¼ºçš„æŒ¯è¡æ¢¯åº¦
gradients = [2.0, -1.8, 1.6, -1.4, 1.2, -1.0, 0.8, -0.6, 0.4, -0.2]

# è®°å½•è·¯å¾„
momentum_path = [x_momentum.data[0]]  # åŒ…æ‹¬åˆå§‹ç‚¹
no_momentum_path = [x_no_momentum.data[0]]

for grad in gradients:
    # æœ‰åŠ¨é‡çš„ä¼˜åŒ–
    x_momentum.grad = np.array([grad])
    optimizer_momentum.step()
    momentum_path.append(x_momentum.data[0])
    optimizer_momentum.zero_grad()
    
    # æ— åŠ¨é‡çš„ä¼˜åŒ–
    x_no_momentum.grad = np.array([grad])
    optimizer_no_momentum.step()
    no_momentum_path.append(x_no_momentum.data[0])
    optimizer_no_momentum.zero_grad()

# è®¡ç®—è·¯å¾„çš„ä¸€é˜¶å·®åˆ†
momentum_diff = np.diff(momentum_path)
no_momentum_diff = np.diff(no_momentum_path)

print(f"æœ‰åŠ¨é‡è·¯å¾„å·®åˆ†: {momentum_diff}")
print(f"æ— åŠ¨é‡è·¯å¾„å·®åˆ†: {no_momentum_diff}")

# è®¡ç®—å·®åˆ†åºåˆ—çš„æ ‡å‡†å·®
momentum_diff_std = np.std(momentum_diff)
no_momentum_diff_std = np.std(no_momentum_diff)

print(f"æœ‰åŠ¨é‡è·¯å¾„å·®åˆ†æ ‡å‡†å·®: {momentum_diff_std:.4f}")
print(f"æ— åŠ¨é‡è·¯å¾„å·®åˆ†æ ‡å‡†å·®: {no_momentum_diff_std:.4f}")

# åŠ¨é‡åº”è¯¥å‡å°‘è·¯å¾„çš„æŒ¯è¡ï¼Œå³å·®åˆ†åºåˆ—çš„æ ‡å‡†å·®æ›´å°
assert momentum_diff_std < no_momentum_diff_std, "åŠ¨é‡åº”è¯¥å‡å°‘ä¼˜åŒ–è·¯å¾„çš„æŒ¯è¡ï¼ˆå·®åˆ†åºåˆ—æ ‡å‡†å·®æ›´å°ï¼‰"

print("âœ“ åŠ¨é‡å¹³æ»‘æ•ˆæœæµ‹è¯•é€šè¿‡")

print("\n" + "=" * 70)
print("æ‰€æœ‰ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ! ğŸ‰")
print("=" * 70)

# æ€§èƒ½æ¯”è¾ƒæ€»ç»“
print("\nä¼˜åŒ–å™¨æ€§èƒ½æ€»ç»“:")
print("- SGD: åŸºç¡€ä½†å¯é ï¼Œé€‚åˆç®€å•é—®é¢˜")
print("- Momentum: åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘æŒ¯è¡") 
print("- AdaGrad: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ•°æ®")
print("- Adam: ç»“åˆåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€šå¸¸æ•ˆæœæœ€ä½³")