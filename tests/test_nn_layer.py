import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from nn import Linear, Dropout, BatchNorm1d, MaxPool2d, Conv2d, Flatten, BatchNorm2d
from core import Tensor



print("Linear å±‚æµ‹è¯•")
print("=" * 50)

# æµ‹è¯• 1: åŸºç¡€åŠŸèƒ½
print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
linear = Linear(10, 5, bias=True)
print(f"åˆ›å»º Linear å±‚: {linear}")
print(f"æƒé‡å½¢çŠ¶: {linear.weight.shape}")
print(f"åç½®å½¢çŠ¶: {linear.bias_param.shape if linear.bias_param is not None else 'æ— åç½®'}")

# æµ‹è¯• 2: å‰å‘ä¼ æ’­ (2D è¾“å…¥)
print("\n2. 2D è¾“å…¥å‰å‘ä¼ æ’­æµ‹è¯•")
x_2d = Tensor(np.random.randn(32, 10), requires_grad=True)  # batch_size=32
output_2d = linear(x_2d)
print(f"è¾“å…¥å½¢çŠ¶: {x_2d.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output_2d.shape}")
assert output_2d.shape == (32, 5), "2D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
print("âœ“ 2D è¾“å…¥æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 3: å‰å‘ä¼ æ’­ (1D è¾“å…¥)
print("\n3. 1D è¾“å…¥å‰å‘ä¼ æ’­æµ‹è¯•")
x_1d = Tensor(np.random.randn(10), requires_grad=True)
output_1d = linear(x_1d)
print(f"è¾“å…¥å½¢çŠ¶: {x_1d.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output_1d.shape}")
assert output_1d.shape == (5,), "1D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
print("âœ“ 1D è¾“å…¥æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 4: æ— åç½®å±‚
print("\n4. æ— åç½®å±‚æµ‹è¯•")
linear_no_bias = Linear(8, 4, bias=False)
x_test = Tensor(np.random.randn(16, 8))
output_no_bias = linear_no_bias(x_test)
print(f"æ— åç½®å±‚è¾“å‡ºå½¢çŠ¶: {output_no_bias.shape}")
assert output_no_bias.shape == (16, 4), "æ— åç½®å±‚è¾“å‡ºå½¢çŠ¶é”™è¯¯"
print("âœ“ æ— åç½®å±‚æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 5: å‚æ•°æ”¶é›†
print("\n5. å‚æ•°æ”¶é›†æµ‹è¯•")
params = linear.parameters()
print(f"å‚æ•°æ•°é‡: {len(params)}")
for i, param in enumerate(params):
    print(f"å‚æ•° {i}: å½¢çŠ¶={param.shape}, éœ€è¦æ¢¯åº¦={param.requires_grad}")
assert len(params) == 2, "å‚æ•°æ•°é‡é”™è¯¯"
print("âœ“ å‚æ•°æ”¶é›†æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 6: é”™è¯¯è¾“å…¥å¤„ç†
print("\n6. é”™è¯¯è¾“å…¥å¤„ç†æµ‹è¯•")
try:
    linear(Tensor(np.random.randn(32, 8)))  # é”™è¯¯ç‰¹å¾æ•°
    print("âœ— é”™è¯¯ç‰¹å¾æ•°æœªæ£€æµ‹åˆ°")
except ValueError as e:
    print(f"âœ“ é”™è¯¯ç‰¹å¾æ•°æ£€æµ‹æ­£å¸¸: {e}")

try:
    linear(Tensor(np.random.randn(32, 10, 5)))  # 3D è¾“å…¥
    print("âœ— 3D è¾“å…¥æœªæ£€æµ‹åˆ°")
except ValueError as e:
    print(f"âœ“ 3D è¾“å…¥æ£€æµ‹æ­£å¸¸: {e}")

print("\n" + "=" * 50)
print("æ‰€æœ‰ Linear å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")
"""

"""
print("Dropout å±‚æµ‹è¯•")
print("=" * 50)

# æµ‹è¯• 1: åŸºç¡€åŠŸèƒ½
print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
dropout = Dropout(p=0.5)
print(f"åˆ›å»º Dropout å±‚: {dropout}")

# æµ‹è¯• 2: è®­ç»ƒæ¨¡å¼
print("\n2. è®­ç»ƒæ¨¡å¼æµ‹è¯•")
x_train = Tensor(np.ones((5, 5)), requires_grad=True)
print("è¾“å…¥ (å…¨1):")
print(x_train.data)

output_train = dropout(x_train)
print("Dropout è¾“å‡º (è®­ç»ƒæ¨¡å¼):")
print(output_train.data)

# æ£€æŸ¥æ˜¯å¦åº”ç”¨äº† dropout
unique_values = np.unique(output_train.data)
print(f"è¾“å‡ºä¸­çš„å”¯ä¸€å€¼: {unique_values}")

# åº”è¯¥åŒ…å« 0 å’Œ 2 (å› ä¸º scale=1/(1-0.5)=2)
assert 0 in unique_values or 2 in unique_values, "è®­ç»ƒæ¨¡å¼ä¸‹æœªæ­£ç¡®åº”ç”¨ dropout"
print("âœ“ è®­ç»ƒæ¨¡å¼æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 3: è¯„ä¼°æ¨¡å¼
print("\n3. è¯„ä¼°æ¨¡å¼æµ‹è¯•")
dropout.eval()
x_eval = Tensor(np.ones((5, 5)), requires_grad=True)
# pdb.set_trace()
output_eval = dropout(x_eval)
print("Dropout è¾“å‡º (è¯„ä¼°æ¨¡å¼):")
print(output_eval.data)  
# åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥ç­‰äºè¾“å…¥
assert np.allclose(output_eval.data, x_eval.data), "è¯„ä¼°æ¨¡å¼ä¸‹è¾“å‡ºä¸ç­‰äºè¾“å…¥"
print("âœ“ è¯„ä¼°æ¨¡å¼æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 4: p=0 çš„æƒ…å†µ
print("\n4. p=0 æµ‹è¯•")
dropout_zero = Dropout(p=0)
dropout_zero.train()
x_zero = Tensor(np.ones((3, 3)), requires_grad=True)
output_zero = dropout_zero(x_zero)

# å½“ p=0 æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥ä¿ç•™
assert np.allclose(output_zero.data, x_zero.data), "p=0 æ—¶è¾“å‡ºä¸ç­‰äºè¾“å…¥"
print("âœ“ p=0 æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 5: p=1 çš„æƒ…å†µ
print("\n5. p=1 æµ‹è¯•")
dropout_one = Dropout(p=1)
dropout_one.train()
x_one = Tensor(np.ones((3, 3)), requires_grad=True)
output_one = dropout_one(x_one)

# å½“ p=1 æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥è¢«ç½®é›¶
assert np.allclose(output_one.data, 0), "p=1 æ—¶è¾“å‡ºä¸å…¨ä¸ºé›¶"
print("âœ“ p=1 æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 6: æœŸæœ›å€¼ä¿æŒ
print("\n6. æœŸæœ›å€¼ä¿æŒæµ‹è¯•")
dropout_test = Dropout(p=0.3)
dropout_test.train()

# å¤šæ¬¡è¿è¡Œï¼Œæ£€æŸ¥æœŸæœ›å€¼
x_test = Tensor(np.ones(1000), requires_grad=True)
total = 0
runs = 100

for _ in range(runs):
    output_test = dropout_test(x_test)
    total += np.mean(output_test.data)

average_mean = total / runs
print(f"å¹³å‡è¾“å‡ºå€¼: {average_mean:.4f} (æœŸæœ›æ¥è¿‘ 1.0)")

# å¹³å‡å€¼åº”è¯¥æ¥è¿‘ 1.0 (ç”±äºç¼©æ”¾)
assert 0.95 < average_mean < 1.05, f"æœŸæœ›å€¼ä¸ä¿æŒï¼Œå¾—åˆ° {average_mean}"
print("âœ“ æœŸæœ›å€¼ä¿æŒæµ‹è¯•é€šè¿‡")

# æµ‹è¯• 7: æ¢¯åº¦æµ‹è¯•
print("\n7. æ¢¯åº¦æµ‹è¯•")
dropout_grad = Dropout(p=0.5)
dropout_grad.train()

x_grad = Tensor(np.random.randn(10, 5), requires_grad=True)
output_grad = dropout_grad(x_grad)

# æ¨¡æ‹Ÿä¸€ä¸ªæŸå¤±å‡½æ•°
loss = output_grad.sum()
loss.backward()

# æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
assert x_grad.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x_grad.grad.shape}")
print("âœ“ æ¢¯åº¦æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 8: æ¨¡å¼åˆ‡æ¢
print("\n8. æ¨¡å¼åˆ‡æ¢æµ‹è¯•")
dropout_switch = Dropout(p=0.5)

# åˆå§‹åº”ä¸ºè®­ç»ƒæ¨¡å¼
assert dropout_switch.training == True, "åˆå§‹æ¨¡å¼ä¸æ˜¯è®­ç»ƒæ¨¡å¼"

# åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
dropout_switch.eval()
assert dropout_switch.training == False, "åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼å¤±è´¥"

# åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
dropout_switch.train()
assert dropout_switch.training == True, "åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼å¤±è´¥"
print("âœ“ æ¨¡å¼åˆ‡æ¢æµ‹è¯•é€šè¿‡")

print("\n" + "=" * 50)
print("æ‰€æœ‰ Dropout å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")

print("BatchNorm å±‚æµ‹è¯•")
print("=" * 50)

# æµ‹è¯• 1: BatchNorm1d åŸºç¡€åŠŸèƒ½
print("\n1. BatchNorm1d åŸºç¡€åŠŸèƒ½æµ‹è¯•")
bn1d = BatchNorm1d(64)
print(f"åˆ›å»º BatchNorm1d: {bn1d}")

# 2D è¾“å…¥æµ‹è¯•
x_2d = Tensor(np.random.randn(32, 64), requires_grad=True)
output_2d = bn1d(x_2d)
print(f"2D è¾“å…¥å½¢çŠ¶: {x_2d.shape}")
print(f"2D è¾“å‡ºå½¢çŠ¶: {output_2d.shape}")
assert output_2d.shape == (32, 64), "2D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"

# æ£€æŸ¥è¾“å‡ºç»Ÿè®¡ç‰¹æ€§
output_mean = np.mean(output_2d.data, axis=0)
output_std = np.std(output_2d.data, axis=0)
print(f"è¾“å‡ºå‡å€¼èŒƒå›´: [{np.min(output_mean):.3f}, {np.max(output_mean):.3f}]")
print(f"è¾“å‡ºæ ‡å‡†å·®èŒƒå›´: [{np.min(output_std):.3f}, {np.max(output_std):.3f}]")

# åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥æ¥è¿‘ N(0,1) åˆ†å¸ƒ
assert np.allclose(output_mean, 0, atol=0.1), "è¾“å‡ºå‡å€¼ä¸æ¥è¿‘ 0"
assert np.allclose(output_std, 1, atol=0.1), "è¾“å‡ºæ ‡å‡†å·®ä¸æ¥è¿‘ 1"
print("âœ“ BatchNorm1d 2D è¾“å…¥æµ‹è¯•é€šè¿‡")

# 3D è¾“å…¥æµ‹è¯•
x_3d = Tensor(np.random.randn(32, 64, 10), requires_grad=True)
output_3d = bn1d(x_3d)
print(f"3D è¾“å…¥å½¢çŠ¶: {x_3d.shape}")
print(f"3D è¾“å‡ºå½¢çŠ¶: {output_3d.shape}")
assert output_3d.shape == (32, 64, 10), "3D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
print("âœ“ BatchNorm1d 3D è¾“å…¥æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 2: BatchNorm1d è®­ç»ƒ/è¯„ä¼°æ¨¡å¼
print("\n2. BatchNorm1d è®­ç»ƒ/è¯„ä¼°æ¨¡å¼æµ‹è¯•")

# è®­ç»ƒæ¨¡å¼
bn1d.train()
x_train = Tensor(np.ones((16, 64)) * 5, requires_grad=True)  # å¸¸æ•°è¾“å…¥
output_train = bn1d(x_train)
print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå‡å€¼: {np.mean(output_train.data):.3f}")

# è¯„ä¼°æ¨¡å¼
bn1d.eval()
x_eval = Tensor(np.ones((16, 64)) * 5, requires_grad=True)
output_eval = bn1d(x_eval)
print(f"è¯„ä¼°æ¨¡å¼è¾“å‡ºå‡å€¼: {np.mean(output_eval.data):.3f}")

# è®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼è¾“å‡ºåº”è¯¥ä¸åŒ
assert not np.allclose(output_train.data, output_eval.data), "è®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼è¾“å‡ºç›¸åŒ"
print("âœ“ BatchNorm1d æ¨¡å¼åˆ‡æ¢æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 3: BatchNorm2d åŸºç¡€åŠŸèƒ½
print("\n3. BatchNorm2d åŸºç¡€åŠŸèƒ½æµ‹è¯•")
bn2d = BatchNorm2d(32)
print(f"åˆ›å»º BatchNorm2d: {bn2d}")

x_4d = Tensor(np.random.randn(8, 32, 14, 14), requires_grad=True)
# pdb.set_trace()
output_4d = bn2d(x_4d)
print(f"4D è¾“å…¥å½¢çŠ¶: {x_4d.shape}")
print(f"4D è¾“å‡ºå½¢çŠ¶: {output_4d.shape}")
assert output_4d.shape == (8, 32, 14, 14), "4D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"

# æ£€æŸ¥è¾“å‡ºç»Ÿè®¡ç‰¹æ€§
output_mean = np.mean(output_4d.data, axis=(0, 2, 3))
output_std = np.std(output_4d.data, axis=(0, 2, 3))
print(f"è¾“å‡ºå‡å€¼èŒƒå›´: [{np.min(output_mean):.3f}, {np.max(output_mean):.3f}]")
print(f"è¾“å‡ºæ ‡å‡†å·®èŒƒå›´: [{np.min(output_std):.3f}, {np.max(output_std):.3f}]")

# åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥æ¥è¿‘ N(0,1) åˆ†å¸ƒ
assert np.allclose(output_mean, 0, atol=0.1), "è¾“å‡ºå‡å€¼ä¸æ¥è¿‘ 0"
assert np.allclose(output_std, 1, atol=0.1), "è¾“å‡ºæ ‡å‡†å·®ä¸æ¥è¿‘ 1"
print("âœ“ BatchNorm2d æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 4: å‚æ•°æ”¶é›†
print("\n4. å‚æ•°æ”¶é›†æµ‹è¯•")
bn1d_params = bn1d.parameters()
bn2d_params = bn2d.parameters()

print(f"BatchNorm1d å‚æ•°æ•°é‡: {len(bn1d_params)}")
print(f"BatchNorm2d å‚æ•°æ•°é‡: {len(bn2d_params)}")

for i, param in enumerate(bn1d_params):
    print(f"BatchNorm1d å‚æ•° {i}: å½¢çŠ¶={param.shape}")

for i, param in enumerate(bn2d_params):
    print(f"BatchNorm2d å‚æ•° {i}: å½¢çŠ¶={param.shape}")

assert len(bn1d_params) == 2, "BatchNorm1d å‚æ•°æ•°é‡é”™è¯¯"
assert len(bn2d_params) == 2, "BatchNorm2d å‚æ•°æ•°é‡é”™è¯¯"
print("âœ“ å‚æ•°æ”¶é›†æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 5: æ— ä»¿å°„å˜æ¢
print("\n5. æ— ä»¿å°„å˜æ¢æµ‹è¯•")
bn_no_affine = BatchNorm1d(32, affine=False)
x_test = Tensor(np.random.randn(16, 32), requires_grad=True)
output_no_affine = bn_no_affine(x_test)
params_no_affine = bn_no_affine.parameters()

print(f"æ— ä»¿å°„å˜æ¢å‚æ•°æ•°é‡: {len(params_no_affine)}")
assert len(params_no_affine) == 0, "æ— ä»¿å°„å˜æ¢æ—¶å‚æ•°æ•°é‡ä¸ä¸º 0"
print("âœ“ æ— ä»¿å°„å˜æ¢æµ‹è¯•é€šè¿‡")

# æµ‹è¯• 6: æ¢¯åº¦æµ‹è¯•
print("\n6. æ¢¯åº¦æµ‹è¯•")
bn_grad = BatchNorm1d(16)
x_grad = Tensor(np.random.randn(8, 16), requires_grad=True)
output_grad = bn_grad(x_grad)

# æ¨¡æ‹ŸæŸå¤±å‡½æ•°
loss = output_grad.sum()
# pdb.set_trace()
loss.backward()

# æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
assert x_grad.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
assert bn_grad.weight.grad is not None, "æƒé‡æ¢¯åº¦æœªè®¡ç®—"
assert bn_grad.bias.grad is not None, "åç½®æ¢¯åº¦æœªè®¡ç®—"

print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x_grad.grad.shape}")
print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {bn_grad.weight.grad.shape}")
print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {bn_grad.bias.grad.shape}")
print("âœ“ æ¢¯åº¦æµ‹è¯•é€šè¿‡")

print("\n" + "=" * 50)
print("æ‰€æœ‰ BatchNorm å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")


print("\n=== CNNå±‚æµ‹è¯• ===")

# æµ‹è¯•Conv2d
print("1. Conv2dæµ‹è¯•")
conv = Conv2d(1, 3, kernel_size=3, padding=1)
x = Tensor(np.random.randn(2, 1, 5, 5) * 0.1, requires_grad=True)
y = conv(x)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")

# æµ‹è¯•æ¢¯åº¦
loss = y.sum()
loss.backward()
print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {conv.weight.grad.shape}")
if conv.bias_param:
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {conv.bias_param.grad.shape}")
print("âœ“ Conv2dæµ‹è¯•é€šè¿‡")

# æµ‹è¯•MaxPool2d
print("\n2. MaxPool2dæµ‹è¯•")
pool = MaxPool2d(2)
x_pool = Tensor(np.random.randn(2, 3, 4, 4) * 0.1, requires_grad=True)
y_pool = pool(x_pool)
print(f"è¾“å…¥å½¢çŠ¶: {x_pool.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y_pool.shape}")

loss_pool = y_pool.sum()
loss_pool.backward()
print("âœ“ MaxPool2dæµ‹è¯•é€šè¿‡")

# æµ‹è¯•Flatten
print("\n3. Flattenæµ‹è¯•")
flatten = Flatten()
x_flat = Tensor(np.random.randn(2, 3, 4, 4) * 0.1, requires_grad=True)
y_flat = flatten(x_flat)
print(f"è¾“å…¥å½¢çŠ¶: {x_flat.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y_flat.shape}")

loss_flat = y_flat.sum()
loss_flat.backward()
print("âœ“ Flattenæµ‹è¯•é€šè¿‡")

print("=== æµ‹è¯•CNNå±‚ ===")
    
# æµ‹è¯•Conv2d
print("1. Conv2dæµ‹è¯•")
conv = Conv2d(1, 3, kernel_size=3, padding=1)
x = Tensor(np.random.randn(2, 1, 5, 5) * 0.1, requires_grad=True)
y = conv(x)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")

# æµ‹è¯•æ¢¯åº¦
loss = y.sum()
loss.backward()
print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {conv.weight.grad.shape}")
if conv.bias_param:
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {conv.bias_param.grad.shape}")
print("âœ“ Conv2dæµ‹è¯•é€šè¿‡")

# æµ‹è¯•MaxPool2d
print("\n2. MaxPool2dæµ‹è¯•")
pool = MaxPool2d(2)
x_pool = Tensor(np.random.randn(2, 3, 4, 4) * 0.1, requires_grad=True)
y_pool = pool(x_pool)
print(f"è¾“å…¥å½¢çŠ¶: {x_pool.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y_pool.shape}")

loss_pool = y_pool.sum()
loss_pool.backward()
print("âœ“ MaxPool2dæµ‹è¯•é€šè¿‡")

# æµ‹è¯•Flatten
print("\n3. Flattenæµ‹è¯•")
flatten = Flatten()
x_flat = Tensor(np.random.randn(2, 3, 4, 4) * 0.1, requires_grad=True)
y_flat = flatten(x_flat)
print(f"è¾“å…¥å½¢çŠ¶: {x_flat.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {y_flat.shape}")

loss_flat = y_flat.sum()
loss_flat.backward()
print("âœ“ Flattenæµ‹è¯•é€šè¿‡")