"""
ç¥ç»ç½‘ç»œå±‚
æä¾›å„ç§ç¥ç»ç½‘ç»œå±‚å®ç°
"""

import numpy as np

# from ..core.tensor import Tensor
# from ..core import functional as F

# åœ¨å½“å‰æ–‡ä»¶ä¸‹è°ƒç”¨tensor
import sys
sys.path.append("..")
from core import Tensor
from core import functional as F


from .init import kaiming_normal_, zeros_, kaiming_uniform_, ones_
from .base import Module
import pdb

class Linear(Module):
    """
    å…¨è¿æ¥å±‚
    å®ç° y = xW^T + b çš„çº¿æ€§å˜æ¢
    
    å‚æ•°:
        in_features: è¾“å…¥ç‰¹å¾æ•°
        out_features: è¾“å‡ºç‰¹å¾æ•°  
        bias: æ˜¯å¦ä½¿ç”¨åç½®é¡¹
        device: è®¾å¤‡ç±»å‹ï¼ˆé¢„ç•™ï¼‰
        dtype: æ•°æ®ç±»å‹ï¼ˆé¢„ç•™ï¼‰

    """
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        """
        åˆå§‹åŒ–å…¨è¿æ¥å±‚
        
        å‚æ•°:
            in_features: è¾“å…¥ç‰¹å¾æ•°
            out_features: è¾“å‡ºç‰¹å¾æ•°
            bias: æ˜¯å¦ä½¿ç”¨åç½®é¡¹
        """
        # TODO: åˆå§‹åŒ–æƒé‡å’Œåç½®å‚æ•°
        # ä½¿ç”¨åˆé€‚çš„åˆå§‹åŒ–æ–¹æ³•åˆå§‹åŒ–self.weight
        # å¦‚æœbiasä¸ºTrueï¼Œåˆå§‹åŒ–self.bias
        # æ³¨å†Œå‚æ•°ä»¥ä¾¿ä¼˜åŒ–å™¨å¯ä»¥æ‰¾åˆ°å®ƒä»¬

        super().__init__() #å¿…é¡»ä½¿ç”¨çˆ¶ç±»è¿›è¡Œåˆå§‹åŒ–

        if in_features <= 0:
            raise ValueError(f"in_feature must be positive integer, but is {in_features}")
        if out_features <= 0:
            raise ValueError(f"out_feature must be postive integer, but is {out_features}")
        self.in_features = in_features
        self.out_features = out_features
        self.bias = bias

        # åˆå§‹åŒ–å‚æ•°
        self.weight = Tensor(
            np.empty((out_features,in_features),dtype=np.float32),
            requires_grad = True
        )
      
        self.register_parameter('weight', self.weight)
        # åˆå§‹åŒ–åç½®å‚æ•°
        if bias:
            self.bias_param = Tensor(
                np.empty(out_features, dtype=np.float32),
                requires_grad=True
            )
     
            self.register_parameter('bias', self.bias_param)
        else:
            self.bias_param =None

        self.reset_parameters()

    def register_parameter(self, name: str, tensor: Tensor) -> None:
        """
        å®‰å…¨åœ°æ³¨å†Œå‚æ•°
        
        å‚æ•°:
            name: å‚æ•°åç§°
            tensor: å‚æ•°å¼ é‡
        """
        if not isinstance(tensor,Tensor):
            raise TypeError(f"parameter must be Tensor, but get the {type(tensor)}")
        if not tensor.requires_grad:
            raise ValueError("register parameter must need grad")
        # è·å–_parameter
        _parameter = object.__getattribute__(self, '_parameters')
        # æ³¨å†Œå‚æ•°
        _parameter[name] = tensor
        object.__setattr__(self, name, tensor)

    def forward(self, x: Tensor) -> Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, in_features) æˆ– (in_features,)
            
        è¿”å›:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, out_features) æˆ– (out_features,)
        """
        # TODO: å®ç°å‰å‘ä¼ æ’­
        # è®¡ç®— x @ self.weight.T + self.bias (å¦‚æœå­˜åœ¨)
        if not isinstance(x, Tensor):
            raise ValueError(f"input must be the tensor, but is {type(x)}")
        # å¤„ç†1Dçš„è¾“å…¥
        if x.ndim == 1:
            if x.shape[0]!=self.in_features:
                raise ValueError(
                    f"input is not pair, expect {self.in_features} but get {x.shape[0]}"
                )
            x_2d = x.reshape(1,-1)
            output = x_2d@self.weight.T
            if self.bias_param is not None:
                output = output + self.bias_param
            # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
            return output.reshape(-1)
        # å¤„ç†2Dè¾“å…¥
        elif x.ndim ==2:
            if x.shape[-1]!=self.in_features:
                raise ValueError(
                    f"input dimension is not pair, expect is {self.in_features}, but get {x.shape[1]}"
                )
            output = x@self.weight.T
            if self.bias_param is not None:
                output = output + self.bias_param
            return output
        
        else:
            raise ValueError(f"input dimension is 1d or 2d, but get the {x.ndim} dimension")

    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def parameters(self):
        """
        è¿”å›å±‚çš„æ‰€æœ‰å‚æ•°
        
        è¿”å›:
            å‚æ•°åˆ—è¡¨
        """
        # TODO: è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        params = [self.weight]
        if self.bias_param is not None:
            params.append(self.bias_param)
        return params

    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº__repr__
        """
        return f"in_feature = {self.in_features}, out_feature = {self.out_features}, bias = {self.bias}"
    
    def __repr__(self) -> str:
        return f"Linear({self.extra_repr()})"
    
    def reset_parameters(self)->None:
        """
        é‡æ–°åˆå§‹åŒ–å‚æ•°
        """
        kaiming_uniform_(self.weight,a = np.sqrt(5), nonlinearity='relu')

        # é‡æ–°åˆå§‹åŒ–åç½®
        if self.bias_param is not None:
            zeros_(self.bias_param)

class Dropout(Module):
    """
    Dropoutå±‚
    åœ¨è®­ç»ƒæœŸé—´éšæœºå°†éƒ¨åˆ†è¾“å…¥å…ƒç´ ç½®é›¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    """
    
    def __init__(self, p: float = 0.5, inplace: bool = False):
        """
        åˆå§‹åŒ–Dropoutå±‚
        å‚æ•°:
            p: å…ƒç´ è¢«ç½®é›¶çš„æ¦‚ç‡ï¼Œé»˜è®¤ 0.5
            inplace: æ˜¯å¦åŸåœ°æ“ä½œï¼Œé»˜è®¤ False
        """
        # TODO: åˆå§‹åŒ–å‚æ•°
        # è®¾ç½®dropoutæ¦‚ç‡
        # åˆå§‹åŒ–è®­ç»ƒæ¨¡å¼æ ‡å¿—
        super().__init__() #å¿…é¡»ä½¿ç”¨çˆ¶ç±»è¿›è¡Œåˆå§‹åŒ–
        if p < 0 or p > 1:
            raise ValueError(f"Dropout possibility must be [0,1], but the value is {p}") 
        self.p = p
        self.inplace = inplace
        self.training = True #é»˜è®¤å¤„äºè®­ç»ƒæ¨¡å¼
        self.mask = None #ä¿å­˜dropoutæ©ç ï¼Œç”¨äºåå‘ä¼ æ’­
    
    def forward(self, x: Tensor) -> Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            
        è¿”å›:
            è¾“å‡ºå¼ é‡
        """
        # TODO: å®ç°å‰å‘ä¼ æ’­
        # å¦‚æœåœ¨è®­ç»ƒæ¨¡å¼ï¼Œéšæœºç”Ÿæˆmaskå¹¶åº”ç”¨
        # å¦‚æœåœ¨è¯„ä¼°æ¨¡å¼ï¼Œç›´æ¥è¿”å›è¾“å…¥
        if not isinstance(x, Tensor):
            raise TypeError(f"input must be tensor, but is {type(x)}")
        if not self.training or self.p ==0:
            if self.inplace:
                return x
            else:
                return x.copy()
        # å¦‚æœæ˜¯1ï¼Œå…¨éƒ¨ä¸¢å¼ƒï¼Œè¾“å‡º0
        if self.p==1:
            if self.inplace:
                x.data = np.zeros_like(x.data)
                return x
            else:
                return Tensor(np.zeros_like(x.data), requires_grad=True)
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨dropout
        # ç”Ÿæˆéšæœºæ©ç ï¼Œ1è¡¨ç¤ºä¿ç•™ï¼Œ0è¡¨ç¤ºæ”¾å¼ƒ
        scale = 1.0 /(1.0 - self.p)# ç¼©æ”¾å› å­ï¼Œä¿æŒæœŸæœ›å€¼ä¸å˜
        # ç”Ÿæˆéšæœºæ©ç ä¸è¾“å…¥å½¢çŠ¶ç›¸åŒ
        mask_data = np.random.binomial(1,1-self.p,x.shape)
        self.mask = Tensor(mask_data * scale, requires_grad=False)

        # åº”ç”¨å±‚dropout
        if self.inplace:
            x.data *= self.mask.data
            return x
        else:
            return x*self.mask

    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        # TODO: è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.training = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        # TODO: è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.training = False

    def parameters(self):
        """
        Dropout æ²¡æœ‰å¯ä»¥è¿”å›çš„è®­ç»ƒå‚æ•°
        """
        # TODO: è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        return []

    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº__repr__
        """
        return f"p={self.p}, inplace={self.inplace}"
    
    def __repr__(self) -> str:
        return f"Dropout({self.extra_repr()})"

class BatchNorm1d(Module):
    """
    ä¸€ç»´æ‰¹å½’ä¸€åŒ–å±‚
    å¯¹å°å‹æ‰¹é‡çš„æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
    """
    
    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, 
                 affine: bool = True, track_running_stats: bool = True):
        """
        åˆå§‹åŒ–æ‰¹å½’ä¸€åŒ–å±‚
        
        å‚æ•°:
            num_features: ç‰¹å¾æ•°
            eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶
            momentum: è¿è¡Œå‡å€¼å’Œæ–¹å·®çš„åŠ¨é‡
            affine: æ˜¯å¦å­¦ä¹ ç¼©æ”¾å’Œåç§»å‚æ•°
            track_running_stats: æ˜¯å¦è·Ÿè¸ªè¿è¡Œç»Ÿè®¡é‡
        """
        # TODO: åˆå§‹åŒ–å‚æ•°
        # åˆå§‹åŒ–å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°
        # åˆå§‹åŒ–è¿è¡Œå‡å€¼å’Œæ–¹å·®
        # è®¾ç½®å…¶ä»–è¶…å‚æ•°
        super.__init__() #å¿…é¡»è°ƒç”¨
        if num_features <=0:
            raise ValueError(f"num_feature must be postive num, but the num is {num_features}")
        if eps<0:
            raise ValueError(f"eps must be the positive number,but get{eps}")
        if momentum<0 or momentum>1:
            raise ValueError(f"momentum must between [0,1], but get {momentum}")
        
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # å¯å­¦ä¹ ç¼©æ”¾å’Œåç§»å‚æ•°
        if affine:
            self.weight = Tensor(
                np.ones(num_features, dtype=np.float32),
                requires_grad= True
            )
            self.bias = Tensor(
                np.zeros(num_features,dtype=np.float32),
                requires_grad=True
            )
        else:
            self.weight = None
            self.bias = None

       # è¿è¡Œç»Ÿè®¡é‡ï¼ˆç”¨äºè¯„ä¼°æ¨¡å¼ï¼‰
        if track_running_stats:
            self.running_mean = Tensor(
                np.zeros(num_features,dtype=np.float32),
                requires_grad=False
            )
            self.running_var = Tensor(
                np.ones(num_features,dtype=np.float32),
                requires_grad=False
            )
        else:
            self.running_mean = None
            self.running_var = None
        # å½“å‰çš„ç»Ÿè®¡é‡ï¼ˆè®­ç»ƒï¼‰
        self.current_mean = None
        self.current_val = None
        # è¯„ä¼°/è®­ç»ƒæ¨¡å¼
        self.training = True
        self.reset_parameters()
    def forward(self, x: Tensor) -> Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, num_features) æˆ– (batch_size, num_features, length)
            
        è¿”å›:
            å½’ä¸€åŒ–åçš„å¼ é‡
        """
        if not isinstance(x,Tensor):
            raise TypeError(f"input must be Tensor, but get {type(x)}")
        # æ£€æŸ¥è¾“å…¥çš„å½¢çŠ¶
        if x.ndim not in [2,3]:
            raise ValueError(f"batchnorm1d expect 2d or 3d, but get {x.ndim} tensor")
        if x.shape[1]!= self.num_features:
            raise ValueError(f"input feature is not match{self.num_features}, but get {x.shape[1]}")
        # ç¡®å®šå½’ä¸€åŒ–çš„è½´
        if x.ndim ==2:
            # å½¢çŠ¶ï¼šï¼ˆbatch_Size, num_feature)
            axes = (0,)# æ²¿æ‰¹æ¬¡ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
        else:
            # å½¢çŠ¶ï¼šï¼ˆbatch_size, num_feature, lengthï¼‰
            axes = (0,2)

        if self.training:
            return self._forward_train(x,axes)# ä½¿ç”¨å½“å‰çš„ç»Ÿè®¡é‡
        else:
            return self._forward_eval(x,axes)# ä½¿ç”¨è¿è¡Œçš„ç»Ÿè®¡é‡

    def _forward_train(self, x: Tensor, axes: tuple) -> Tensor:
        """è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­"""
        # TODO: å®ç°å‰å‘ä¼ æ’­
        # å¦‚æœåœ¨è®­ç»ƒæ¨¡å¼ï¼Œè®¡ç®—å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®ï¼Œæ›´æ–°è¿è¡Œç»Ÿè®¡é‡
        # å¦‚æœåœ¨è¯„ä¼°æ¨¡å¼ï¼Œä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡
        # åº”ç”¨å½’ä¸€åŒ–ï¼š (x - mean) / sqrt(var + eps)
        # åº”ç”¨ç¼©æ”¾å’Œåç§»ï¼š gamma * normalized_x + beta
        self.current_mean = x.mean(axis=axes, keepdims = True)
        self.current_val = x.var(axis = axes, keepdims = True)

        # æ›´æ–°è®¡ç®—ç»Ÿè®¡é‡
        if self.track_running_stats:
            # ä½¿ç”¨no_gradä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with Tensor.no_grad():# è¿è¡Œç»Ÿè®¡é‡ä¸å‚ä¸æ¢¯åº¦è¿ç®—
                self.running_mean.data = (
                    (1-self.momentum)*self.running_mean.data+\
                    self.momentum*self.current_mean.data.reshape(-1)
                )
                self.running_var.data = (
                    (1-self.momentum)*self.running_var.data+\
                    self.momentum*self.current_val.data.reshape(-1)
                )
        # å½’ä¸€åŒ–
        # pdb.set_trace()
        x_normalized = (x - self.current_mean)/(self.current_val + self.eps).sqrt()
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        if self.affine:
            # é‡å¡‘æƒé‡å’Œåç½®
            if x.ndim ==2:
                weight_reshaped = self.weight.reshape(1,-1)
                bias_reshaped = self.bias.reshape(1,-1)
            else:
                weight_reshaped = self.weight.reshape(1,-1,1)
                bias_reshaped = self.bias.reshape(1,-1,1)

            x_normalized = x_normalized * weight_reshaped + bias_reshaped
        return x_normalized
    
    def _forward_eval(self, x: Tensor, axes: tuple) -> Tensor:
        #è¯„ä¼°æ¨¡å¼çš„å‰å‘ä¼ æ’­
        if not self.track_running_stats:
            raise RuntimeError("In eval mode need use track_running_stats")
        if x.ndim ==2:
            running_mean_reshaped = self.running_mean.reshape(1,-1)
            running_var_reshaped = self.running_var.reshape(1,-1)
        else:
            running_mean_reshaped = self.running_mean.reshape(1,-1,1) 
            running_var_reshaped =  self.running_var.reshape(1,-1,1)


        # å½’ä¸€åŒ–
        x_normalized = (x - running_mean_reshaped)/np.sqrt(running_var_reshaped + self.eps)
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        if self.affine:
            # é‡å¡‘æƒé‡å’Œåç½®
            if x.ndim ==2:
                weight_reshaped = self.weight.reshape(1,-1)
                bias_reshaped = self.bias.reshape(1,-1)
            else:
                weight_reshaped = self.weight.reshape(1,-1,1)
                bias_reshaped = self.bias.reshape(1,-1,1)

            x_normalized = x_normalized * weight_reshaped + bias_reshaped
        return x_normalized
    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        # TODO: è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.training = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        # TODO: è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.training = False
    
    def parameters(self):
        """
        è¿”å›å±‚çš„æ‰€æœ‰å‚æ•°
        
        è¿”å›:
            å‚æ•°åˆ—è¡¨
        """
        # TODO: è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        params = []
        if self.affine:
            params.extend([self.weight, self.bias])
        return params

    def reset_parameters(self) -> None:
        """é‡ç½®å‚æ•°"""
        if self.affine:
            ones_(self.weight)
            zeros_(self.bias)
        if self.track_running_stats:
            zeros_(self.running_mean)
            ones_(self.running_var)

    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº__repr__
        """
        return f"{self.num_features}, eps = {self.eps}, momentum = {self.momentum}, affline = {self.affine},\
        track_running_stats = {self.track_running_stats}"
    
    def __repr__(self) -> str:
        return f"BatchNorm1d({self.extra_repr()})"
    
class BatchNorm2d(Module):
    """
    äºŒç»´æ‰¹å½’ä¸€åŒ–å±‚
    ç”¨äºå·ç§¯å±‚çš„æ‰¹å½’ä¸€åŒ–
    """
    
    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, 
                 affine: bool = True, track_running_stats: bool = True):
        """
        åˆå§‹åŒ–äºŒç»´æ‰¹å½’ä¸€åŒ–å±‚
        
        å‚æ•°:
            num_features: ç‰¹å¾æ•°ï¼ˆé€šé“æ•°ï¼‰
            eps: æ•°å€¼ç¨³å®šæ€§å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶
            momentum: è¿è¡Œå‡å€¼å’Œæ–¹å·®çš„åŠ¨é‡
            affine: æ˜¯å¦å­¦ä¹ ç¼©æ”¾å’Œåç§»å‚æ•°
            track_running_stats: æ˜¯å¦è·Ÿè¸ªè¿è¡Œç»Ÿè®¡é‡
        """
        # TODO: åˆå§‹åŒ–å‚æ•°
        # åˆå§‹åŒ–å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°
        # åˆå§‹åŒ–è¿è¡Œå‡å€¼å’Œæ–¹å·®
        # è®¾ç½®å…¶ä»–è¶…å‚æ•°
        super.__init__() 
        if num_features <=0:
            raise ValueError(f"num_feature must be postive num, but the num is {num_features}")
        if eps<0:
            raise ValueError(f"eps must be the positive number,but get{eps}")
        if momentum<0 or momentum>1:
            raise ValueError(f"momentum must between [0,1], but get {momentum}")
        
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # å¯å­¦ä¹ ç¼©æ”¾å’Œåç§»å‚æ•°
        if affine:
            self.weight = Tensor(
                np.ones(num_features, dtype=np.float32),
                requires_grad= True
            )
            self.bias = Tensor(
                np.zeros(num_features,dtype=np.float32),
                requires_grad=True
            )
        else:
            self.weight = None
            self.bias = None

       # è¿è¡Œç»Ÿè®¡é‡ï¼ˆç”¨äºè¯„ä¼°æ¨¡å¼ï¼‰
        if track_running_stats:
            self.running_mean = Tensor(
                np.zeros(num_features,dtype=np.float32),
                requires_grad=False
            )
            self.running_var = Tensor(
                np.ones(num_features,dtype=np.float32),
                requires_grad=False
            )
        else:
            self.running_mean = None
            self.running_var = None
        # å½“å‰çš„ç»Ÿè®¡é‡ï¼ˆè®­ç»ƒï¼‰
        self.current_mean = None
        self.current_val = None
        # è¯„ä¼°/è®­ç»ƒæ¨¡å¼
        self.training = True

        self.reset_parameters()
    
    def forward(self, x: Tensor) -> Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, channels, height, width)
            
        è¿”å›:
            å½’ä¸€åŒ–åçš„å¼ é‡
        """
        # TODO: å®ç°å‰å‘ä¼ æ’­
        # å¤„ç†4Dè¾“å…¥ï¼ŒæŒ‰é€šé“å½’ä¸€åŒ–
        # å…¶ä»–é€»è¾‘ä¸BatchNorm1dç±»ä¼¼
        if not isinstance(x,Tensor):
            raise TypeError(f"input must be Tensor, but get {type(x)}")
        # æ£€æŸ¥è¾“å…¥çš„å½¢çŠ¶
        if x.ndim != 4:
            raise ValueError(f"batchnorm1d expect 4d, but get {x.ndim} tensor")
        if x.shape[1]!= self.num_features:
            raise ValueError(f"input feature is not match{self.num_features}, but get {x.shape[1]}")
        # ç¡®å®šå½’ä¸€åŒ–çš„è½´,æ²¿ç€æ‰¹æ¬¡ï¼Œé«˜åº¦å’Œå®½åº¦

        axes = (0,2,3)

        if self.training:
            return self._forward_train(x,axes)# ä½¿ç”¨å½“å‰çš„ç»Ÿè®¡é‡
        else:
            return self._forward_eval(x,axes)# ä½¿ç”¨è¿è¡Œçš„ç»Ÿè®¡é‡
        
    def _forward_train(self, x: Tensor, axes: tuple) -> Tensor:
        """
        è®¡ç®—å½“å‰çš„æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®ï¼Œè®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­
        """
        self.current_mean = x.mean(axis=axes, keepdims = True)
        self.current_val = x.var(axis = axes, keepdims = True)

        # æ›´æ–°è®¡ç®—ç»Ÿè®¡é‡
        if self.track_running_stats:
            with Tensor.no_grad():# è¿è¡Œç»Ÿè®¡é‡ä¸å‚ä¸æ¢¯åº¦è¿ç®—
                self.running_mean.data = (
                    (1-self.momentum)*self.running_mean.data+\
                    self.momentum*self.current_mean.data.reshape(-1)
                )
                self.running_var.data = (
                    (1-self.momentum)*self.running_var.data+\
                    self.momentum*self.current_val.data.reshape(-1)
                )
        # å½’ä¸€åŒ–
        x_normalized = (x - self.current_mean)/(self.current_val + self.eps).sqrt()
        
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        if self.affine:
            # é‡å¡‘æƒé‡å’Œåç½®
            weight_reshaped = self.weight.reshape(1,-1,1,1)
            bias_reshaped = self.bias.reshape(1,-1,1,1)
            x_normalized = x_normalized * weight_reshaped + bias_reshaped

        return x_normalized
    

    def _forward_eval(self, x: Tensor, axes: tuple) -> Tensor:
        """
        è¯„ä¼°æ¨¡å¼çš„å‰å‘ä¼ æ’­
        """
        if not self.track_running_stats:
            raise RuntimeError("In eval mode need use track_running_stats")
   
        running_mean_reshaped = self.running_mean.reshape(1,-1,1,1)
        running_var_reshaped = self.running_var.reshape(1,-1,1,1)

        # å½’ä¸€åŒ–
        x_normalized = (x - running_mean_reshaped)/np.sqrt(running_var_reshaped + self.eps)
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        if self.affine:
            weight_reshaped = self.weight.reshape(1,-1,1,1)
            bias_reshaped = self.bias.reshape(1,-1,1,1)
            x_normalized = x_normalized * weight_reshaped + bias_reshaped
        return x_normalized
    
    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        # TODO: è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.track_running_stats = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        # TODO: è®¾ç½®è¯„ä¼°æ¨¡å¼
        self.track_running_stats = False
    
    def parameters(self):
        """
        è¿”å›å±‚çš„æ‰€æœ‰å‚æ•°
        
        è¿”å›:
            å‚æ•°åˆ—è¡¨
        """
        # TODO: è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°

        params = []
        if self.affine:
            params.extend([self.weight, self.bias])
        return params

    def reset_parameters(self) -> None:
        """é‡ç½®å‚æ•°"""
        if self.affine:
            ones_(self.weight)
            zeros_(self.bias)
        if self.track_running_stats:
            zeros_(self.running_mean)
            ones_(self.running_var)

    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº__repr__
        """
        return f"{self.num_features}, eps = {self.eps},\
        momentum = {self.momentum}, affline = {self.affine},\
        track_running_stats = {self.track_running_stats}"
    
    def __repr__(self) -> str:
        return f"BatchNorm2d({self.extra_repr()})"


class ReLU(Module):
    def __init__(self, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.mask = None  # ä¿å­˜æ¿€æ´»æ©ç ï¼Œç”¨äºåå‘ä¼ æ’­

    def forward(self, x:Tensor)->Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            
        è¿”å›:
            è¾“å‡ºå¼ é‡
        """
        return F.relu(x, inplace=self.inplace)
    
    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def parameters(self):
        """
        ReLU å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°
        
        è¿”å›:
            ç©ºåˆ—è¡¨
        """
        return []
    
    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº __repr__
        """
        return f'inplace={self.inplace}'
    
    def __repr__(self) -> str:
        return f'ReLU({self.extra_repr()})'

class Sigmoid(Module):
    def __init__(self, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.mask = None  # ä¿å­˜æ¿€æ´»æ©ç ï¼Œç”¨äºåå‘ä¼ æ’­

    def forward(self, x:Tensor)->Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            
        è¿”å›:
            è¾“å‡ºå¼ é‡
        """
        return F.sigmoid(x, inplace=self.inplace)
    
    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def parameters(self):
        """
        ReLU å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°
        
        è¿”å›:
            ç©ºåˆ—è¡¨
        """
        return []
    
    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº __repr__
        """
        return f'inplace={self.inplace}'
    
    def __repr__(self) -> str:
        return f'Sigmoid({self.extra_repr()})'
    
class Tanh(Module):
    def __init__(self, inplace=False):
        super().__init__()
        self.inplace = inplace
        self.mask = None  # ä¿å­˜æ¿€æ´»æ©ç ï¼Œç”¨äºåå‘ä¼ æ’­

    def forward(self, x:Tensor)->Tensor:
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡
            
        è¿”å›:
            è¾“å‡ºå¼ é‡
        """
        return F.tanh(x, inplace=self.inplace)
    
    def __call__(self, x: Tensor) -> Tensor:
        """ä½¿å®ä¾‹å¯è°ƒç”¨"""
        return self.forward(x)
    
    def parameters(self):
        """
        ReLU å±‚æ²¡æœ‰å¯è®­ç»ƒå‚æ•°
        
        è¿”å›:
            ç©ºåˆ—è¡¨
        """
        return []
    
    def extra_repr(self) -> str:
        """
        è¿”å›å±‚çš„é¢å¤–æè¿°ä¿¡æ¯ï¼Œç”¨äº __repr__
        """
        return f'inplace={self.inplace}'
    
    def __repr__(self) -> str:
        return f'tanh({self.extra_repr()})'

class LeakyReLU(Module):
    pass

if __name__ == "__main__":
  
    print("Linear å±‚æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯• 1: åŸºç¡€åŠŸèƒ½
    print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    linear = Linear(10, 5, bias=True)
    print(f"åˆ›å»º Linear å±‚: {linear}")
    print(f"æƒé‡å½¢çŠ¶: {linear.weight.shape}")
    print(f"åç½®å½¢çŠ¶: {linear.bias_param.shape if linear.bias_param is not None else 'æ— åç½®'}")
    
    # æµ‹è¯• 2: å‰å‘ä¼ æ’­ (2D è¾“å…¥)
    print("\n2. 2D è¾“å…¥å‰å‘ä¼ æ’­æµ‹è¯•")
    x_2d = Tensor(np.random.randn(32, 10), requires_grad=True)  # batch_size=32
    output_2d = linear(x_2d)
    print(f"è¾“å…¥å½¢çŠ¶: {x_2d.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output_2d.shape}")
    assert output_2d.shape == (32, 5), "2D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("âœ“ 2D è¾“å…¥æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 3: å‰å‘ä¼ æ’­ (1D è¾“å…¥)
    print("\n3. 1D è¾“å…¥å‰å‘ä¼ æ’­æµ‹è¯•")
    x_1d = Tensor(np.random.randn(10), requires_grad=True)
    output_1d = linear(x_1d)
    print(f"è¾“å…¥å½¢çŠ¶: {x_1d.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output_1d.shape}")
    assert output_1d.shape == (5,), "1D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("âœ“ 1D è¾“å…¥æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 4: æ— åç½®å±‚
    print("\n4. æ— åç½®å±‚æµ‹è¯•")
    linear_no_bias = Linear(8, 4, bias=False)
    x_test = Tensor(np.random.randn(16, 8))
    output_no_bias = linear_no_bias(x_test)
    print(f"æ— åç½®å±‚è¾“å‡ºå½¢çŠ¶: {output_no_bias.shape}")
    assert output_no_bias.shape == (16, 4), "æ— åç½®å±‚è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("âœ“ æ— åç½®å±‚æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 5: å‚æ•°æ”¶é›†
    print("\n5. å‚æ•°æ”¶é›†æµ‹è¯•")
    params = linear.parameters()
    print(f"å‚æ•°æ•°é‡: {len(params)}")
    for i, param in enumerate(params):
        print(f"å‚æ•° {i}: å½¢çŠ¶={param.shape}, éœ€è¦æ¢¯åº¦={param.requires_grad}")
    assert len(params) == 2, "å‚æ•°æ•°é‡é”™è¯¯"
    print("âœ“ å‚æ•°æ”¶é›†æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 6: é”™è¯¯è¾“å…¥å¤„ç†
    print("\n6. é”™è¯¯è¾“å…¥å¤„ç†æµ‹è¯•")
    try:
        linear(Tensor(np.random.randn(32, 8)))  # é”™è¯¯ç‰¹å¾æ•°
        print("âœ— é”™è¯¯ç‰¹å¾æ•°æœªæ£€æµ‹åˆ°")
    except ValueError as e:
        print(f"âœ“ é”™è¯¯ç‰¹å¾æ•°æ£€æµ‹æ­£å¸¸: {e}")
    
    try:
        linear(Tensor(np.random.randn(32, 10, 5)))  # 3D è¾“å…¥
        print("âœ— 3D è¾“å…¥æœªæ£€æµ‹åˆ°")
    except ValueError as e:
        print(f"âœ“ 3D è¾“å…¥æ£€æµ‹æ­£å¸¸: {e}")
    
    print("\n" + "=" * 50)
    print("æ‰€æœ‰ Linear å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")
    """

    """
    print("Dropout å±‚æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯• 1: åŸºç¡€åŠŸèƒ½
    print("\n1. åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    dropout = Dropout(p=0.5)
    print(f"åˆ›å»º Dropout å±‚: {dropout}")
    
    # æµ‹è¯• 2: è®­ç»ƒæ¨¡å¼
    print("\n2. è®­ç»ƒæ¨¡å¼æµ‹è¯•")
    x_train = Tensor(np.ones((5, 5)), requires_grad=True)
    print("è¾“å…¥ (å…¨1):")
    print(x_train.data)
    
    output_train = dropout(x_train)
    print("Dropout è¾“å‡º (è®­ç»ƒæ¨¡å¼):")
    print(output_train.data)
    
    # æ£€æŸ¥æ˜¯å¦åº”ç”¨äº† dropout
    unique_values = np.unique(output_train.data)
    print(f"è¾“å‡ºä¸­çš„å”¯ä¸€å€¼: {unique_values}")
    
    # åº”è¯¥åŒ…å« 0 å’Œ 2 (å› ä¸º scale=1/(1-0.5)=2)
    assert 0 in unique_values or 2 in unique_values, "è®­ç»ƒæ¨¡å¼ä¸‹æœªæ­£ç¡®åº”ç”¨ dropout"
    print("âœ“ è®­ç»ƒæ¨¡å¼æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 3: è¯„ä¼°æ¨¡å¼
    print("\n3. è¯„ä¼°æ¨¡å¼æµ‹è¯•")
    dropout.eval()
    x_eval = Tensor(np.ones((5, 5)), requires_grad=True)
    # pdb.set_trace()
    output_eval = dropout(x_eval)
    print("Dropout è¾“å‡º (è¯„ä¼°æ¨¡å¼):")
    print(output_eval.data)  
    # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥ç­‰äºè¾“å…¥
    assert np.allclose(output_eval.data, x_eval.data), "è¯„ä¼°æ¨¡å¼ä¸‹è¾“å‡ºä¸ç­‰äºè¾“å…¥"
    print("âœ“ è¯„ä¼°æ¨¡å¼æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 4: p=0 çš„æƒ…å†µ
    print("\n4. p=0 æµ‹è¯•")
    dropout_zero = Dropout(p=0)
    dropout_zero.train()
    x_zero = Tensor(np.ones((3, 3)), requires_grad=True)
    output_zero = dropout_zero(x_zero)
    
    # å½“ p=0 æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥ä¿ç•™
    assert np.allclose(output_zero.data, x_zero.data), "p=0 æ—¶è¾“å‡ºä¸ç­‰äºè¾“å…¥"
    print("âœ“ p=0 æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 5: p=1 çš„æƒ…å†µ
    print("\n5. p=1 æµ‹è¯•")
    dropout_one = Dropout(p=1)
    dropout_one.train()
    x_one = Tensor(np.ones((3, 3)), requires_grad=True)
    output_one = dropout_one(x_one)
    
    # å½“ p=1 æ—¶ï¼Œæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥è¢«ç½®é›¶
    assert np.allclose(output_one.data, 0), "p=1 æ—¶è¾“å‡ºä¸å…¨ä¸ºé›¶"
    print("âœ“ p=1 æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 6: æœŸæœ›å€¼ä¿æŒ
    print("\n6. æœŸæœ›å€¼ä¿æŒæµ‹è¯•")
    dropout_test = Dropout(p=0.3)
    dropout_test.train()
    
    # å¤šæ¬¡è¿è¡Œï¼Œæ£€æŸ¥æœŸæœ›å€¼
    x_test = Tensor(np.ones(1000), requires_grad=True)
    total = 0
    runs = 100
    
    for _ in range(runs):
        output_test = dropout_test(x_test)
        total += np.mean(output_test.data)
    
    average_mean = total / runs
    print(f"å¹³å‡è¾“å‡ºå€¼: {average_mean:.4f} (æœŸæœ›æ¥è¿‘ 1.0)")
    
    # å¹³å‡å€¼åº”è¯¥æ¥è¿‘ 1.0 (ç”±äºç¼©æ”¾)
    assert 0.95 < average_mean < 1.05, f"æœŸæœ›å€¼ä¸ä¿æŒï¼Œå¾—åˆ° {average_mean}"
    print("âœ“ æœŸæœ›å€¼ä¿æŒæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 7: æ¢¯åº¦æµ‹è¯•
    print("\n7. æ¢¯åº¦æµ‹è¯•")
    dropout_grad = Dropout(p=0.5)
    dropout_grad.train()
    
    x_grad = Tensor(np.random.randn(10, 5), requires_grad=True)
    output_grad = dropout_grad(x_grad)
    
    # æ¨¡æ‹Ÿä¸€ä¸ªæŸå¤±å‡½æ•°
    loss = output_grad.sum()
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    assert x_grad.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x_grad.grad.shape}")
    print("âœ“ æ¢¯åº¦æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 8: æ¨¡å¼åˆ‡æ¢
    print("\n8. æ¨¡å¼åˆ‡æ¢æµ‹è¯•")
    dropout_switch = Dropout(p=0.5)
    
    # åˆå§‹åº”ä¸ºè®­ç»ƒæ¨¡å¼
    assert dropout_switch.training == True, "åˆå§‹æ¨¡å¼ä¸æ˜¯è®­ç»ƒæ¨¡å¼"
    
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    dropout_switch.eval()
    assert dropout_switch.training == False, "åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼å¤±è´¥"
    
    # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    dropout_switch.train()
    assert dropout_switch.training == True, "åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼å¤±è´¥"
    print("âœ“ æ¨¡å¼åˆ‡æ¢æµ‹è¯•é€šè¿‡")
    
    print("\n" + "=" * 50)
    print("æ‰€æœ‰ Dropout å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")
   
    print("BatchNorm å±‚æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯• 1: BatchNorm1d åŸºç¡€åŠŸèƒ½
    print("\n1. BatchNorm1d åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    bn1d = BatchNorm1d(64)
    print(f"åˆ›å»º BatchNorm1d: {bn1d}")
    
    # 2D è¾“å…¥æµ‹è¯•
    x_2d = Tensor(np.random.randn(32, 64), requires_grad=True)
    output_2d = bn1d(x_2d)
    print(f"2D è¾“å…¥å½¢çŠ¶: {x_2d.shape}")
    print(f"2D è¾“å‡ºå½¢çŠ¶: {output_2d.shape}")
    assert output_2d.shape == (32, 64), "2D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    
    # æ£€æŸ¥è¾“å‡ºç»Ÿè®¡ç‰¹æ€§
    output_mean = np.mean(output_2d.data, axis=0)
    output_std = np.std(output_2d.data, axis=0)
    print(f"è¾“å‡ºå‡å€¼èŒƒå›´: [{np.min(output_mean):.3f}, {np.max(output_mean):.3f}]")
    print(f"è¾“å‡ºæ ‡å‡†å·®èŒƒå›´: [{np.min(output_std):.3f}, {np.max(output_std):.3f}]")
    
    # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥æ¥è¿‘ N(0,1) åˆ†å¸ƒ
    assert np.allclose(output_mean, 0, atol=0.1), "è¾“å‡ºå‡å€¼ä¸æ¥è¿‘ 0"
    assert np.allclose(output_std, 1, atol=0.1), "è¾“å‡ºæ ‡å‡†å·®ä¸æ¥è¿‘ 1"
    print("âœ“ BatchNorm1d 2D è¾“å…¥æµ‹è¯•é€šè¿‡")
    
    # 3D è¾“å…¥æµ‹è¯•
    x_3d = Tensor(np.random.randn(32, 64, 10), requires_grad=True)
    output_3d = bn1d(x_3d)
    print(f"3D è¾“å…¥å½¢çŠ¶: {x_3d.shape}")
    print(f"3D è¾“å‡ºå½¢çŠ¶: {output_3d.shape}")
    assert output_3d.shape == (32, 64, 10), "3D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    print("âœ“ BatchNorm1d 3D è¾“å…¥æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 2: BatchNorm1d è®­ç»ƒ/è¯„ä¼°æ¨¡å¼
    print("\n2. BatchNorm1d è®­ç»ƒ/è¯„ä¼°æ¨¡å¼æµ‹è¯•")
    
    # è®­ç»ƒæ¨¡å¼
    bn1d.train()
    x_train = Tensor(np.ones((16, 64)) * 5, requires_grad=True)  # å¸¸æ•°è¾“å…¥
    output_train = bn1d(x_train)
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå‡å€¼: {np.mean(output_train.data):.3f}")
    
    # è¯„ä¼°æ¨¡å¼
    bn1d.eval()
    x_eval = Tensor(np.ones((16, 64)) * 5, requires_grad=True)
    output_eval = bn1d(x_eval)
    print(f"è¯„ä¼°æ¨¡å¼è¾“å‡ºå‡å€¼: {np.mean(output_eval.data):.3f}")
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼è¾“å‡ºåº”è¯¥ä¸åŒ
    assert not np.allclose(output_train.data, output_eval.data), "è®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼è¾“å‡ºç›¸åŒ"
    print("âœ“ BatchNorm1d æ¨¡å¼åˆ‡æ¢æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 3: BatchNorm2d åŸºç¡€åŠŸèƒ½
    print("\n3. BatchNorm2d åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    bn2d = BatchNorm2d(32)
    print(f"åˆ›å»º BatchNorm2d: {bn2d}")
    
    x_4d = Tensor(np.random.randn(8, 32, 14, 14), requires_grad=True)
    # pdb.set_trace()
    output_4d = bn2d(x_4d)
    print(f"4D è¾“å…¥å½¢çŠ¶: {x_4d.shape}")
    print(f"4D è¾“å‡ºå½¢çŠ¶: {output_4d.shape}")
    assert output_4d.shape == (8, 32, 14, 14), "4D è¾“å…¥è¾“å‡ºå½¢çŠ¶é”™è¯¯"
    
    # æ£€æŸ¥è¾“å‡ºç»Ÿè®¡ç‰¹æ€§
    output_mean = np.mean(output_4d.data, axis=(0, 2, 3))
    output_std = np.std(output_4d.data, axis=(0, 2, 3))
    print(f"è¾“å‡ºå‡å€¼èŒƒå›´: [{np.min(output_mean):.3f}, {np.max(output_mean):.3f}]")
    print(f"è¾“å‡ºæ ‡å‡†å·®èŒƒå›´: [{np.min(output_std):.3f}, {np.max(output_std):.3f}]")
    
    # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¾“å‡ºåº”è¯¥æ¥è¿‘ N(0,1) åˆ†å¸ƒ
    assert np.allclose(output_mean, 0, atol=0.1), "è¾“å‡ºå‡å€¼ä¸æ¥è¿‘ 0"
    assert np.allclose(output_std, 1, atol=0.1), "è¾“å‡ºæ ‡å‡†å·®ä¸æ¥è¿‘ 1"
    print("âœ“ BatchNorm2d æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 4: å‚æ•°æ”¶é›†
    print("\n4. å‚æ•°æ”¶é›†æµ‹è¯•")
    bn1d_params = bn1d.parameters()
    bn2d_params = bn2d.parameters()
    
    print(f"BatchNorm1d å‚æ•°æ•°é‡: {len(bn1d_params)}")
    print(f"BatchNorm2d å‚æ•°æ•°é‡: {len(bn2d_params)}")
    
    for i, param in enumerate(bn1d_params):
        print(f"BatchNorm1d å‚æ•° {i}: å½¢çŠ¶={param.shape}")
    
    for i, param in enumerate(bn2d_params):
        print(f"BatchNorm2d å‚æ•° {i}: å½¢çŠ¶={param.shape}")
    
    assert len(bn1d_params) == 2, "BatchNorm1d å‚æ•°æ•°é‡é”™è¯¯"
    assert len(bn2d_params) == 2, "BatchNorm2d å‚æ•°æ•°é‡é”™è¯¯"
    print("âœ“ å‚æ•°æ”¶é›†æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 5: æ— ä»¿å°„å˜æ¢
    print("\n5. æ— ä»¿å°„å˜æ¢æµ‹è¯•")
    bn_no_affine = BatchNorm1d(32, affine=False)
    x_test = Tensor(np.random.randn(16, 32), requires_grad=True)
    output_no_affine = bn_no_affine(x_test)
    params_no_affine = bn_no_affine.parameters()
    
    print(f"æ— ä»¿å°„å˜æ¢å‚æ•°æ•°é‡: {len(params_no_affine)}")
    assert len(params_no_affine) == 0, "æ— ä»¿å°„å˜æ¢æ—¶å‚æ•°æ•°é‡ä¸ä¸º 0"
    print("âœ“ æ— ä»¿å°„å˜æ¢æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯• 6: æ¢¯åº¦æµ‹è¯•
    print("\n6. æ¢¯åº¦æµ‹è¯•")
    bn_grad = BatchNorm1d(16)
    x_grad = Tensor(np.random.randn(8, 16), requires_grad=True)
    output_grad = bn_grad(x_grad)
    
    # æ¨¡æ‹ŸæŸå¤±å‡½æ•°
    loss = output_grad.sum()
    # pdb.set_trace()
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
    assert x_grad.grad is not None, "è¾“å…¥æ¢¯åº¦æœªè®¡ç®—"
    assert bn_grad.weight.grad is not None, "æƒé‡æ¢¯åº¦æœªè®¡ç®—"
    assert bn_grad.bias.grad is not None, "åç½®æ¢¯åº¦æœªè®¡ç®—"
    
    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x_grad.grad.shape}")
    print(f"æƒé‡æ¢¯åº¦å½¢çŠ¶: {bn_grad.weight.grad.shape}")
    print(f"åç½®æ¢¯åº¦å½¢çŠ¶: {bn_grad.bias.grad.shape}")
    print("âœ“ æ¢¯åº¦æµ‹è¯•é€šè¿‡")
    
    print("\n" + "=" * 50)
    print("æ‰€æœ‰ BatchNorm å±‚æµ‹è¯•é€šè¿‡ï¼ğŸ‰")